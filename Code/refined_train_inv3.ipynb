{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOn4I4EzKXS+DheRG83eaFv"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"premium"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bOnJbuxqyIna","executionInfo":{"status":"ok","timestamp":1668030968007,"user_tz":300,"elapsed":791,"user":{"displayName":"Zhou Zeqi","userId":"01018575624929983032"}},"outputId":"7ff0466a-9ee9-418a-cdd6-cd69f2e51278"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import sys\n","sys.path.insert(0,'/content/drive/MyDrive/566/final/TensorFlow2.0_InceptionV3-master')"],"metadata":{"id":"6va3iv2AyUw1","executionInfo":{"status":"ok","timestamp":1668030968812,"user_tz":300,"elapsed":99,"user":{"displayName":"Zhou Zeqi","userId":"01018575624929983032"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# some training parameters\n","EPOCHS = 50\n","BATCH_SIZE = 8\n","NUM_CLASSES = 3\n","image_height = 299\n","image_width = 299\n","channels = 3\n","save_model_dir = \"/content/drive/MyDrive/566/final/TensorFlow2.0_InceptionV3-master/saved_model/model\"\n","dataset_dir = \"/content/drive/MyDrive/566/final/divided_dataset/\"\n","train_dir = dataset_dir + \"train\"\n","valid_dir = dataset_dir + \"valid\"\n","test_dir = dataset_dir + \"test\""],"metadata":{"id":"Np8cpAehS-ZK","executionInfo":{"status":"ok","timestamp":1668032451906,"user_tz":300,"elapsed":2,"user":{"displayName":"Zhou Zeqi","userId":"01018575624929983032"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["from __future__ import absolute_import, division, print_function\n","import tensorflow as tf\n","from models import inception_v3\n","import config\n","from prepare_data import generate_datasets\n","import math\n","\n","def get_model():\n","    model = inception_v3.InceptionV3(num_class=NUM_CLASSES)\n","\n","    model.build(input_shape=(None, image_height, image_width, channels))\n","    model.summary()\n","\n","    return model\n","\n","\n","if __name__ == '__main__':\n","    # GPU settings\n","    gpus = tf.config.experimental.list_physical_devices('GPU')\n","    if gpus:\n","        for gpu in gpus:\n","            tf.config.experimental.set_memory_growth(gpu, True)\n","\n","\n","    # get the original_dataset\n","    train_dataset, valid_dataset, test_dataset, train_count, valid_count, test_count = generate_datasets(train_dir,valid_dir,test_dir)\n","\n","\n","    # create model\n","    model = get_model()\n","\n","    # define loss and optimizer\n","    loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n","    optimizer = tf.keras.optimizers.Adadelta()\n","\n","    train_loss = tf.keras.metrics.Mean(name='train_loss')\n","    train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n","\n","    valid_loss = tf.keras.metrics.Mean(name='valid_loss')\n","    valid_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='valid_accuracy')\n","\n","    @tf.function\n","    def train_step(images, labels):\n","        with tf.GradientTape() as tape:\n","            predictions = model(images, include_aux_logits=True, training=True)\n","            loss_aux = loss_object(y_true=labels, y_pred=predictions.aux_logits)\n","            loss = 0.5 * loss_aux + 0.5 * loss_object(y_true=labels, y_pred=predictions.logits)\n","        gradients = tape.gradient(loss, model.trainable_variables)\n","        optimizer.apply_gradients(grads_and_vars=zip(gradients, model.trainable_variables))\n","\n","        train_loss(loss)\n","        train_accuracy(labels, predictions.logits)\n","\n","    @tf.function\n","    def valid_step(images, labels):\n","        predictions = model(images, include_aux_logits=False, training=False)\n","        v_loss = loss_object(labels, predictions)\n","\n","        valid_loss(v_loss)\n","        valid_accuracy(labels, predictions)\n","\n","    # start training\n","    for epoch in range(EPOCHS):\n","        train_loss.reset_states()\n","        train_accuracy.reset_states()\n","        valid_loss.reset_states()\n","        valid_accuracy.reset_states()\n","        step = 0\n","        for images, labels in train_dataset:\n","            step += 1\n","            train_step(images, labels)\n","            print(\"Epoch: {}/{}, step: {}/{}, loss: {:.5f}, accuracy: {:.5f}\".format(epoch + 1,\n","                                                                                     config.EPOCHS,\n","                                                                                     step,\n","                                                                                     math.ceil(train_count / config.BATCH_SIZE),\n","                                                                                     train_loss.result(),\n","                                                                                     train_accuracy.result()))\n","\n","        for valid_images, valid_labels in valid_dataset:\n","            valid_step(valid_images, valid_labels)\n","\n","        print(\"Epoch: {}/{}, train loss: {:.5f}, train accuracy: {:.5f}, \"\n","              \"valid loss: {:.5f}, valid accuracy: {:.5f}\".format(epoch + 1,\n","                                                                  config.EPOCHS,\n","                                                                  train_loss.result(),\n","                                                                  train_accuracy.result(),\n","                                                                  valid_loss.result(),\n","                                                                  valid_accuracy.result()))\n","\n","    model.save_weights(filepath=save_model_dir, save_format='tf')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"evQquGPByn_v","executionInfo":{"status":"ok","timestamp":1668031741672,"user_tz":300,"elapsed":315428,"user":{"displayName":"Zhou Zeqi","userId":"01018575624929983032"}},"outputId":"8692ec03-81b8-453a-b8bd-16751df90db6"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"inception_v3_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," preprocess_1 (Preprocess)   multiple                  173872    \n","                                                                 \n"," sequential_3 (Sequential)   (None, 38, 38, 288)       822896    \n","                                                                 \n"," sequential_4 (Sequential)   (None, 18, 18, 768)       7997312   \n","                                                                 \n"," inception_aux_1 (InceptionA  multiple                 2562691   \n"," ux)                                                             \n","                                                                 \n"," sequential_5 (Sequential)   (None, 8, 8, 2048)        11065984  \n","                                                                 \n"," average_pooling2d_13 (Avera  multiple                 0         \n"," gePooling2D)                                                    \n","                                                                 \n"," dropout_1 (Dropout)         multiple                  0         \n","                                                                 \n"," flatten_3 (Flatten)         multiple                  0         \n","                                                                 \n"," dense_3 (Dense)             multiple                  6147      \n","                                                                 \n","=================================================================\n","Total params: 22,628,902\n","Trainable params: 22,595,750\n","Non-trainable params: 33,152\n","_________________________________________________________________\n","Epoch: 1/50, step: 1/83, loss: 7.49721, accuracy: 0.50000\n","Epoch: 1/50, step: 2/83, loss: 7.77140, accuracy: 0.50000\n","Epoch: 1/50, step: 3/83, loss: 8.59018, accuracy: 0.41667\n","Epoch: 1/50, step: 4/83, loss: 8.90876, accuracy: 0.43750\n","Epoch: 1/50, step: 5/83, loss: 8.69363, accuracy: 0.42500\n","Epoch: 1/50, step: 6/83, loss: 8.89804, accuracy: 0.43750\n","Epoch: 1/50, step: 7/83, loss: 8.84985, accuracy: 0.39286\n","Epoch: 1/50, step: 8/83, loss: 8.88556, accuracy: 0.40625\n","Epoch: 1/50, step: 9/83, loss: 8.88844, accuracy: 0.38889\n","Epoch: 1/50, step: 10/83, loss: 9.06684, accuracy: 0.40000\n","Epoch: 1/50, step: 11/83, loss: 9.06177, accuracy: 0.40909\n","Epoch: 1/50, step: 12/83, loss: 9.12512, accuracy: 0.42708\n","Epoch: 1/50, step: 13/83, loss: 9.11485, accuracy: 0.41346\n","Epoch: 1/50, step: 14/83, loss: 9.11381, accuracy: 0.40179\n","Epoch: 1/50, step: 15/83, loss: 9.09665, accuracy: 0.42500\n","Epoch: 1/50, step: 16/83, loss: 9.08899, accuracy: 0.42969\n","Epoch: 1/50, step: 17/83, loss: 9.18704, accuracy: 0.42647\n","Epoch: 1/50, step: 18/83, loss: 9.19988, accuracy: 0.42361\n","Epoch: 1/50, step: 19/83, loss: 9.19123, accuracy: 0.41447\n","Epoch: 1/50, step: 20/83, loss: 9.13716, accuracy: 0.41250\n","Epoch: 1/50, step: 21/83, loss: 9.04423, accuracy: 0.41071\n","Epoch: 1/50, step: 22/83, loss: 9.00154, accuracy: 0.40909\n","Epoch: 1/50, step: 23/83, loss: 8.97234, accuracy: 0.39674\n","Epoch: 1/50, step: 24/83, loss: 9.02428, accuracy: 0.39062\n","Epoch: 1/50, step: 25/83, loss: 9.02362, accuracy: 0.38500\n","Epoch: 1/50, step: 26/83, loss: 9.03164, accuracy: 0.37981\n","Epoch: 1/50, step: 27/83, loss: 9.01986, accuracy: 0.38889\n","Epoch: 1/50, step: 28/83, loss: 9.00173, accuracy: 0.37946\n","Epoch: 1/50, step: 29/83, loss: 8.91902, accuracy: 0.37069\n","Epoch: 1/50, step: 30/83, loss: 8.88692, accuracy: 0.37500\n","Epoch: 1/50, step: 31/83, loss: 8.86697, accuracy: 0.37903\n","Epoch: 1/50, step: 32/83, loss: 8.86722, accuracy: 0.37891\n","Epoch: 1/50, step: 33/83, loss: 8.82234, accuracy: 0.37500\n","Epoch: 1/50, step: 34/83, loss: 8.79784, accuracy: 0.37500\n","Epoch: 1/50, step: 35/83, loss: 8.80095, accuracy: 0.37857\n","Epoch: 1/50, step: 36/83, loss: 8.77936, accuracy: 0.37500\n","Epoch: 1/50, step: 37/83, loss: 8.76182, accuracy: 0.37162\n","Epoch: 1/50, step: 38/83, loss: 8.71587, accuracy: 0.37500\n","Epoch: 1/50, step: 39/83, loss: 8.70798, accuracy: 0.37500\n","Epoch: 1/50, step: 40/83, loss: 8.60455, accuracy: 0.38125\n","Epoch: 1/50, step: 41/83, loss: 8.63544, accuracy: 0.38110\n","Epoch: 1/50, step: 42/83, loss: 8.66065, accuracy: 0.38095\n","Epoch: 1/50, step: 43/83, loss: 8.66386, accuracy: 0.38663\n","Epoch: 1/50, step: 44/83, loss: 8.62693, accuracy: 0.38920\n","Epoch: 1/50, step: 45/83, loss: 8.61985, accuracy: 0.38611\n","Epoch: 1/50, step: 46/83, loss: 8.61198, accuracy: 0.38587\n","Epoch: 1/50, step: 47/83, loss: 8.60850, accuracy: 0.38564\n","Epoch: 1/50, step: 48/83, loss: 8.57843, accuracy: 0.38802\n","Epoch: 1/50, step: 49/83, loss: 8.57124, accuracy: 0.38520\n","Epoch: 1/50, step: 50/83, loss: 8.58254, accuracy: 0.38250\n","Epoch: 1/50, step: 51/83, loss: 8.57742, accuracy: 0.37745\n","Epoch: 1/50, step: 52/83, loss: 8.53869, accuracy: 0.37260\n","Epoch: 1/50, step: 53/83, loss: 8.49711, accuracy: 0.37500\n","Epoch: 1/50, step: 54/83, loss: 8.48696, accuracy: 0.37731\n","Epoch: 1/50, step: 55/83, loss: 8.44437, accuracy: 0.38182\n","Epoch: 1/50, step: 56/83, loss: 8.44877, accuracy: 0.38616\n","Epoch: 1/50, step: 57/83, loss: 8.45910, accuracy: 0.38596\n","Epoch: 1/50, step: 58/83, loss: 8.47357, accuracy: 0.38362\n","Epoch: 1/50, step: 59/83, loss: 8.48031, accuracy: 0.38347\n","Epoch: 1/50, step: 60/83, loss: 8.45664, accuracy: 0.38542\n","Epoch: 1/50, step: 61/83, loss: 8.50280, accuracy: 0.37910\n","Epoch: 1/50, step: 62/83, loss: 8.49938, accuracy: 0.37500\n","Epoch: 1/50, step: 63/83, loss: 8.48989, accuracy: 0.37302\n","Epoch: 1/50, step: 64/83, loss: 8.50445, accuracy: 0.36914\n","Epoch: 1/50, step: 65/83, loss: 8.48299, accuracy: 0.36923\n","Epoch: 1/50, step: 66/83, loss: 8.46218, accuracy: 0.36932\n","Epoch: 1/50, step: 67/83, loss: 8.46576, accuracy: 0.36940\n","Epoch: 1/50, step: 68/83, loss: 8.45862, accuracy: 0.37316\n","Epoch: 1/50, step: 69/83, loss: 8.45168, accuracy: 0.37681\n","Epoch: 1/50, step: 70/83, loss: 8.42885, accuracy: 0.37143\n","Epoch: 1/50, step: 71/83, loss: 8.43923, accuracy: 0.36972\n","Epoch: 1/50, step: 72/83, loss: 8.43452, accuracy: 0.36806\n","Epoch: 1/50, step: 73/83, loss: 8.40185, accuracy: 0.36986\n","Epoch: 1/50, step: 74/83, loss: 8.37193, accuracy: 0.37669\n","Epoch: 1/50, step: 75/83, loss: 8.35794, accuracy: 0.37500\n","Epoch: 1/50, step: 76/83, loss: 8.35079, accuracy: 0.37829\n","Epoch: 1/50, step: 77/83, loss: 8.33390, accuracy: 0.38149\n","Epoch: 1/50, step: 78/83, loss: 8.33296, accuracy: 0.38141\n","Epoch: 1/50, step: 79/83, loss: 8.32864, accuracy: 0.38449\n","Epoch: 1/50, step: 80/83, loss: 8.30424, accuracy: 0.38594\n","Epoch: 1/50, step: 81/83, loss: 8.28135, accuracy: 0.38580\n","Epoch: 1/50, step: 82/83, loss: 8.26004, accuracy: 0.38415\n","Epoch: 1/50, step: 83/83, loss: 8.25356, accuracy: 0.38508\n","Epoch: 1/50, train loss: 8.25356, train accuracy: 0.38508, valid loss: 7.40354, valid accuracy: 0.51131\n","Epoch: 2/50, step: 1/83, loss: 5.80249, accuracy: 0.37500\n","Epoch: 2/50, step: 2/83, loss: 7.37771, accuracy: 0.37500\n","Epoch: 2/50, step: 3/83, loss: 6.98869, accuracy: 0.33333\n","Epoch: 2/50, step: 4/83, loss: 7.32062, accuracy: 0.34375\n","Epoch: 2/50, step: 5/83, loss: 6.64297, accuracy: 0.37500\n","Epoch: 2/50, step: 6/83, loss: 6.86071, accuracy: 0.39583\n","Epoch: 2/50, step: 7/83, loss: 6.60065, accuracy: 0.44643\n","Epoch: 2/50, step: 8/83, loss: 6.87812, accuracy: 0.45312\n","Epoch: 2/50, step: 9/83, loss: 6.85459, accuracy: 0.45833\n","Epoch: 2/50, step: 10/83, loss: 6.70792, accuracy: 0.46250\n","Epoch: 2/50, step: 11/83, loss: 6.82133, accuracy: 0.45455\n","Epoch: 2/50, step: 12/83, loss: 6.86744, accuracy: 0.44792\n","Epoch: 2/50, step: 13/83, loss: 7.08039, accuracy: 0.46154\n","Epoch: 2/50, step: 14/83, loss: 7.03115, accuracy: 0.44643\n","Epoch: 2/50, step: 15/83, loss: 7.02571, accuracy: 0.45833\n","Epoch: 2/50, step: 16/83, loss: 7.14337, accuracy: 0.46094\n","Epoch: 2/50, step: 17/83, loss: 7.25336, accuracy: 0.44118\n","Epoch: 2/50, step: 18/83, loss: 7.34686, accuracy: 0.43750\n","Epoch: 2/50, step: 19/83, loss: 7.26072, accuracy: 0.42763\n","Epoch: 2/50, step: 20/83, loss: 7.30707, accuracy: 0.42500\n","Epoch: 2/50, step: 21/83, loss: 7.35037, accuracy: 0.41667\n","Epoch: 2/50, step: 22/83, loss: 7.41407, accuracy: 0.42045\n","Epoch: 2/50, step: 23/83, loss: 7.45032, accuracy: 0.42391\n","Epoch: 2/50, step: 24/83, loss: 7.37944, accuracy: 0.42188\n","Epoch: 2/50, step: 25/83, loss: 7.34193, accuracy: 0.41500\n","Epoch: 2/50, step: 26/83, loss: 7.33091, accuracy: 0.41827\n","Epoch: 2/50, step: 27/83, loss: 7.31424, accuracy: 0.41667\n","Epoch: 2/50, step: 28/83, loss: 7.33966, accuracy: 0.41964\n","Epoch: 2/50, step: 29/83, loss: 7.38829, accuracy: 0.41810\n","Epoch: 2/50, step: 30/83, loss: 7.35722, accuracy: 0.42083\n","Epoch: 2/50, step: 31/83, loss: 7.37566, accuracy: 0.41935\n","Epoch: 2/50, step: 32/83, loss: 7.36031, accuracy: 0.42188\n","Epoch: 2/50, step: 33/83, loss: 7.33007, accuracy: 0.41667\n","Epoch: 2/50, step: 34/83, loss: 7.34687, accuracy: 0.41912\n","Epoch: 2/50, step: 35/83, loss: 7.36066, accuracy: 0.42143\n","Epoch: 2/50, step: 36/83, loss: 7.28761, accuracy: 0.41667\n","Epoch: 2/50, step: 37/83, loss: 7.22080, accuracy: 0.40878\n","Epoch: 2/50, step: 38/83, loss: 7.17216, accuracy: 0.41118\n","Epoch: 2/50, step: 39/83, loss: 7.10772, accuracy: 0.41026\n","Epoch: 2/50, step: 40/83, loss: 7.13224, accuracy: 0.40938\n","Epoch: 2/50, step: 41/83, loss: 7.18551, accuracy: 0.40244\n","Epoch: 2/50, step: 42/83, loss: 7.18808, accuracy: 0.40179\n","Epoch: 2/50, step: 43/83, loss: 7.22669, accuracy: 0.39535\n","Epoch: 2/50, step: 44/83, loss: 7.18311, accuracy: 0.39489\n","Epoch: 2/50, step: 45/83, loss: 7.22394, accuracy: 0.39167\n","Epoch: 2/50, step: 46/83, loss: 7.18155, accuracy: 0.39946\n","Epoch: 2/50, step: 47/83, loss: 7.21984, accuracy: 0.39628\n","Epoch: 2/50, step: 48/83, loss: 7.24687, accuracy: 0.38802\n","Epoch: 2/50, step: 49/83, loss: 7.26416, accuracy: 0.39031\n","Epoch: 2/50, step: 50/83, loss: 7.24586, accuracy: 0.38750\n","Epoch: 2/50, step: 51/83, loss: 7.21307, accuracy: 0.38971\n","Epoch: 2/50, step: 52/83, loss: 7.23044, accuracy: 0.38942\n","Epoch: 2/50, step: 53/83, loss: 7.21413, accuracy: 0.38915\n","Epoch: 2/50, step: 54/83, loss: 7.18703, accuracy: 0.39120\n","Epoch: 2/50, step: 55/83, loss: 7.18613, accuracy: 0.39091\n","Epoch: 2/50, step: 56/83, loss: 7.16948, accuracy: 0.39286\n","Epoch: 2/50, step: 57/83, loss: 7.14866, accuracy: 0.39693\n","Epoch: 2/50, step: 58/83, loss: 7.12368, accuracy: 0.39224\n","Epoch: 2/50, step: 59/83, loss: 7.14216, accuracy: 0.39407\n","Epoch: 2/50, step: 60/83, loss: 7.14136, accuracy: 0.39583\n","Epoch: 2/50, step: 61/83, loss: 7.15886, accuracy: 0.39139\n","Epoch: 2/50, step: 62/83, loss: 7.11901, accuracy: 0.39113\n","Epoch: 2/50, step: 63/83, loss: 7.07757, accuracy: 0.39484\n","Epoch: 2/50, step: 64/83, loss: 7.06750, accuracy: 0.39062\n","Epoch: 2/50, step: 65/83, loss: 7.05658, accuracy: 0.39038\n","Epoch: 2/50, step: 66/83, loss: 7.07356, accuracy: 0.39015\n","Epoch: 2/50, step: 67/83, loss: 7.07646, accuracy: 0.38993\n","Epoch: 2/50, step: 68/83, loss: 7.06545, accuracy: 0.38971\n","Epoch: 2/50, step: 69/83, loss: 7.08329, accuracy: 0.38768\n","Epoch: 2/50, step: 70/83, loss: 7.09794, accuracy: 0.38929\n","Epoch: 2/50, step: 71/83, loss: 7.08483, accuracy: 0.39437\n","Epoch: 2/50, step: 72/83, loss: 7.05728, accuracy: 0.39757\n","Epoch: 2/50, step: 73/83, loss: 7.03448, accuracy: 0.39897\n","Epoch: 2/50, step: 74/83, loss: 7.02688, accuracy: 0.39696\n","Epoch: 2/50, step: 75/83, loss: 7.02862, accuracy: 0.39500\n","Epoch: 2/50, step: 76/83, loss: 7.02034, accuracy: 0.39309\n","Epoch: 2/50, step: 77/83, loss: 7.03130, accuracy: 0.39448\n","Epoch: 2/50, step: 78/83, loss: 7.05782, accuracy: 0.39263\n","Epoch: 2/50, step: 79/83, loss: 7.04771, accuracy: 0.39399\n","Epoch: 2/50, step: 80/83, loss: 7.04838, accuracy: 0.39531\n","Epoch: 2/50, step: 81/83, loss: 7.04813, accuracy: 0.39660\n","Epoch: 2/50, step: 82/83, loss: 7.04899, accuracy: 0.39634\n","Epoch: 2/50, step: 83/83, loss: 7.06231, accuracy: 0.39574\n","Epoch: 2/50, train loss: 7.06231, train accuracy: 0.39574, valid loss: 6.89841, valid accuracy: 0.51131\n","Epoch: 3/50, step: 1/83, loss: 6.99228, accuracy: 0.62500\n","Epoch: 3/50, step: 2/83, loss: 6.20257, accuracy: 0.50000\n","Epoch: 3/50, step: 3/83, loss: 5.96569, accuracy: 0.45833\n","Epoch: 3/50, step: 4/83, loss: 6.03409, accuracy: 0.46875\n","Epoch: 3/50, step: 5/83, loss: 6.59768, accuracy: 0.47500\n","Epoch: 3/50, step: 6/83, loss: 6.93317, accuracy: 0.47917\n","Epoch: 3/50, step: 7/83, loss: 6.86584, accuracy: 0.50000\n","Epoch: 3/50, step: 8/83, loss: 6.68369, accuracy: 0.50000\n","Epoch: 3/50, step: 9/83, loss: 6.70765, accuracy: 0.52778\n","Epoch: 3/50, step: 10/83, loss: 6.83747, accuracy: 0.52500\n","Epoch: 3/50, step: 11/83, loss: 6.79684, accuracy: 0.52273\n","Epoch: 3/50, step: 12/83, loss: 6.77547, accuracy: 0.53125\n","Epoch: 3/50, step: 13/83, loss: 6.80138, accuracy: 0.52885\n","Epoch: 3/50, step: 14/83, loss: 6.76285, accuracy: 0.50000\n","Epoch: 3/50, step: 15/83, loss: 6.81298, accuracy: 0.47500\n","Epoch: 3/50, step: 16/83, loss: 6.59240, accuracy: 0.48438\n","Epoch: 3/50, step: 17/83, loss: 6.47875, accuracy: 0.47059\n","Epoch: 3/50, step: 18/83, loss: 6.55118, accuracy: 0.45833\n","Epoch: 3/50, step: 19/83, loss: 6.58821, accuracy: 0.44737\n","Epoch: 3/50, step: 20/83, loss: 6.56579, accuracy: 0.45625\n","Epoch: 3/50, step: 21/83, loss: 6.59850, accuracy: 0.45238\n","Epoch: 3/50, step: 22/83, loss: 6.62089, accuracy: 0.44886\n","Epoch: 3/50, step: 23/83, loss: 6.66460, accuracy: 0.45109\n","Epoch: 3/50, step: 24/83, loss: 6.60800, accuracy: 0.46354\n","Epoch: 3/50, step: 25/83, loss: 6.63610, accuracy: 0.45500\n","Epoch: 3/50, step: 26/83, loss: 6.55405, accuracy: 0.44712\n","Epoch: 3/50, step: 27/83, loss: 6.59714, accuracy: 0.44444\n","Epoch: 3/50, step: 28/83, loss: 6.63148, accuracy: 0.43304\n","Epoch: 3/50, step: 29/83, loss: 6.66038, accuracy: 0.42672\n","Epoch: 3/50, step: 30/83, loss: 6.61708, accuracy: 0.43333\n","Epoch: 3/50, step: 31/83, loss: 6.65377, accuracy: 0.42339\n","Epoch: 3/50, step: 32/83, loss: 6.64105, accuracy: 0.42188\n","Epoch: 3/50, step: 33/83, loss: 6.70318, accuracy: 0.42424\n","Epoch: 3/50, step: 34/83, loss: 6.72012, accuracy: 0.41912\n","Epoch: 3/50, step: 35/83, loss: 6.73715, accuracy: 0.41786\n","Epoch: 3/50, step: 36/83, loss: 6.79972, accuracy: 0.40972\n","Epoch: 3/50, step: 37/83, loss: 6.80741, accuracy: 0.41554\n","Epoch: 3/50, step: 38/83, loss: 6.79398, accuracy: 0.42105\n","Epoch: 3/50, step: 39/83, loss: 6.75402, accuracy: 0.41667\n","Epoch: 3/50, step: 40/83, loss: 6.73628, accuracy: 0.41875\n","Epoch: 3/50, step: 41/83, loss: 6.72960, accuracy: 0.41159\n","Epoch: 3/50, step: 42/83, loss: 6.72923, accuracy: 0.41071\n","Epoch: 3/50, step: 43/83, loss: 6.73239, accuracy: 0.41279\n","Epoch: 3/50, step: 44/83, loss: 6.70201, accuracy: 0.41761\n","Epoch: 3/50, step: 45/83, loss: 6.72439, accuracy: 0.42222\n","Epoch: 3/50, step: 46/83, loss: 6.72889, accuracy: 0.42120\n","Epoch: 3/50, step: 47/83, loss: 6.69125, accuracy: 0.42287\n","Epoch: 3/50, step: 48/83, loss: 6.71019, accuracy: 0.41667\n","Epoch: 3/50, step: 49/83, loss: 6.68407, accuracy: 0.41837\n","Epoch: 3/50, step: 50/83, loss: 6.70025, accuracy: 0.41000\n","Epoch: 3/50, step: 51/83, loss: 6.76337, accuracy: 0.40441\n","Epoch: 3/50, step: 52/83, loss: 6.78945, accuracy: 0.39904\n","Epoch: 3/50, step: 53/83, loss: 6.83622, accuracy: 0.39151\n","Epoch: 3/50, step: 54/83, loss: 6.82471, accuracy: 0.39120\n","Epoch: 3/50, step: 55/83, loss: 6.81386, accuracy: 0.38636\n","Epoch: 3/50, step: 56/83, loss: 6.80261, accuracy: 0.38616\n","Epoch: 3/50, step: 57/83, loss: 6.77532, accuracy: 0.38596\n","Epoch: 3/50, step: 58/83, loss: 6.78319, accuracy: 0.38793\n","Epoch: 3/50, step: 59/83, loss: 6.83638, accuracy: 0.38559\n","Epoch: 3/50, step: 60/83, loss: 6.81301, accuracy: 0.38333\n","Epoch: 3/50, step: 61/83, loss: 6.82830, accuracy: 0.38115\n","Epoch: 3/50, step: 62/83, loss: 6.82047, accuracy: 0.38306\n","Epoch: 3/50, step: 63/83, loss: 6.83724, accuracy: 0.38492\n","Epoch: 3/50, step: 64/83, loss: 6.82847, accuracy: 0.38281\n","Epoch: 3/50, step: 65/83, loss: 6.81624, accuracy: 0.38462\n","Epoch: 3/50, step: 66/83, loss: 6.81592, accuracy: 0.38447\n","Epoch: 3/50, step: 67/83, loss: 6.82401, accuracy: 0.39366\n","Epoch: 3/50, step: 68/83, loss: 6.82511, accuracy: 0.39522\n","Epoch: 3/50, step: 69/83, loss: 6.87431, accuracy: 0.38949\n","Epoch: 3/50, step: 70/83, loss: 6.83865, accuracy: 0.39464\n","Epoch: 3/50, step: 71/83, loss: 6.84246, accuracy: 0.39613\n","Epoch: 3/50, step: 72/83, loss: 6.84563, accuracy: 0.39583\n","Epoch: 3/50, step: 73/83, loss: 6.88665, accuracy: 0.39555\n","Epoch: 3/50, step: 74/83, loss: 6.88023, accuracy: 0.40034\n","Epoch: 3/50, step: 75/83, loss: 6.89475, accuracy: 0.40167\n","Epoch: 3/50, step: 76/83, loss: 6.87797, accuracy: 0.39967\n","Epoch: 3/50, step: 77/83, loss: 6.86112, accuracy: 0.39935\n","Epoch: 3/50, step: 78/83, loss: 6.84642, accuracy: 0.39904\n","Epoch: 3/50, step: 79/83, loss: 6.83201, accuracy: 0.40190\n","Epoch: 3/50, step: 80/83, loss: 6.84274, accuracy: 0.40156\n","Epoch: 3/50, step: 81/83, loss: 6.84397, accuracy: 0.40123\n","Epoch: 3/50, step: 82/83, loss: 6.82588, accuracy: 0.40396\n","Epoch: 3/50, step: 83/83, loss: 6.84788, accuracy: 0.40335\n","Epoch: 3/50, train loss: 6.84788, train accuracy: 0.40335, valid loss: 7.63277, valid accuracy: 0.51131\n","Epoch: 4/50, step: 1/83, loss: 7.26634, accuracy: 0.25000\n","Epoch: 4/50, step: 2/83, loss: 7.30080, accuracy: 0.25000\n","Epoch: 4/50, step: 3/83, loss: 6.92598, accuracy: 0.25000\n","Epoch: 4/50, step: 4/83, loss: 7.41665, accuracy: 0.31250\n","Epoch: 4/50, step: 5/83, loss: 7.37449, accuracy: 0.37500\n","Epoch: 4/50, step: 6/83, loss: 7.02640, accuracy: 0.39583\n","Epoch: 4/50, step: 7/83, loss: 6.79033, accuracy: 0.44643\n","Epoch: 4/50, step: 8/83, loss: 6.70058, accuracy: 0.40625\n","Epoch: 4/50, step: 9/83, loss: 6.39225, accuracy: 0.41667\n","Epoch: 4/50, step: 10/83, loss: 6.37423, accuracy: 0.45000\n","Epoch: 4/50, step: 11/83, loss: 6.31846, accuracy: 0.46591\n","Epoch: 4/50, step: 12/83, loss: 6.37462, accuracy: 0.47917\n","Epoch: 4/50, step: 13/83, loss: 6.24531, accuracy: 0.47115\n","Epoch: 4/50, step: 14/83, loss: 6.23505, accuracy: 0.48214\n","Epoch: 4/50, step: 15/83, loss: 6.05773, accuracy: 0.47500\n","Epoch: 4/50, step: 16/83, loss: 6.17496, accuracy: 0.46094\n","Epoch: 4/50, step: 17/83, loss: 6.16875, accuracy: 0.46324\n","Epoch: 4/50, step: 18/83, loss: 6.08272, accuracy: 0.45833\n","Epoch: 4/50, step: 19/83, loss: 6.27506, accuracy: 0.44737\n","Epoch: 4/50, step: 20/83, loss: 6.23790, accuracy: 0.43125\n","Epoch: 4/50, step: 21/83, loss: 6.15756, accuracy: 0.42857\n","Epoch: 4/50, step: 22/83, loss: 6.22881, accuracy: 0.42614\n","Epoch: 4/50, step: 23/83, loss: 6.17230, accuracy: 0.41848\n","Epoch: 4/50, step: 24/83, loss: 6.25085, accuracy: 0.41146\n","Epoch: 4/50, step: 25/83, loss: 6.29898, accuracy: 0.42000\n","Epoch: 4/50, step: 26/83, loss: 6.23260, accuracy: 0.42788\n","Epoch: 4/50, step: 27/83, loss: 6.13750, accuracy: 0.43056\n","Epoch: 4/50, step: 28/83, loss: 6.14935, accuracy: 0.43750\n","Epoch: 4/50, step: 29/83, loss: 6.11995, accuracy: 0.43966\n","Epoch: 4/50, step: 30/83, loss: 6.15692, accuracy: 0.43750\n","Epoch: 4/50, step: 31/83, loss: 6.13848, accuracy: 0.43145\n","Epoch: 4/50, step: 32/83, loss: 6.13783, accuracy: 0.42969\n","Epoch: 4/50, step: 33/83, loss: 6.13438, accuracy: 0.42045\n","Epoch: 4/50, step: 34/83, loss: 6.15963, accuracy: 0.42279\n","Epoch: 4/50, step: 35/83, loss: 6.15986, accuracy: 0.42500\n","Epoch: 4/50, step: 36/83, loss: 6.21338, accuracy: 0.42014\n","Epoch: 4/50, step: 37/83, loss: 6.21258, accuracy: 0.42568\n","Epoch: 4/50, step: 38/83, loss: 6.19901, accuracy: 0.42763\n","Epoch: 4/50, step: 39/83, loss: 6.24747, accuracy: 0.41667\n","Epoch: 4/50, step: 40/83, loss: 6.20658, accuracy: 0.41563\n","Epoch: 4/50, step: 41/83, loss: 6.16335, accuracy: 0.41463\n","Epoch: 4/50, step: 42/83, loss: 6.16441, accuracy: 0.41369\n","Epoch: 4/50, step: 43/83, loss: 6.18142, accuracy: 0.40988\n","Epoch: 4/50, step: 44/83, loss: 6.13015, accuracy: 0.40909\n","Epoch: 4/50, step: 45/83, loss: 6.11331, accuracy: 0.40556\n","Epoch: 4/50, step: 46/83, loss: 6.10411, accuracy: 0.40217\n","Epoch: 4/50, step: 47/83, loss: 6.10899, accuracy: 0.39628\n","Epoch: 4/50, step: 48/83, loss: 6.17167, accuracy: 0.39844\n","Epoch: 4/50, step: 49/83, loss: 6.17388, accuracy: 0.39796\n","Epoch: 4/50, step: 50/83, loss: 6.19720, accuracy: 0.39500\n","Epoch: 4/50, step: 51/83, loss: 6.20946, accuracy: 0.39461\n","Epoch: 4/50, step: 52/83, loss: 6.20674, accuracy: 0.39663\n","Epoch: 4/50, step: 53/83, loss: 6.17590, accuracy: 0.39623\n","Epoch: 4/50, step: 54/83, loss: 6.13640, accuracy: 0.39352\n","Epoch: 4/50, step: 55/83, loss: 6.13914, accuracy: 0.39091\n","Epoch: 4/50, step: 56/83, loss: 6.10952, accuracy: 0.39286\n","Epoch: 4/50, step: 57/83, loss: 6.09531, accuracy: 0.39693\n","Epoch: 4/50, step: 58/83, loss: 6.08238, accuracy: 0.39871\n","Epoch: 4/50, step: 59/83, loss: 6.05565, accuracy: 0.40042\n","Epoch: 4/50, step: 60/83, loss: 6.07661, accuracy: 0.39792\n","Epoch: 4/50, step: 61/83, loss: 6.08917, accuracy: 0.40574\n","Epoch: 4/50, step: 62/83, loss: 6.11678, accuracy: 0.40927\n","Epoch: 4/50, step: 63/83, loss: 6.13574, accuracy: 0.40873\n","Epoch: 4/50, step: 64/83, loss: 6.13462, accuracy: 0.40820\n","Epoch: 4/50, step: 65/83, loss: 6.15126, accuracy: 0.41154\n","Epoch: 4/50, step: 66/83, loss: 6.15500, accuracy: 0.40909\n","Epoch: 4/50, step: 67/83, loss: 6.11371, accuracy: 0.41231\n","Epoch: 4/50, step: 68/83, loss: 6.11330, accuracy: 0.41176\n","Epoch: 4/50, step: 69/83, loss: 6.11718, accuracy: 0.40942\n","Epoch: 4/50, step: 70/83, loss: 6.12156, accuracy: 0.41250\n","Epoch: 4/50, step: 71/83, loss: 6.14806, accuracy: 0.41549\n","Epoch: 4/50, step: 72/83, loss: 6.16133, accuracy: 0.41667\n","Epoch: 4/50, step: 73/83, loss: 6.11967, accuracy: 0.41267\n","Epoch: 4/50, step: 74/83, loss: 6.12362, accuracy: 0.41047\n","Epoch: 4/50, step: 75/83, loss: 6.11109, accuracy: 0.41333\n","Epoch: 4/50, step: 76/83, loss: 6.13259, accuracy: 0.41283\n","Epoch: 4/50, step: 77/83, loss: 6.12377, accuracy: 0.41396\n","Epoch: 4/50, step: 78/83, loss: 6.09951, accuracy: 0.41346\n","Epoch: 4/50, step: 79/83, loss: 6.09069, accuracy: 0.41139\n","Epoch: 4/50, step: 80/83, loss: 6.12584, accuracy: 0.41094\n","Epoch: 4/50, step: 81/83, loss: 6.10912, accuracy: 0.41049\n","Epoch: 4/50, step: 82/83, loss: 6.13297, accuracy: 0.40854\n","Epoch: 4/50, step: 83/83, loss: 6.15205, accuracy: 0.40944\n","Epoch: 4/50, train loss: 6.15205, train accuracy: 0.40944, valid loss: 5.03785, valid accuracy: 0.53846\n","Epoch: 5/50, step: 1/83, loss: 5.49216, accuracy: 0.37500\n","Epoch: 5/50, step: 2/83, loss: 6.44036, accuracy: 0.31250\n","Epoch: 5/50, step: 3/83, loss: 6.32475, accuracy: 0.33333\n","Epoch: 5/50, step: 4/83, loss: 6.35950, accuracy: 0.37500\n","Epoch: 5/50, step: 5/83, loss: 6.54265, accuracy: 0.37500\n","Epoch: 5/50, step: 6/83, loss: 6.49525, accuracy: 0.39583\n","Epoch: 5/50, step: 7/83, loss: 6.75897, accuracy: 0.39286\n","Epoch: 5/50, step: 8/83, loss: 6.72059, accuracy: 0.39062\n","Epoch: 5/50, step: 9/83, loss: 7.04043, accuracy: 0.34722\n","Epoch: 5/50, step: 10/83, loss: 7.11752, accuracy: 0.35000\n","Epoch: 5/50, step: 11/83, loss: 7.26556, accuracy: 0.37500\n","Epoch: 5/50, step: 12/83, loss: 6.94712, accuracy: 0.40625\n","Epoch: 5/50, step: 13/83, loss: 6.90210, accuracy: 0.40385\n","Epoch: 5/50, step: 14/83, loss: 6.85712, accuracy: 0.40179\n","Epoch: 5/50, step: 15/83, loss: 6.82175, accuracy: 0.39167\n","Epoch: 5/50, step: 16/83, loss: 6.89231, accuracy: 0.38281\n","Epoch: 5/50, step: 17/83, loss: 6.68631, accuracy: 0.40441\n","Epoch: 5/50, step: 18/83, loss: 6.63034, accuracy: 0.39583\n","Epoch: 5/50, step: 19/83, loss: 6.60653, accuracy: 0.40789\n","Epoch: 5/50, step: 20/83, loss: 6.59614, accuracy: 0.40625\n","Epoch: 5/50, step: 21/83, loss: 6.53821, accuracy: 0.39881\n","Epoch: 5/50, step: 22/83, loss: 6.51357, accuracy: 0.40341\n","Epoch: 5/50, step: 23/83, loss: 6.55002, accuracy: 0.39674\n","Epoch: 5/50, step: 24/83, loss: 6.50566, accuracy: 0.39583\n","Epoch: 5/50, step: 25/83, loss: 6.59746, accuracy: 0.40000\n","Epoch: 5/50, step: 26/83, loss: 6.50351, accuracy: 0.39904\n","Epoch: 5/50, step: 27/83, loss: 6.55793, accuracy: 0.39815\n","Epoch: 5/50, step: 28/83, loss: 6.63776, accuracy: 0.40179\n","Epoch: 5/50, step: 29/83, loss: 6.62856, accuracy: 0.40086\n","Epoch: 5/50, step: 30/83, loss: 6.61174, accuracy: 0.40000\n","Epoch: 5/50, step: 31/83, loss: 6.60810, accuracy: 0.40323\n","Epoch: 5/50, step: 32/83, loss: 6.62724, accuracy: 0.39844\n","Epoch: 5/50, step: 33/83, loss: 6.61671, accuracy: 0.39773\n","Epoch: 5/50, step: 34/83, loss: 6.60509, accuracy: 0.39338\n","Epoch: 5/50, step: 35/83, loss: 6.56567, accuracy: 0.40000\n","Epoch: 5/50, step: 36/83, loss: 6.55778, accuracy: 0.39931\n","Epoch: 5/50, step: 37/83, loss: 6.54823, accuracy: 0.40541\n","Epoch: 5/50, step: 38/83, loss: 6.54069, accuracy: 0.40789\n","Epoch: 5/50, step: 39/83, loss: 6.46781, accuracy: 0.40705\n","Epoch: 5/50, step: 40/83, loss: 6.45202, accuracy: 0.41250\n","Epoch: 5/50, step: 41/83, loss: 6.48333, accuracy: 0.41463\n","Epoch: 5/50, step: 42/83, loss: 6.47910, accuracy: 0.41964\n","Epoch: 5/50, step: 43/83, loss: 6.43439, accuracy: 0.41860\n","Epoch: 5/50, step: 44/83, loss: 6.42222, accuracy: 0.41477\n","Epoch: 5/50, step: 45/83, loss: 6.39426, accuracy: 0.42222\n","Epoch: 5/50, step: 46/83, loss: 6.41379, accuracy: 0.41576\n","Epoch: 5/50, step: 47/83, loss: 6.46739, accuracy: 0.41489\n","Epoch: 5/50, step: 48/83, loss: 6.47908, accuracy: 0.41406\n","Epoch: 5/50, step: 49/83, loss: 6.43669, accuracy: 0.42347\n","Epoch: 5/50, step: 50/83, loss: 6.35952, accuracy: 0.43000\n","Epoch: 5/50, step: 51/83, loss: 6.37671, accuracy: 0.42647\n","Epoch: 5/50, step: 52/83, loss: 6.37963, accuracy: 0.42788\n","Epoch: 5/50, step: 53/83, loss: 6.34873, accuracy: 0.42689\n","Epoch: 5/50, step: 54/83, loss: 6.31230, accuracy: 0.42361\n","Epoch: 5/50, step: 55/83, loss: 6.37942, accuracy: 0.42045\n","Epoch: 5/50, step: 56/83, loss: 6.42247, accuracy: 0.41518\n","Epoch: 5/50, step: 57/83, loss: 6.41503, accuracy: 0.41667\n","Epoch: 5/50, step: 58/83, loss: 6.42676, accuracy: 0.41595\n","Epoch: 5/50, step: 59/83, loss: 6.39207, accuracy: 0.41737\n","Epoch: 5/50, step: 60/83, loss: 6.33246, accuracy: 0.42083\n","Epoch: 5/50, step: 61/83, loss: 6.32703, accuracy: 0.42623\n","Epoch: 5/50, step: 62/83, loss: 6.29766, accuracy: 0.42742\n","Epoch: 5/50, step: 63/83, loss: 6.28557, accuracy: 0.42262\n","Epoch: 5/50, step: 64/83, loss: 6.30977, accuracy: 0.42773\n","Epoch: 5/50, step: 65/83, loss: 6.28210, accuracy: 0.42692\n","Epoch: 5/50, step: 66/83, loss: 6.31033, accuracy: 0.42424\n","Epoch: 5/50, step: 67/83, loss: 6.30500, accuracy: 0.42724\n","Epoch: 5/50, step: 68/83, loss: 6.28809, accuracy: 0.43199\n","Epoch: 5/50, step: 69/83, loss: 6.26025, accuracy: 0.43297\n","Epoch: 5/50, step: 70/83, loss: 6.23679, accuracy: 0.43393\n","Epoch: 5/50, step: 71/83, loss: 6.23258, accuracy: 0.43486\n","Epoch: 5/50, step: 72/83, loss: 6.24212, accuracy: 0.43576\n","Epoch: 5/50, step: 73/83, loss: 6.22718, accuracy: 0.43664\n","Epoch: 5/50, step: 74/83, loss: 6.27484, accuracy: 0.43412\n","Epoch: 5/50, step: 75/83, loss: 6.27350, accuracy: 0.43333\n","Epoch: 5/50, step: 76/83, loss: 6.24811, accuracy: 0.43421\n","Epoch: 5/50, step: 77/83, loss: 6.22589, accuracy: 0.43344\n","Epoch: 5/50, step: 78/83, loss: 6.20281, accuracy: 0.43429\n","Epoch: 5/50, step: 79/83, loss: 6.21380, accuracy: 0.43196\n","Epoch: 5/50, step: 80/83, loss: 6.19154, accuracy: 0.43437\n","Epoch: 5/50, step: 81/83, loss: 6.16000, accuracy: 0.43519\n","Epoch: 5/50, step: 82/83, loss: 6.16048, accuracy: 0.43293\n","Epoch: 5/50, step: 83/83, loss: 6.17915, accuracy: 0.43379\n","Epoch: 5/50, train loss: 6.17915, train accuracy: 0.43379, valid loss: 1.78050, valid accuracy: 0.52489\n","Epoch: 6/50, step: 1/83, loss: 7.66850, accuracy: 0.37500\n","Epoch: 6/50, step: 2/83, loss: 5.62063, accuracy: 0.56250\n","Epoch: 6/50, step: 3/83, loss: 5.48039, accuracy: 0.54167\n","Epoch: 6/50, step: 4/83, loss: 5.14619, accuracy: 0.59375\n","Epoch: 6/50, step: 5/83, loss: 5.56756, accuracy: 0.57500\n","Epoch: 6/50, step: 6/83, loss: 5.38721, accuracy: 0.54167\n","Epoch: 6/50, step: 7/83, loss: 5.27629, accuracy: 0.51786\n","Epoch: 6/50, step: 8/83, loss: 5.18143, accuracy: 0.46875\n","Epoch: 6/50, step: 9/83, loss: 5.33446, accuracy: 0.45833\n","Epoch: 6/50, step: 10/83, loss: 5.32555, accuracy: 0.46250\n","Epoch: 6/50, step: 11/83, loss: 5.48897, accuracy: 0.46591\n","Epoch: 6/50, step: 12/83, loss: 5.70394, accuracy: 0.45833\n","Epoch: 6/50, step: 13/83, loss: 5.74188, accuracy: 0.47115\n","Epoch: 6/50, step: 14/83, loss: 5.72097, accuracy: 0.45536\n","Epoch: 6/50, step: 15/83, loss: 5.67086, accuracy: 0.45833\n","Epoch: 6/50, step: 16/83, loss: 5.64125, accuracy: 0.46094\n","Epoch: 6/50, step: 17/83, loss: 5.68033, accuracy: 0.47059\n","Epoch: 6/50, step: 18/83, loss: 5.61043, accuracy: 0.46528\n","Epoch: 6/50, step: 19/83, loss: 5.49449, accuracy: 0.46711\n","Epoch: 6/50, step: 20/83, loss: 5.62147, accuracy: 0.46875\n","Epoch: 6/50, step: 21/83, loss: 5.65952, accuracy: 0.47024\n","Epoch: 6/50, step: 22/83, loss: 5.64851, accuracy: 0.47727\n","Epoch: 6/50, step: 23/83, loss: 5.59924, accuracy: 0.47283\n","Epoch: 6/50, step: 24/83, loss: 5.51360, accuracy: 0.47396\n","Epoch: 6/50, step: 25/83, loss: 5.50313, accuracy: 0.47000\n","Epoch: 6/50, step: 26/83, loss: 5.48372, accuracy: 0.48558\n","Epoch: 6/50, step: 27/83, loss: 5.50748, accuracy: 0.49537\n","Epoch: 6/50, step: 28/83, loss: 5.60175, accuracy: 0.49554\n","Epoch: 6/50, step: 29/83, loss: 5.59601, accuracy: 0.49138\n","Epoch: 6/50, step: 30/83, loss: 5.51104, accuracy: 0.50417\n","Epoch: 6/50, step: 31/83, loss: 5.53208, accuracy: 0.49597\n","Epoch: 6/50, step: 32/83, loss: 5.50552, accuracy: 0.49609\n","Epoch: 6/50, step: 33/83, loss: 5.52675, accuracy: 0.48864\n","Epoch: 6/50, step: 34/83, loss: 5.52070, accuracy: 0.49265\n","Epoch: 6/50, step: 35/83, loss: 5.49081, accuracy: 0.50357\n","Epoch: 6/50, step: 36/83, loss: 5.58508, accuracy: 0.50000\n","Epoch: 6/50, step: 37/83, loss: 5.56538, accuracy: 0.50338\n","Epoch: 6/50, step: 38/83, loss: 5.60063, accuracy: 0.50329\n","Epoch: 6/50, step: 39/83, loss: 5.56324, accuracy: 0.51603\n","Epoch: 6/50, step: 40/83, loss: 5.60006, accuracy: 0.51250\n","Epoch: 6/50, step: 41/83, loss: 5.67281, accuracy: 0.50610\n","Epoch: 6/50, step: 42/83, loss: 5.70544, accuracy: 0.50595\n","Epoch: 6/50, step: 43/83, loss: 5.74199, accuracy: 0.50000\n","Epoch: 6/50, step: 44/83, loss: 5.75365, accuracy: 0.50284\n","Epoch: 6/50, step: 45/83, loss: 5.76166, accuracy: 0.50278\n","Epoch: 6/50, step: 46/83, loss: 5.78332, accuracy: 0.49728\n","Epoch: 6/50, step: 47/83, loss: 5.82106, accuracy: 0.49468\n","Epoch: 6/50, step: 48/83, loss: 5.81845, accuracy: 0.49479\n","Epoch: 6/50, step: 49/83, loss: 5.88060, accuracy: 0.49490\n","Epoch: 6/50, step: 50/83, loss: 5.85742, accuracy: 0.49000\n","Epoch: 6/50, step: 51/83, loss: 5.82753, accuracy: 0.49020\n","Epoch: 6/50, step: 52/83, loss: 5.83270, accuracy: 0.48317\n","Epoch: 6/50, step: 53/83, loss: 5.83565, accuracy: 0.48821\n","Epoch: 6/50, step: 54/83, loss: 5.79754, accuracy: 0.48380\n","Epoch: 6/50, step: 55/83, loss: 5.79100, accuracy: 0.48409\n","Epoch: 6/50, step: 56/83, loss: 5.81304, accuracy: 0.48214\n","Epoch: 6/50, step: 57/83, loss: 5.81950, accuracy: 0.47807\n","Epoch: 6/50, step: 58/83, loss: 5.80659, accuracy: 0.47629\n","Epoch: 6/50, step: 59/83, loss: 5.81653, accuracy: 0.47458\n","Epoch: 6/50, step: 60/83, loss: 5.85448, accuracy: 0.47083\n","Epoch: 6/50, step: 61/83, loss: 5.81631, accuracy: 0.47336\n","Epoch: 6/50, step: 62/83, loss: 5.81012, accuracy: 0.47581\n","Epoch: 6/50, step: 63/83, loss: 5.81733, accuracy: 0.47619\n","Epoch: 6/50, step: 64/83, loss: 5.81016, accuracy: 0.47852\n","Epoch: 6/50, step: 65/83, loss: 5.78556, accuracy: 0.48269\n","Epoch: 6/50, step: 66/83, loss: 5.79143, accuracy: 0.48485\n","Epoch: 6/50, step: 67/83, loss: 5.74316, accuracy: 0.48694\n","Epoch: 6/50, step: 68/83, loss: 5.72308, accuracy: 0.48713\n","Epoch: 6/50, step: 69/83, loss: 5.71467, accuracy: 0.48551\n","Epoch: 6/50, step: 70/83, loss: 5.68246, accuracy: 0.48929\n","Epoch: 6/50, step: 71/83, loss: 5.67649, accuracy: 0.49120\n","Epoch: 6/50, step: 72/83, loss: 5.64860, accuracy: 0.48785\n","Epoch: 6/50, step: 73/83, loss: 5.63403, accuracy: 0.48801\n","Epoch: 6/50, step: 74/83, loss: 5.65885, accuracy: 0.48480\n","Epoch: 6/50, step: 75/83, loss: 5.65596, accuracy: 0.48333\n","Epoch: 6/50, step: 76/83, loss: 5.67242, accuracy: 0.48191\n","Epoch: 6/50, step: 77/83, loss: 5.65416, accuracy: 0.48377\n","Epoch: 6/50, step: 78/83, loss: 5.64930, accuracy: 0.48237\n","Epoch: 6/50, step: 79/83, loss: 5.64715, accuracy: 0.48101\n","Epoch: 6/50, step: 80/83, loss: 5.63144, accuracy: 0.48125\n","Epoch: 6/50, step: 81/83, loss: 5.64161, accuracy: 0.47994\n","Epoch: 6/50, step: 82/83, loss: 5.63955, accuracy: 0.47561\n","Epoch: 6/50, step: 83/83, loss: 5.67768, accuracy: 0.47489\n","Epoch: 6/50, train loss: 5.67768, train accuracy: 0.47489, valid loss: 2.08780, valid accuracy: 0.38009\n","Epoch: 7/50, step: 1/83, loss: 5.04213, accuracy: 0.62500\n","Epoch: 7/50, step: 2/83, loss: 4.83494, accuracy: 0.50000\n","Epoch: 7/50, step: 3/83, loss: 4.41598, accuracy: 0.41667\n","Epoch: 7/50, step: 4/83, loss: 4.42786, accuracy: 0.40625\n","Epoch: 7/50, step: 5/83, loss: 4.77542, accuracy: 0.45000\n","Epoch: 7/50, step: 6/83, loss: 5.18118, accuracy: 0.39583\n","Epoch: 7/50, step: 7/83, loss: 5.06563, accuracy: 0.42857\n","Epoch: 7/50, step: 8/83, loss: 5.56698, accuracy: 0.39062\n","Epoch: 7/50, step: 9/83, loss: 5.70521, accuracy: 0.40278\n","Epoch: 7/50, step: 10/83, loss: 5.48365, accuracy: 0.42500\n","Epoch: 7/50, step: 11/83, loss: 5.38304, accuracy: 0.42045\n","Epoch: 7/50, step: 12/83, loss: 5.38483, accuracy: 0.41667\n","Epoch: 7/50, step: 13/83, loss: 5.38907, accuracy: 0.42308\n","Epoch: 7/50, step: 14/83, loss: 5.42240, accuracy: 0.43750\n","Epoch: 7/50, step: 15/83, loss: 5.29010, accuracy: 0.45000\n","Epoch: 7/50, step: 16/83, loss: 5.30183, accuracy: 0.42969\n","Epoch: 7/50, step: 17/83, loss: 5.24717, accuracy: 0.44118\n","Epoch: 7/50, step: 18/83, loss: 5.24631, accuracy: 0.43750\n","Epoch: 7/50, step: 19/83, loss: 5.34564, accuracy: 0.44079\n","Epoch: 7/50, step: 20/83, loss: 5.28774, accuracy: 0.43125\n","Epoch: 7/50, step: 21/83, loss: 5.32507, accuracy: 0.43452\n","Epoch: 7/50, step: 22/83, loss: 5.27544, accuracy: 0.43182\n","Epoch: 7/50, step: 23/83, loss: 5.37607, accuracy: 0.42935\n","Epoch: 7/50, step: 24/83, loss: 5.32769, accuracy: 0.42708\n","Epoch: 7/50, step: 25/83, loss: 5.26120, accuracy: 0.42500\n","Epoch: 7/50, step: 26/83, loss: 5.17607, accuracy: 0.42308\n","Epoch: 7/50, step: 27/83, loss: 5.20146, accuracy: 0.42593\n","Epoch: 7/50, step: 28/83, loss: 5.20129, accuracy: 0.43750\n","Epoch: 7/50, step: 29/83, loss: 5.17559, accuracy: 0.44397\n","Epoch: 7/50, step: 30/83, loss: 5.11526, accuracy: 0.45417\n","Epoch: 7/50, step: 31/83, loss: 5.11916, accuracy: 0.45968\n","Epoch: 7/50, step: 32/83, loss: 5.13383, accuracy: 0.46094\n","Epoch: 7/50, step: 33/83, loss: 5.11650, accuracy: 0.46212\n","Epoch: 7/50, step: 34/83, loss: 5.14362, accuracy: 0.46691\n","Epoch: 7/50, step: 35/83, loss: 5.09819, accuracy: 0.47143\n","Epoch: 7/50, step: 36/83, loss: 5.08685, accuracy: 0.47569\n","Epoch: 7/50, step: 37/83, loss: 5.06043, accuracy: 0.48311\n","Epoch: 7/50, step: 38/83, loss: 5.08792, accuracy: 0.48684\n","Epoch: 7/50, step: 39/83, loss: 5.11077, accuracy: 0.48718\n","Epoch: 7/50, step: 40/83, loss: 5.18421, accuracy: 0.48750\n","Epoch: 7/50, step: 41/83, loss: 5.16314, accuracy: 0.49085\n","Epoch: 7/50, step: 42/83, loss: 5.14410, accuracy: 0.48810\n","Epoch: 7/50, step: 43/83, loss: 5.18831, accuracy: 0.49419\n","Epoch: 7/50, step: 44/83, loss: 5.18761, accuracy: 0.50000\n","Epoch: 7/50, step: 45/83, loss: 5.23492, accuracy: 0.49444\n","Epoch: 7/50, step: 46/83, loss: 5.21970, accuracy: 0.49185\n","Epoch: 7/50, step: 47/83, loss: 5.18510, accuracy: 0.49468\n","Epoch: 7/50, step: 48/83, loss: 5.11611, accuracy: 0.49740\n","Epoch: 7/50, step: 49/83, loss: 5.18995, accuracy: 0.49235\n","Epoch: 7/50, step: 50/83, loss: 5.16120, accuracy: 0.49000\n","Epoch: 7/50, step: 51/83, loss: 5.12774, accuracy: 0.49020\n","Epoch: 7/50, step: 52/83, loss: 5.14717, accuracy: 0.49038\n","Epoch: 7/50, step: 53/83, loss: 5.23442, accuracy: 0.48821\n","Epoch: 7/50, step: 54/83, loss: 5.23716, accuracy: 0.49074\n","Epoch: 7/50, step: 55/83, loss: 5.24580, accuracy: 0.49545\n","Epoch: 7/50, step: 56/83, loss: 5.23462, accuracy: 0.49554\n","Epoch: 7/50, step: 57/83, loss: 5.21633, accuracy: 0.50000\n","Epoch: 7/50, step: 58/83, loss: 5.23766, accuracy: 0.50647\n","Epoch: 7/50, step: 59/83, loss: 5.23358, accuracy: 0.51059\n","Epoch: 7/50, step: 60/83, loss: 5.22422, accuracy: 0.51458\n","Epoch: 7/50, step: 61/83, loss: 5.23580, accuracy: 0.51434\n","Epoch: 7/50, step: 62/83, loss: 5.24009, accuracy: 0.51411\n","Epoch: 7/50, step: 63/83, loss: 5.23991, accuracy: 0.51984\n","Epoch: 7/50, step: 64/83, loss: 5.22858, accuracy: 0.51953\n","Epoch: 7/50, step: 65/83, loss: 5.23191, accuracy: 0.51731\n","Epoch: 7/50, step: 66/83, loss: 5.25730, accuracy: 0.51894\n","Epoch: 7/50, step: 67/83, loss: 5.23371, accuracy: 0.52052\n","Epoch: 7/50, step: 68/83, loss: 5.19228, accuracy: 0.52390\n","Epoch: 7/50, step: 69/83, loss: 5.18144, accuracy: 0.52174\n","Epoch: 7/50, step: 70/83, loss: 5.15659, accuracy: 0.52321\n","Epoch: 7/50, step: 71/83, loss: 5.17292, accuracy: 0.52289\n","Epoch: 7/50, step: 72/83, loss: 5.15012, accuracy: 0.52257\n","Epoch: 7/50, step: 73/83, loss: 5.14485, accuracy: 0.52568\n","Epoch: 7/50, step: 74/83, loss: 5.09970, accuracy: 0.52872\n","Epoch: 7/50, step: 75/83, loss: 5.12689, accuracy: 0.52333\n","Epoch: 7/50, step: 76/83, loss: 5.16537, accuracy: 0.52303\n","Epoch: 7/50, step: 77/83, loss: 5.17685, accuracy: 0.52273\n","Epoch: 7/50, step: 78/83, loss: 5.16728, accuracy: 0.52083\n","Epoch: 7/50, step: 79/83, loss: 5.15832, accuracy: 0.51899\n","Epoch: 7/50, step: 80/83, loss: 5.15935, accuracy: 0.51719\n","Epoch: 7/50, step: 81/83, loss: 5.15300, accuracy: 0.51543\n","Epoch: 7/50, step: 82/83, loss: 5.13419, accuracy: 0.51372\n","Epoch: 7/50, step: 83/83, loss: 5.16540, accuracy: 0.51446\n","Epoch: 7/50, train loss: 5.16540, train accuracy: 0.51446, valid loss: 8.70587, valid accuracy: 0.18100\n","Epoch: 8/50, step: 1/83, loss: 2.96013, accuracy: 0.75000\n","Epoch: 8/50, step: 2/83, loss: 3.66053, accuracy: 0.62500\n","Epoch: 8/50, step: 3/83, loss: 3.91245, accuracy: 0.54167\n","Epoch: 8/50, step: 4/83, loss: 4.72884, accuracy: 0.53125\n","Epoch: 8/50, step: 5/83, loss: 4.61911, accuracy: 0.57500\n","Epoch: 8/50, step: 6/83, loss: 4.58927, accuracy: 0.56250\n","Epoch: 8/50, step: 7/83, loss: 4.64333, accuracy: 0.55357\n","Epoch: 8/50, step: 8/83, loss: 5.16765, accuracy: 0.53125\n","Epoch: 8/50, step: 9/83, loss: 5.14298, accuracy: 0.55556\n","Epoch: 8/50, step: 10/83, loss: 5.07772, accuracy: 0.52500\n","Epoch: 8/50, step: 11/83, loss: 4.92494, accuracy: 0.52273\n","Epoch: 8/50, step: 12/83, loss: 4.88181, accuracy: 0.52083\n","Epoch: 8/50, step: 13/83, loss: 4.91710, accuracy: 0.50000\n","Epoch: 8/50, step: 14/83, loss: 5.00948, accuracy: 0.50000\n","Epoch: 8/50, step: 15/83, loss: 5.14429, accuracy: 0.48333\n","Epoch: 8/50, step: 16/83, loss: 5.19818, accuracy: 0.48438\n","Epoch: 8/50, step: 17/83, loss: 5.09763, accuracy: 0.49265\n","Epoch: 8/50, step: 18/83, loss: 5.25732, accuracy: 0.47222\n","Epoch: 8/50, step: 19/83, loss: 5.30422, accuracy: 0.47368\n","Epoch: 8/50, step: 20/83, loss: 5.12369, accuracy: 0.48750\n","Epoch: 8/50, step: 21/83, loss: 5.05149, accuracy: 0.47619\n","Epoch: 8/50, step: 22/83, loss: 4.94117, accuracy: 0.48864\n","Epoch: 8/50, step: 23/83, loss: 5.02549, accuracy: 0.49457\n","Epoch: 8/50, step: 24/83, loss: 5.03641, accuracy: 0.50000\n","Epoch: 8/50, step: 25/83, loss: 4.98181, accuracy: 0.49500\n","Epoch: 8/50, step: 26/83, loss: 4.99115, accuracy: 0.50000\n","Epoch: 8/50, step: 27/83, loss: 5.01418, accuracy: 0.49074\n","Epoch: 8/50, step: 28/83, loss: 5.04399, accuracy: 0.50000\n","Epoch: 8/50, step: 29/83, loss: 4.95716, accuracy: 0.50431\n","Epoch: 8/50, step: 30/83, loss: 4.87150, accuracy: 0.50833\n","Epoch: 8/50, step: 31/83, loss: 4.91151, accuracy: 0.50403\n","Epoch: 8/50, step: 32/83, loss: 4.93000, accuracy: 0.50000\n","Epoch: 8/50, step: 33/83, loss: 4.89391, accuracy: 0.50000\n","Epoch: 8/50, step: 34/83, loss: 4.90565, accuracy: 0.50000\n","Epoch: 8/50, step: 35/83, loss: 4.92365, accuracy: 0.49643\n","Epoch: 8/50, step: 36/83, loss: 4.90600, accuracy: 0.50347\n","Epoch: 8/50, step: 37/83, loss: 4.89056, accuracy: 0.50338\n","Epoch: 8/50, step: 38/83, loss: 4.83352, accuracy: 0.50329\n","Epoch: 8/50, step: 39/83, loss: 4.84853, accuracy: 0.49679\n","Epoch: 8/50, step: 40/83, loss: 4.92387, accuracy: 0.49687\n","Epoch: 8/50, step: 41/83, loss: 4.92551, accuracy: 0.49695\n","Epoch: 8/50, step: 42/83, loss: 4.97213, accuracy: 0.50000\n","Epoch: 8/50, step: 43/83, loss: 4.96000, accuracy: 0.50291\n","Epoch: 8/50, step: 44/83, loss: 4.92288, accuracy: 0.50568\n","Epoch: 8/50, step: 45/83, loss: 4.93196, accuracy: 0.50278\n","Epoch: 8/50, step: 46/83, loss: 4.91728, accuracy: 0.50543\n","Epoch: 8/50, step: 47/83, loss: 4.94159, accuracy: 0.50532\n","Epoch: 8/50, step: 48/83, loss: 4.97075, accuracy: 0.49740\n","Epoch: 8/50, step: 49/83, loss: 4.92440, accuracy: 0.49745\n","Epoch: 8/50, step: 50/83, loss: 4.92138, accuracy: 0.50250\n","Epoch: 8/50, step: 51/83, loss: 4.91246, accuracy: 0.50245\n","Epoch: 8/50, step: 52/83, loss: 4.88569, accuracy: 0.50481\n","Epoch: 8/50, step: 53/83, loss: 4.93596, accuracy: 0.50236\n","Epoch: 8/50, step: 54/83, loss: 4.91413, accuracy: 0.50231\n","Epoch: 8/50, step: 55/83, loss: 4.90646, accuracy: 0.50682\n","Epoch: 8/50, step: 56/83, loss: 4.92380, accuracy: 0.51116\n","Epoch: 8/50, step: 57/83, loss: 4.92550, accuracy: 0.51535\n","Epoch: 8/50, step: 58/83, loss: 4.92081, accuracy: 0.51509\n","Epoch: 8/50, step: 59/83, loss: 4.89269, accuracy: 0.51907\n","Epoch: 8/50, step: 60/83, loss: 4.85543, accuracy: 0.52083\n","Epoch: 8/50, step: 61/83, loss: 4.83327, accuracy: 0.52254\n","Epoch: 8/50, step: 62/83, loss: 4.83194, accuracy: 0.51815\n","Epoch: 8/50, step: 63/83, loss: 4.87217, accuracy: 0.51786\n","Epoch: 8/50, step: 64/83, loss: 4.86163, accuracy: 0.52344\n","Epoch: 8/50, step: 65/83, loss: 4.88313, accuracy: 0.51923\n","Epoch: 8/50, step: 66/83, loss: 4.90347, accuracy: 0.51894\n","Epoch: 8/50, step: 67/83, loss: 4.92024, accuracy: 0.51866\n","Epoch: 8/50, step: 68/83, loss: 4.88313, accuracy: 0.52390\n","Epoch: 8/50, step: 69/83, loss: 4.90301, accuracy: 0.52355\n","Epoch: 8/50, step: 70/83, loss: 4.88253, accuracy: 0.52500\n","Epoch: 8/50, step: 71/83, loss: 4.88821, accuracy: 0.52289\n","Epoch: 8/50, step: 72/83, loss: 4.89700, accuracy: 0.52431\n","Epoch: 8/50, step: 73/83, loss: 4.83998, accuracy: 0.52740\n","Epoch: 8/50, step: 74/83, loss: 4.84880, accuracy: 0.52534\n","Epoch: 8/50, step: 75/83, loss: 4.83286, accuracy: 0.52167\n","Epoch: 8/50, step: 76/83, loss: 4.82877, accuracy: 0.51974\n","Epoch: 8/50, step: 77/83, loss: 4.82178, accuracy: 0.51948\n","Epoch: 8/50, step: 78/83, loss: 4.84344, accuracy: 0.51442\n","Epoch: 8/50, step: 79/83, loss: 4.82955, accuracy: 0.51424\n","Epoch: 8/50, step: 80/83, loss: 4.80106, accuracy: 0.51562\n","Epoch: 8/50, step: 81/83, loss: 4.80753, accuracy: 0.51543\n","Epoch: 8/50, step: 82/83, loss: 4.81259, accuracy: 0.51677\n","Epoch: 8/50, step: 83/83, loss: 4.85757, accuracy: 0.51598\n","Epoch: 8/50, train loss: 4.85757, train accuracy: 0.51598, valid loss: 9.99399, valid accuracy: 0.16290\n","Epoch: 9/50, step: 1/83, loss: 5.67437, accuracy: 0.62500\n","Epoch: 9/50, step: 2/83, loss: 6.24971, accuracy: 0.56250\n","Epoch: 9/50, step: 3/83, loss: 5.61860, accuracy: 0.45833\n","Epoch: 9/50, step: 4/83, loss: 5.34758, accuracy: 0.43750\n","Epoch: 9/50, step: 5/83, loss: 5.30838, accuracy: 0.47500\n","Epoch: 9/50, step: 6/83, loss: 4.87425, accuracy: 0.47917\n","Epoch: 9/50, step: 7/83, loss: 4.71045, accuracy: 0.48214\n","Epoch: 9/50, step: 8/83, loss: 4.61901, accuracy: 0.48438\n","Epoch: 9/50, step: 9/83, loss: 4.39728, accuracy: 0.48611\n","Epoch: 9/50, step: 10/83, loss: 4.35515, accuracy: 0.50000\n","Epoch: 9/50, step: 11/83, loss: 4.57561, accuracy: 0.52273\n","Epoch: 9/50, step: 12/83, loss: 4.51981, accuracy: 0.53125\n","Epoch: 9/50, step: 13/83, loss: 4.41895, accuracy: 0.55769\n","Epoch: 9/50, step: 14/83, loss: 4.50664, accuracy: 0.52679\n","Epoch: 9/50, step: 15/83, loss: 4.62980, accuracy: 0.53333\n","Epoch: 9/50, step: 16/83, loss: 4.85772, accuracy: 0.50781\n","Epoch: 9/50, step: 17/83, loss: 4.87711, accuracy: 0.51471\n","Epoch: 9/50, step: 18/83, loss: 4.74154, accuracy: 0.53472\n","Epoch: 9/50, step: 19/83, loss: 4.66723, accuracy: 0.53289\n","Epoch: 9/50, step: 20/83, loss: 4.68355, accuracy: 0.53750\n","Epoch: 9/50, step: 21/83, loss: 4.70209, accuracy: 0.54762\n","Epoch: 9/50, step: 22/83, loss: 4.84577, accuracy: 0.54545\n","Epoch: 9/50, step: 23/83, loss: 4.71225, accuracy: 0.54891\n","Epoch: 9/50, step: 24/83, loss: 4.70536, accuracy: 0.54688\n","Epoch: 9/50, step: 25/83, loss: 4.66076, accuracy: 0.54500\n","Epoch: 9/50, step: 26/83, loss: 4.64732, accuracy: 0.55769\n","Epoch: 9/50, step: 27/83, loss: 4.59800, accuracy: 0.56019\n","Epoch: 9/50, step: 28/83, loss: 4.62912, accuracy: 0.55357\n","Epoch: 9/50, step: 29/83, loss: 4.74000, accuracy: 0.55172\n","Epoch: 9/50, step: 30/83, loss: 4.69573, accuracy: 0.55417\n","Epoch: 9/50, step: 31/83, loss: 4.77066, accuracy: 0.55242\n","Epoch: 9/50, step: 32/83, loss: 4.70222, accuracy: 0.55859\n","Epoch: 9/50, step: 33/83, loss: 4.74892, accuracy: 0.55303\n","Epoch: 9/50, step: 34/83, loss: 4.71840, accuracy: 0.54779\n","Epoch: 9/50, step: 35/83, loss: 4.75166, accuracy: 0.54643\n","Epoch: 9/50, step: 36/83, loss: 4.78526, accuracy: 0.53819\n","Epoch: 9/50, step: 37/83, loss: 4.74353, accuracy: 0.54392\n","Epoch: 9/50, step: 38/83, loss: 4.78444, accuracy: 0.53947\n","Epoch: 9/50, step: 39/83, loss: 4.79674, accuracy: 0.54167\n","Epoch: 9/50, step: 40/83, loss: 4.76531, accuracy: 0.54375\n","Epoch: 9/50, step: 41/83, loss: 4.76083, accuracy: 0.54268\n","Epoch: 9/50, step: 42/83, loss: 4.79466, accuracy: 0.54464\n","Epoch: 9/50, step: 43/83, loss: 4.77121, accuracy: 0.54360\n","Epoch: 9/50, step: 44/83, loss: 4.72655, accuracy: 0.53977\n","Epoch: 9/50, step: 45/83, loss: 4.81551, accuracy: 0.54167\n","Epoch: 9/50, step: 46/83, loss: 4.78050, accuracy: 0.54620\n","Epoch: 9/50, step: 47/83, loss: 4.74901, accuracy: 0.54787\n","Epoch: 9/50, step: 48/83, loss: 4.74895, accuracy: 0.54167\n","Epoch: 9/50, step: 49/83, loss: 4.78332, accuracy: 0.53316\n","Epoch: 9/50, step: 50/83, loss: 4.80999, accuracy: 0.53000\n","Epoch: 9/50, step: 51/83, loss: 4.83555, accuracy: 0.52696\n","Epoch: 9/50, step: 52/83, loss: 4.82854, accuracy: 0.53125\n","Epoch: 9/50, step: 53/83, loss: 4.88093, accuracy: 0.53538\n","Epoch: 9/50, step: 54/83, loss: 4.88560, accuracy: 0.53472\n","Epoch: 9/50, step: 55/83, loss: 4.82696, accuracy: 0.53864\n","Epoch: 9/50, step: 56/83, loss: 4.82315, accuracy: 0.53571\n","Epoch: 9/50, step: 57/83, loss: 4.83481, accuracy: 0.53070\n","Epoch: 9/50, step: 58/83, loss: 4.84265, accuracy: 0.52802\n","Epoch: 9/50, step: 59/83, loss: 4.79434, accuracy: 0.52542\n","Epoch: 9/50, step: 60/83, loss: 4.75500, accuracy: 0.52500\n","Epoch: 9/50, step: 61/83, loss: 4.77986, accuracy: 0.52049\n","Epoch: 9/50, step: 62/83, loss: 4.81812, accuracy: 0.51613\n","Epoch: 9/50, step: 63/83, loss: 4.84174, accuracy: 0.51984\n","Epoch: 9/50, step: 64/83, loss: 4.89494, accuracy: 0.51562\n","Epoch: 9/50, step: 65/83, loss: 4.93864, accuracy: 0.51346\n","Epoch: 9/50, step: 66/83, loss: 4.90056, accuracy: 0.51515\n","Epoch: 9/50, step: 67/83, loss: 4.91206, accuracy: 0.51493\n","Epoch: 9/50, step: 68/83, loss: 4.86569, accuracy: 0.51471\n","Epoch: 9/50, step: 69/83, loss: 4.88055, accuracy: 0.51449\n","Epoch: 9/50, step: 70/83, loss: 4.89952, accuracy: 0.51250\n","Epoch: 9/50, step: 71/83, loss: 4.89125, accuracy: 0.51408\n","Epoch: 9/50, step: 72/83, loss: 4.89359, accuracy: 0.51562\n","Epoch: 9/50, step: 73/83, loss: 4.88851, accuracy: 0.51541\n","Epoch: 9/50, step: 74/83, loss: 4.86938, accuracy: 0.51520\n","Epoch: 9/50, step: 75/83, loss: 4.86262, accuracy: 0.51667\n","Epoch: 9/50, step: 76/83, loss: 4.83670, accuracy: 0.51809\n","Epoch: 9/50, step: 77/83, loss: 4.84205, accuracy: 0.51948\n","Epoch: 9/50, step: 78/83, loss: 4.82333, accuracy: 0.52244\n","Epoch: 9/50, step: 79/83, loss: 4.81342, accuracy: 0.52848\n","Epoch: 9/50, step: 80/83, loss: 4.85474, accuracy: 0.52656\n","Epoch: 9/50, step: 81/83, loss: 4.83482, accuracy: 0.52778\n","Epoch: 9/50, step: 82/83, loss: 4.86969, accuracy: 0.52744\n","Epoch: 9/50, step: 83/83, loss: 4.81961, accuracy: 0.52664\n","Epoch: 9/50, train loss: 4.81961, train accuracy: 0.52664, valid loss: 9.03395, valid accuracy: 0.20362\n","Epoch: 10/50, step: 1/83, loss: 5.89299, accuracy: 0.87500\n","Epoch: 10/50, step: 2/83, loss: 5.28084, accuracy: 0.56250\n","Epoch: 10/50, step: 3/83, loss: 6.70894, accuracy: 0.41667\n","Epoch: 10/50, step: 4/83, loss: 6.09291, accuracy: 0.50000\n","Epoch: 10/50, step: 5/83, loss: 6.10760, accuracy: 0.55000\n","Epoch: 10/50, step: 6/83, loss: 5.98198, accuracy: 0.58333\n","Epoch: 10/50, step: 7/83, loss: 6.07842, accuracy: 0.51786\n","Epoch: 10/50, step: 8/83, loss: 5.90320, accuracy: 0.48438\n","Epoch: 10/50, step: 9/83, loss: 5.61847, accuracy: 0.50000\n","Epoch: 10/50, step: 10/83, loss: 5.38752, accuracy: 0.51250\n","Epoch: 10/50, step: 11/83, loss: 5.29617, accuracy: 0.51136\n","Epoch: 10/50, step: 12/83, loss: 5.37615, accuracy: 0.51042\n","Epoch: 10/50, step: 13/83, loss: 5.15147, accuracy: 0.52885\n","Epoch: 10/50, step: 14/83, loss: 5.01707, accuracy: 0.54464\n","Epoch: 10/50, step: 15/83, loss: 4.85842, accuracy: 0.54167\n","Epoch: 10/50, step: 16/83, loss: 4.93343, accuracy: 0.53906\n","Epoch: 10/50, step: 17/83, loss: 4.89930, accuracy: 0.52941\n","Epoch: 10/50, step: 18/83, loss: 4.95763, accuracy: 0.53472\n","Epoch: 10/50, step: 19/83, loss: 4.96902, accuracy: 0.53289\n","Epoch: 10/50, step: 20/83, loss: 5.07765, accuracy: 0.53125\n","Epoch: 10/50, step: 21/83, loss: 4.99953, accuracy: 0.54167\n","Epoch: 10/50, step: 22/83, loss: 5.07191, accuracy: 0.53409\n","Epoch: 10/50, step: 23/83, loss: 5.12080, accuracy: 0.53804\n","Epoch: 10/50, step: 24/83, loss: 5.12955, accuracy: 0.53646\n","Epoch: 10/50, step: 25/83, loss: 5.07433, accuracy: 0.53500\n","Epoch: 10/50, step: 26/83, loss: 5.05048, accuracy: 0.53846\n","Epoch: 10/50, step: 27/83, loss: 4.92900, accuracy: 0.53704\n","Epoch: 10/50, step: 28/83, loss: 4.84679, accuracy: 0.53571\n","Epoch: 10/50, step: 29/83, loss: 4.83256, accuracy: 0.53879\n","Epoch: 10/50, step: 30/83, loss: 4.84020, accuracy: 0.54167\n","Epoch: 10/50, step: 31/83, loss: 4.83032, accuracy: 0.54032\n","Epoch: 10/50, step: 32/83, loss: 4.85170, accuracy: 0.53125\n","Epoch: 10/50, step: 33/83, loss: 4.83684, accuracy: 0.53030\n","Epoch: 10/50, step: 34/83, loss: 4.82003, accuracy: 0.53676\n","Epoch: 10/50, step: 35/83, loss: 4.84298, accuracy: 0.53571\n","Epoch: 10/50, step: 36/83, loss: 4.82903, accuracy: 0.53819\n","Epoch: 10/50, step: 37/83, loss: 4.81565, accuracy: 0.54392\n","Epoch: 10/50, step: 38/83, loss: 4.87220, accuracy: 0.54605\n","Epoch: 10/50, step: 39/83, loss: 4.94169, accuracy: 0.54167\n","Epoch: 10/50, step: 40/83, loss: 4.86596, accuracy: 0.54375\n","Epoch: 10/50, step: 41/83, loss: 4.80328, accuracy: 0.54878\n","Epoch: 10/50, step: 42/83, loss: 4.75103, accuracy: 0.55060\n","Epoch: 10/50, step: 43/83, loss: 4.76546, accuracy: 0.54942\n","Epoch: 10/50, step: 44/83, loss: 4.81772, accuracy: 0.54830\n","Epoch: 10/50, step: 45/83, loss: 4.80014, accuracy: 0.54722\n","Epoch: 10/50, step: 46/83, loss: 4.80075, accuracy: 0.54620\n","Epoch: 10/50, step: 47/83, loss: 4.81644, accuracy: 0.54255\n","Epoch: 10/50, step: 48/83, loss: 4.76952, accuracy: 0.54688\n","Epoch: 10/50, step: 49/83, loss: 4.81515, accuracy: 0.54082\n","Epoch: 10/50, step: 50/83, loss: 4.80359, accuracy: 0.54500\n","Epoch: 10/50, step: 51/83, loss: 4.76340, accuracy: 0.54167\n","Epoch: 10/50, step: 52/83, loss: 4.75831, accuracy: 0.54087\n","Epoch: 10/50, step: 53/83, loss: 4.76984, accuracy: 0.53538\n","Epoch: 10/50, step: 54/83, loss: 4.75831, accuracy: 0.54167\n","Epoch: 10/50, step: 55/83, loss: 4.72138, accuracy: 0.54091\n","Epoch: 10/50, step: 56/83, loss: 4.69516, accuracy: 0.54464\n","Epoch: 10/50, step: 57/83, loss: 4.66010, accuracy: 0.54167\n","Epoch: 10/50, step: 58/83, loss: 4.67180, accuracy: 0.54310\n","Epoch: 10/50, step: 59/83, loss: 4.66161, accuracy: 0.54661\n","Epoch: 10/50, step: 60/83, loss: 4.65483, accuracy: 0.55000\n","Epoch: 10/50, step: 61/83, loss: 4.68515, accuracy: 0.54713\n","Epoch: 10/50, step: 62/83, loss: 4.69508, accuracy: 0.54637\n","Epoch: 10/50, step: 63/83, loss: 4.68884, accuracy: 0.54365\n","Epoch: 10/50, step: 64/83, loss: 4.68651, accuracy: 0.54102\n","Epoch: 10/50, step: 65/83, loss: 4.68475, accuracy: 0.53846\n","Epoch: 10/50, step: 66/83, loss: 4.62608, accuracy: 0.54356\n","Epoch: 10/50, step: 67/83, loss: 4.59691, accuracy: 0.54664\n","Epoch: 10/50, step: 68/83, loss: 4.57849, accuracy: 0.54963\n","Epoch: 10/50, step: 69/83, loss: 4.62593, accuracy: 0.54891\n","Epoch: 10/50, step: 70/83, loss: 4.63822, accuracy: 0.54643\n","Epoch: 10/50, step: 71/83, loss: 4.61694, accuracy: 0.55106\n","Epoch: 10/50, step: 72/83, loss: 4.61438, accuracy: 0.55035\n","Epoch: 10/50, step: 73/83, loss: 4.61124, accuracy: 0.55137\n","Epoch: 10/50, step: 74/83, loss: 4.64240, accuracy: 0.55068\n","Epoch: 10/50, step: 75/83, loss: 4.63998, accuracy: 0.54667\n","Epoch: 10/50, step: 76/83, loss: 4.64917, accuracy: 0.54770\n","Epoch: 10/50, step: 77/83, loss: 4.65609, accuracy: 0.54545\n","Epoch: 10/50, step: 78/83, loss: 4.62327, accuracy: 0.54808\n","Epoch: 10/50, step: 79/83, loss: 4.59756, accuracy: 0.54747\n","Epoch: 10/50, step: 80/83, loss: 4.59809, accuracy: 0.54688\n","Epoch: 10/50, step: 81/83, loss: 4.56483, accuracy: 0.54475\n","Epoch: 10/50, step: 82/83, loss: 4.53699, accuracy: 0.54878\n","Epoch: 10/50, step: 83/83, loss: 4.50231, accuracy: 0.54795\n","Epoch: 10/50, train loss: 4.50231, train accuracy: 0.54795, valid loss: 8.95871, valid accuracy: 0.24887\n","Epoch: 11/50, step: 1/83, loss: 3.59206, accuracy: 0.75000\n","Epoch: 11/50, step: 2/83, loss: 5.78742, accuracy: 0.56250\n","Epoch: 11/50, step: 3/83, loss: 5.06573, accuracy: 0.62500\n","Epoch: 11/50, step: 4/83, loss: 5.01043, accuracy: 0.50000\n","Epoch: 11/50, step: 5/83, loss: 4.53530, accuracy: 0.57500\n","Epoch: 11/50, step: 6/83, loss: 4.50034, accuracy: 0.54167\n","Epoch: 11/50, step: 7/83, loss: 4.44477, accuracy: 0.55357\n","Epoch: 11/50, step: 8/83, loss: 4.54283, accuracy: 0.54688\n","Epoch: 11/50, step: 9/83, loss: 4.53627, accuracy: 0.54167\n","Epoch: 11/50, step: 10/83, loss: 4.74715, accuracy: 0.50000\n","Epoch: 11/50, step: 11/83, loss: 4.46144, accuracy: 0.51136\n","Epoch: 11/50, step: 12/83, loss: 4.39170, accuracy: 0.51042\n","Epoch: 11/50, step: 13/83, loss: 4.63197, accuracy: 0.53846\n","Epoch: 11/50, step: 14/83, loss: 4.46759, accuracy: 0.54464\n","Epoch: 11/50, step: 15/83, loss: 4.39170, accuracy: 0.55833\n","Epoch: 11/50, step: 16/83, loss: 4.34106, accuracy: 0.56250\n","Epoch: 11/50, step: 17/83, loss: 4.32306, accuracy: 0.58088\n","Epoch: 11/50, step: 18/83, loss: 4.22688, accuracy: 0.57639\n","Epoch: 11/50, step: 19/83, loss: 4.42574, accuracy: 0.57237\n","Epoch: 11/50, step: 20/83, loss: 4.36719, accuracy: 0.57500\n","Epoch: 11/50, step: 21/83, loss: 4.36382, accuracy: 0.58333\n","Epoch: 11/50, step: 22/83, loss: 4.40321, accuracy: 0.57955\n","Epoch: 11/50, step: 23/83, loss: 4.43506, accuracy: 0.57609\n","Epoch: 11/50, step: 24/83, loss: 4.47074, accuracy: 0.57812\n","Epoch: 11/50, step: 25/83, loss: 4.38514, accuracy: 0.58500\n","Epoch: 11/50, step: 26/83, loss: 4.37980, accuracy: 0.58654\n","Epoch: 11/50, step: 27/83, loss: 4.38326, accuracy: 0.57870\n","Epoch: 11/50, step: 28/83, loss: 4.32157, accuracy: 0.57143\n","Epoch: 11/50, step: 29/83, loss: 4.32444, accuracy: 0.56897\n","Epoch: 11/50, step: 30/83, loss: 4.41260, accuracy: 0.56667\n","Epoch: 11/50, step: 31/83, loss: 4.44278, accuracy: 0.56452\n","Epoch: 11/50, step: 32/83, loss: 4.35545, accuracy: 0.56641\n","Epoch: 11/50, step: 33/83, loss: 4.35634, accuracy: 0.57197\n","Epoch: 11/50, step: 34/83, loss: 4.31001, accuracy: 0.57353\n","Epoch: 11/50, step: 35/83, loss: 4.33788, accuracy: 0.57143\n","Epoch: 11/50, step: 36/83, loss: 4.31764, accuracy: 0.57292\n","Epoch: 11/50, step: 37/83, loss: 4.33980, accuracy: 0.57432\n","Epoch: 11/50, step: 38/83, loss: 4.41130, accuracy: 0.56579\n","Epoch: 11/50, step: 39/83, loss: 4.40952, accuracy: 0.56731\n","Epoch: 11/50, step: 40/83, loss: 4.43076, accuracy: 0.56563\n","Epoch: 11/50, step: 41/83, loss: 4.40727, accuracy: 0.56707\n","Epoch: 11/50, step: 42/83, loss: 4.35999, accuracy: 0.57143\n","Epoch: 11/50, step: 43/83, loss: 4.38679, accuracy: 0.56977\n","Epoch: 11/50, step: 44/83, loss: 4.43269, accuracy: 0.56250\n","Epoch: 11/50, step: 45/83, loss: 4.47267, accuracy: 0.56389\n","Epoch: 11/50, step: 46/83, loss: 4.45279, accuracy: 0.55978\n","Epoch: 11/50, step: 47/83, loss: 4.43483, accuracy: 0.56117\n","Epoch: 11/50, step: 48/83, loss: 4.41939, accuracy: 0.55990\n","Epoch: 11/50, step: 49/83, loss: 4.41386, accuracy: 0.56122\n","Epoch: 11/50, step: 50/83, loss: 4.35895, accuracy: 0.56250\n","Epoch: 11/50, step: 51/83, loss: 4.37426, accuracy: 0.56127\n","Epoch: 11/50, step: 52/83, loss: 4.33930, accuracy: 0.56490\n","Epoch: 11/50, step: 53/83, loss: 4.35915, accuracy: 0.56368\n","Epoch: 11/50, step: 54/83, loss: 4.42833, accuracy: 0.55324\n","Epoch: 11/50, step: 55/83, loss: 4.45809, accuracy: 0.55227\n","Epoch: 11/50, step: 56/83, loss: 4.43437, accuracy: 0.55580\n","Epoch: 11/50, step: 57/83, loss: 4.43029, accuracy: 0.55921\n","Epoch: 11/50, step: 58/83, loss: 4.49009, accuracy: 0.55819\n","Epoch: 11/50, step: 59/83, loss: 4.46274, accuracy: 0.56144\n","Epoch: 11/50, step: 60/83, loss: 4.46700, accuracy: 0.56458\n","Epoch: 11/50, step: 61/83, loss: 4.47140, accuracy: 0.56352\n","Epoch: 11/50, step: 62/83, loss: 4.45338, accuracy: 0.56250\n","Epoch: 11/50, step: 63/83, loss: 4.43221, accuracy: 0.56548\n","Epoch: 11/50, step: 64/83, loss: 4.46397, accuracy: 0.56250\n","Epoch: 11/50, step: 65/83, loss: 4.43785, accuracy: 0.56538\n","Epoch: 11/50, step: 66/83, loss: 4.45943, accuracy: 0.57008\n","Epoch: 11/50, step: 67/83, loss: 4.47022, accuracy: 0.57090\n","Epoch: 11/50, step: 68/83, loss: 4.45490, accuracy: 0.57169\n","Epoch: 11/50, step: 69/83, loss: 4.46993, accuracy: 0.57065\n","Epoch: 11/50, step: 70/83, loss: 4.45334, accuracy: 0.56786\n","Epoch: 11/50, step: 71/83, loss: 4.41435, accuracy: 0.56866\n","Epoch: 11/50, step: 72/83, loss: 4.38562, accuracy: 0.57118\n","Epoch: 11/50, step: 73/83, loss: 4.41135, accuracy: 0.57363\n","Epoch: 11/50, step: 74/83, loss: 4.38349, accuracy: 0.57601\n","Epoch: 11/50, step: 75/83, loss: 4.38566, accuracy: 0.57167\n","Epoch: 11/50, step: 76/83, loss: 4.43077, accuracy: 0.57401\n","Epoch: 11/50, step: 77/83, loss: 4.45083, accuracy: 0.56981\n","Epoch: 11/50, step: 78/83, loss: 4.50985, accuracy: 0.56571\n","Epoch: 11/50, step: 79/83, loss: 4.53018, accuracy: 0.56013\n","Epoch: 11/50, step: 80/83, loss: 4.56011, accuracy: 0.56094\n","Epoch: 11/50, step: 81/83, loss: 4.52482, accuracy: 0.56173\n","Epoch: 11/50, step: 82/83, loss: 4.52272, accuracy: 0.55945\n","Epoch: 11/50, step: 83/83, loss: 4.56673, accuracy: 0.55860\n","Epoch: 11/50, train loss: 4.56673, train accuracy: 0.55860, valid loss: 9.40925, valid accuracy: 0.26697\n","Epoch: 12/50, step: 1/83, loss: 5.01068, accuracy: 0.62500\n","Epoch: 12/50, step: 2/83, loss: 4.77540, accuracy: 0.62500\n","Epoch: 12/50, step: 3/83, loss: 5.29200, accuracy: 0.54167\n","Epoch: 12/50, step: 4/83, loss: 4.61796, accuracy: 0.53125\n","Epoch: 12/50, step: 5/83, loss: 4.43432, accuracy: 0.55000\n","Epoch: 12/50, step: 6/83, loss: 4.85939, accuracy: 0.52083\n","Epoch: 12/50, step: 7/83, loss: 4.93264, accuracy: 0.53571\n","Epoch: 12/50, step: 8/83, loss: 5.07399, accuracy: 0.54688\n","Epoch: 12/50, step: 9/83, loss: 5.10714, accuracy: 0.54167\n","Epoch: 12/50, step: 10/83, loss: 5.03644, accuracy: 0.52500\n","Epoch: 12/50, step: 11/83, loss: 5.08371, accuracy: 0.50000\n","Epoch: 12/50, step: 12/83, loss: 5.02104, accuracy: 0.50000\n","Epoch: 12/50, step: 13/83, loss: 5.04031, accuracy: 0.50000\n","Epoch: 12/50, step: 14/83, loss: 4.98986, accuracy: 0.50893\n","Epoch: 12/50, step: 15/83, loss: 4.87732, accuracy: 0.52500\n","Epoch: 12/50, step: 16/83, loss: 4.78476, accuracy: 0.52344\n","Epoch: 12/50, step: 17/83, loss: 4.91193, accuracy: 0.51471\n","Epoch: 12/50, step: 18/83, loss: 4.93536, accuracy: 0.50694\n","Epoch: 12/50, step: 19/83, loss: 4.82329, accuracy: 0.50658\n","Epoch: 12/50, step: 20/83, loss: 4.83827, accuracy: 0.51250\n","Epoch: 12/50, step: 21/83, loss: 4.72588, accuracy: 0.51786\n","Epoch: 12/50, step: 22/83, loss: 4.67609, accuracy: 0.51705\n","Epoch: 12/50, step: 23/83, loss: 4.66950, accuracy: 0.51087\n","Epoch: 12/50, step: 24/83, loss: 4.73645, accuracy: 0.51562\n","Epoch: 12/50, step: 25/83, loss: 4.67980, accuracy: 0.52000\n","Epoch: 12/50, step: 26/83, loss: 4.73903, accuracy: 0.52404\n","Epoch: 12/50, step: 27/83, loss: 4.77664, accuracy: 0.53241\n","Epoch: 12/50, step: 28/83, loss: 4.82756, accuracy: 0.53125\n","Epoch: 12/50, step: 29/83, loss: 4.78102, accuracy: 0.53448\n","Epoch: 12/50, step: 30/83, loss: 4.73862, accuracy: 0.52917\n","Epoch: 12/50, step: 31/83, loss: 4.72131, accuracy: 0.53629\n","Epoch: 12/50, step: 32/83, loss: 4.73183, accuracy: 0.53906\n","Epoch: 12/50, step: 33/83, loss: 4.69001, accuracy: 0.54167\n","Epoch: 12/50, step: 34/83, loss: 4.67793, accuracy: 0.55147\n","Epoch: 12/50, step: 35/83, loss: 4.62275, accuracy: 0.55714\n","Epoch: 12/50, step: 36/83, loss: 4.55638, accuracy: 0.56597\n","Epoch: 12/50, step: 37/83, loss: 4.59444, accuracy: 0.57095\n","Epoch: 12/50, step: 38/83, loss: 4.61757, accuracy: 0.56250\n","Epoch: 12/50, step: 39/83, loss: 4.66056, accuracy: 0.55449\n","Epoch: 12/50, step: 40/83, loss: 4.67505, accuracy: 0.55313\n","Epoch: 12/50, step: 41/83, loss: 4.62313, accuracy: 0.56098\n","Epoch: 12/50, step: 42/83, loss: 4.67780, accuracy: 0.55952\n","Epoch: 12/50, step: 43/83, loss: 4.67027, accuracy: 0.55814\n","Epoch: 12/50, step: 44/83, loss: 4.61922, accuracy: 0.56250\n","Epoch: 12/50, step: 45/83, loss: 4.59185, accuracy: 0.56944\n","Epoch: 12/50, step: 46/83, loss: 4.56583, accuracy: 0.57065\n","Epoch: 12/50, step: 47/83, loss: 4.62212, accuracy: 0.56383\n","Epoch: 12/50, step: 48/83, loss: 4.66921, accuracy: 0.55990\n","Epoch: 12/50, step: 49/83, loss: 4.69905, accuracy: 0.55867\n","Epoch: 12/50, step: 50/83, loss: 4.67645, accuracy: 0.55500\n","Epoch: 12/50, step: 51/83, loss: 4.68458, accuracy: 0.55637\n","Epoch: 12/50, step: 52/83, loss: 4.69524, accuracy: 0.55529\n","Epoch: 12/50, step: 53/83, loss: 4.69394, accuracy: 0.55425\n","Epoch: 12/50, step: 54/83, loss: 4.66294, accuracy: 0.56019\n","Epoch: 12/50, step: 55/83, loss: 4.65856, accuracy: 0.55909\n","Epoch: 12/50, step: 56/83, loss: 4.63616, accuracy: 0.56027\n","Epoch: 12/50, step: 57/83, loss: 4.64781, accuracy: 0.55702\n","Epoch: 12/50, step: 58/83, loss: 4.65953, accuracy: 0.55603\n","Epoch: 12/50, step: 59/83, loss: 4.68982, accuracy: 0.55297\n","Epoch: 12/50, step: 60/83, loss: 4.62213, accuracy: 0.55625\n","Epoch: 12/50, step: 61/83, loss: 4.63752, accuracy: 0.55328\n","Epoch: 12/50, step: 62/83, loss: 4.63164, accuracy: 0.55444\n","Epoch: 12/50, step: 63/83, loss: 4.65464, accuracy: 0.55556\n","Epoch: 12/50, step: 64/83, loss: 4.60910, accuracy: 0.55859\n","Epoch: 12/50, step: 65/83, loss: 4.59985, accuracy: 0.55769\n","Epoch: 12/50, step: 66/83, loss: 4.58445, accuracy: 0.56061\n","Epoch: 12/50, step: 67/83, loss: 4.57068, accuracy: 0.55784\n","Epoch: 12/50, step: 68/83, loss: 4.56412, accuracy: 0.56250\n","Epoch: 12/50, step: 69/83, loss: 4.54656, accuracy: 0.56341\n","Epoch: 12/50, step: 70/83, loss: 4.50384, accuracy: 0.56786\n","Epoch: 12/50, step: 71/83, loss: 4.48773, accuracy: 0.56866\n","Epoch: 12/50, step: 72/83, loss: 4.48591, accuracy: 0.57118\n","Epoch: 12/50, step: 73/83, loss: 4.47237, accuracy: 0.56849\n","Epoch: 12/50, step: 74/83, loss: 4.46487, accuracy: 0.56926\n","Epoch: 12/50, step: 75/83, loss: 4.44995, accuracy: 0.57333\n","Epoch: 12/50, step: 76/83, loss: 4.46274, accuracy: 0.57072\n","Epoch: 12/50, step: 77/83, loss: 4.47492, accuracy: 0.57143\n","Epoch: 12/50, step: 78/83, loss: 4.45632, accuracy: 0.57532\n","Epoch: 12/50, step: 79/83, loss: 4.46374, accuracy: 0.57437\n","Epoch: 12/50, step: 80/83, loss: 4.50009, accuracy: 0.57031\n","Epoch: 12/50, step: 81/83, loss: 4.47522, accuracy: 0.57253\n","Epoch: 12/50, step: 82/83, loss: 4.46390, accuracy: 0.57317\n","Epoch: 12/50, step: 83/83, loss: 4.42015, accuracy: 0.57382\n","Epoch: 12/50, train loss: 4.42015, train accuracy: 0.57382, valid loss: 9.74024, valid accuracy: 0.24887\n","Epoch: 13/50, step: 1/83, loss: 5.21620, accuracy: 0.62500\n","Epoch: 13/50, step: 2/83, loss: 4.24372, accuracy: 0.62500\n","Epoch: 13/50, step: 3/83, loss: 4.36840, accuracy: 0.58333\n","Epoch: 13/50, step: 4/83, loss: 4.14852, accuracy: 0.56250\n","Epoch: 13/50, step: 5/83, loss: 4.23666, accuracy: 0.52500\n","Epoch: 13/50, step: 6/83, loss: 4.27300, accuracy: 0.54167\n","Epoch: 13/50, step: 7/83, loss: 3.96128, accuracy: 0.58929\n","Epoch: 13/50, step: 8/83, loss: 4.14220, accuracy: 0.62500\n","Epoch: 13/50, step: 9/83, loss: 4.16912, accuracy: 0.65278\n","Epoch: 13/50, step: 10/83, loss: 4.18307, accuracy: 0.65000\n","Epoch: 13/50, step: 11/83, loss: 4.53684, accuracy: 0.63636\n","Epoch: 13/50, step: 12/83, loss: 4.24777, accuracy: 0.65625\n","Epoch: 13/50, step: 13/83, loss: 4.33170, accuracy: 0.64423\n","Epoch: 13/50, step: 14/83, loss: 4.32742, accuracy: 0.63393\n","Epoch: 13/50, step: 15/83, loss: 4.50173, accuracy: 0.62500\n","Epoch: 13/50, step: 16/83, loss: 4.54749, accuracy: 0.60938\n","Epoch: 13/50, step: 17/83, loss: 4.59526, accuracy: 0.59559\n","Epoch: 13/50, step: 18/83, loss: 4.68665, accuracy: 0.59028\n","Epoch: 13/50, step: 19/83, loss: 4.68994, accuracy: 0.58553\n","Epoch: 13/50, step: 20/83, loss: 4.67121, accuracy: 0.57500\n","Epoch: 13/50, step: 21/83, loss: 4.59733, accuracy: 0.58929\n","Epoch: 13/50, step: 22/83, loss: 4.54187, accuracy: 0.59091\n","Epoch: 13/50, step: 23/83, loss: 4.48391, accuracy: 0.59783\n","Epoch: 13/50, step: 24/83, loss: 4.45648, accuracy: 0.58854\n","Epoch: 13/50, step: 25/83, loss: 4.38686, accuracy: 0.59000\n","Epoch: 13/50, step: 26/83, loss: 4.35165, accuracy: 0.59135\n","Epoch: 13/50, step: 27/83, loss: 4.37789, accuracy: 0.58796\n","Epoch: 13/50, step: 28/83, loss: 4.44743, accuracy: 0.57589\n","Epoch: 13/50, step: 29/83, loss: 4.47386, accuracy: 0.57759\n","Epoch: 13/50, step: 30/83, loss: 4.40096, accuracy: 0.57917\n","Epoch: 13/50, step: 31/83, loss: 4.36654, accuracy: 0.58468\n","Epoch: 13/50, step: 32/83, loss: 4.34014, accuracy: 0.58203\n","Epoch: 13/50, step: 33/83, loss: 4.32338, accuracy: 0.57576\n","Epoch: 13/50, step: 34/83, loss: 4.29241, accuracy: 0.58088\n","Epoch: 13/50, step: 35/83, loss: 4.29009, accuracy: 0.58571\n","Epoch: 13/50, step: 36/83, loss: 4.29159, accuracy: 0.58333\n","Epoch: 13/50, step: 37/83, loss: 4.24067, accuracy: 0.59459\n","Epoch: 13/50, step: 38/83, loss: 4.28516, accuracy: 0.59868\n","Epoch: 13/50, step: 39/83, loss: 4.31346, accuracy: 0.59295\n","Epoch: 13/50, step: 40/83, loss: 4.29429, accuracy: 0.59375\n","Epoch: 13/50, step: 41/83, loss: 4.29726, accuracy: 0.59146\n","Epoch: 13/50, step: 42/83, loss: 4.29458, accuracy: 0.58631\n","Epoch: 13/50, step: 43/83, loss: 4.31616, accuracy: 0.57849\n","Epoch: 13/50, step: 44/83, loss: 4.27744, accuracy: 0.57955\n","Epoch: 13/50, step: 45/83, loss: 4.21927, accuracy: 0.58056\n","Epoch: 13/50, step: 46/83, loss: 4.19296, accuracy: 0.58152\n","Epoch: 13/50, step: 47/83, loss: 4.21453, accuracy: 0.58245\n","Epoch: 13/50, step: 48/83, loss: 4.25346, accuracy: 0.58333\n","Epoch: 13/50, step: 49/83, loss: 4.21635, accuracy: 0.58418\n","Epoch: 13/50, step: 50/83, loss: 4.25014, accuracy: 0.58250\n","Epoch: 13/50, step: 51/83, loss: 4.19541, accuracy: 0.58824\n","Epoch: 13/50, step: 52/83, loss: 4.14979, accuracy: 0.58894\n","Epoch: 13/50, step: 53/83, loss: 4.17307, accuracy: 0.58726\n","Epoch: 13/50, step: 54/83, loss: 4.21024, accuracy: 0.58333\n","Epoch: 13/50, step: 55/83, loss: 4.22422, accuracy: 0.58182\n","Epoch: 13/50, step: 56/83, loss: 4.22210, accuracy: 0.58482\n","Epoch: 13/50, step: 57/83, loss: 4.19252, accuracy: 0.58772\n","Epoch: 13/50, step: 58/83, loss: 4.25832, accuracy: 0.58405\n","Epoch: 13/50, step: 59/83, loss: 4.24334, accuracy: 0.58475\n","Epoch: 13/50, step: 60/83, loss: 4.23893, accuracy: 0.58333\n","Epoch: 13/50, step: 61/83, loss: 4.25463, accuracy: 0.58607\n","Epoch: 13/50, step: 62/83, loss: 4.24130, accuracy: 0.58669\n","Epoch: 13/50, step: 63/83, loss: 4.25459, accuracy: 0.58730\n","Epoch: 13/50, step: 64/83, loss: 4.26017, accuracy: 0.58203\n","Epoch: 13/50, step: 65/83, loss: 4.24581, accuracy: 0.58462\n","Epoch: 13/50, step: 66/83, loss: 4.23272, accuracy: 0.58523\n","Epoch: 13/50, step: 67/83, loss: 4.24928, accuracy: 0.58396\n","Epoch: 13/50, step: 68/83, loss: 4.26517, accuracy: 0.58272\n","Epoch: 13/50, step: 69/83, loss: 4.28032, accuracy: 0.57971\n","Epoch: 13/50, step: 70/83, loss: 4.31630, accuracy: 0.57500\n","Epoch: 13/50, step: 71/83, loss: 4.31534, accuracy: 0.57570\n","Epoch: 13/50, step: 72/83, loss: 4.30904, accuracy: 0.57639\n","Epoch: 13/50, step: 73/83, loss: 4.29116, accuracy: 0.57534\n","Epoch: 13/50, step: 74/83, loss: 4.29519, accuracy: 0.57264\n","Epoch: 13/50, step: 75/83, loss: 4.25862, accuracy: 0.57500\n","Epoch: 13/50, step: 76/83, loss: 4.25664, accuracy: 0.57895\n","Epoch: 13/50, step: 77/83, loss: 4.24831, accuracy: 0.57630\n","Epoch: 13/50, step: 78/83, loss: 4.24948, accuracy: 0.57372\n","Epoch: 13/50, step: 79/83, loss: 4.22548, accuracy: 0.57595\n","Epoch: 13/50, step: 80/83, loss: 4.25164, accuracy: 0.57344\n","Epoch: 13/50, step: 81/83, loss: 4.23238, accuracy: 0.57253\n","Epoch: 13/50, step: 82/83, loss: 4.21219, accuracy: 0.57317\n","Epoch: 13/50, step: 83/83, loss: 4.17430, accuracy: 0.57382\n","Epoch: 13/50, train loss: 4.17430, train accuracy: 0.57382, valid loss: 9.66252, valid accuracy: 0.24434\n","Epoch: 14/50, step: 1/83, loss: 5.07303, accuracy: 0.62500\n","Epoch: 14/50, step: 2/83, loss: 3.38631, accuracy: 0.56250\n","Epoch: 14/50, step: 3/83, loss: 3.38631, accuracy: 0.54167\n","Epoch: 14/50, step: 4/83, loss: 3.63038, accuracy: 0.59375\n","Epoch: 14/50, step: 5/83, loss: 3.58056, accuracy: 0.60000\n","Epoch: 14/50, step: 6/83, loss: 3.66861, accuracy: 0.56250\n","Epoch: 14/50, step: 7/83, loss: 3.66576, accuracy: 0.57143\n","Epoch: 14/50, step: 8/83, loss: 3.74116, accuracy: 0.59375\n","Epoch: 14/50, step: 9/83, loss: 3.90627, accuracy: 0.55556\n","Epoch: 14/50, step: 10/83, loss: 4.13582, accuracy: 0.53750\n","Epoch: 14/50, step: 11/83, loss: 4.25258, accuracy: 0.52273\n","Epoch: 14/50, step: 12/83, loss: 4.17255, accuracy: 0.54167\n","Epoch: 14/50, step: 13/83, loss: 4.39910, accuracy: 0.52885\n","Epoch: 14/50, step: 14/83, loss: 4.31218, accuracy: 0.54464\n","Epoch: 14/50, step: 15/83, loss: 4.23728, accuracy: 0.54167\n","Epoch: 14/50, step: 16/83, loss: 4.13719, accuracy: 0.53906\n","Epoch: 14/50, step: 17/83, loss: 4.09003, accuracy: 0.55147\n","Epoch: 14/50, step: 18/83, loss: 3.98974, accuracy: 0.56944\n","Epoch: 14/50, step: 19/83, loss: 3.86966, accuracy: 0.57237\n","Epoch: 14/50, step: 20/83, loss: 3.93795, accuracy: 0.56250\n","Epoch: 14/50, step: 21/83, loss: 3.95175, accuracy: 0.56548\n","Epoch: 14/50, step: 22/83, loss: 4.09065, accuracy: 0.56818\n","Epoch: 14/50, step: 23/83, loss: 4.10126, accuracy: 0.57065\n","Epoch: 14/50, step: 24/83, loss: 4.04020, accuracy: 0.57292\n","Epoch: 14/50, step: 25/83, loss: 4.09014, accuracy: 0.57000\n","Epoch: 14/50, step: 26/83, loss: 4.09933, accuracy: 0.56731\n","Epoch: 14/50, step: 27/83, loss: 4.06874, accuracy: 0.57407\n","Epoch: 14/50, step: 28/83, loss: 4.07396, accuracy: 0.57589\n","Epoch: 14/50, step: 29/83, loss: 4.05982, accuracy: 0.57328\n","Epoch: 14/50, step: 30/83, loss: 4.03291, accuracy: 0.57917\n","Epoch: 14/50, step: 31/83, loss: 3.98161, accuracy: 0.58468\n","Epoch: 14/50, step: 32/83, loss: 3.95950, accuracy: 0.58984\n","Epoch: 14/50, step: 33/83, loss: 3.88631, accuracy: 0.59470\n","Epoch: 14/50, step: 34/83, loss: 3.90949, accuracy: 0.58824\n","Epoch: 14/50, step: 35/83, loss: 3.89545, accuracy: 0.59286\n","Epoch: 14/50, step: 36/83, loss: 3.89897, accuracy: 0.60069\n","Epoch: 14/50, step: 37/83, loss: 3.90855, accuracy: 0.60473\n","Epoch: 14/50, step: 38/83, loss: 3.85007, accuracy: 0.60855\n","Epoch: 14/50, step: 39/83, loss: 3.88326, accuracy: 0.61538\n","Epoch: 14/50, step: 40/83, loss: 3.98381, accuracy: 0.60938\n","Epoch: 14/50, step: 41/83, loss: 3.94659, accuracy: 0.61280\n","Epoch: 14/50, step: 42/83, loss: 3.95404, accuracy: 0.61012\n","Epoch: 14/50, step: 43/83, loss: 3.94139, accuracy: 0.60756\n","Epoch: 14/50, step: 44/83, loss: 3.93497, accuracy: 0.60511\n","Epoch: 14/50, step: 45/83, loss: 3.88419, accuracy: 0.60556\n","Epoch: 14/50, step: 46/83, loss: 3.91748, accuracy: 0.59783\n","Epoch: 14/50, step: 47/83, loss: 3.92061, accuracy: 0.59840\n","Epoch: 14/50, step: 48/83, loss: 3.91470, accuracy: 0.59375\n","Epoch: 14/50, step: 49/83, loss: 3.95734, accuracy: 0.58673\n","Epoch: 14/50, step: 50/83, loss: 3.95543, accuracy: 0.57750\n","Epoch: 14/50, step: 51/83, loss: 4.01791, accuracy: 0.57108\n","Epoch: 14/50, step: 52/83, loss: 4.01033, accuracy: 0.56731\n","Epoch: 14/50, step: 53/83, loss: 4.02152, accuracy: 0.56132\n","Epoch: 14/50, step: 54/83, loss: 4.00859, accuracy: 0.56019\n","Epoch: 14/50, step: 55/83, loss: 3.98330, accuracy: 0.56364\n","Epoch: 14/50, step: 56/83, loss: 3.97200, accuracy: 0.56696\n","Epoch: 14/50, step: 57/83, loss: 4.02816, accuracy: 0.56140\n","Epoch: 14/50, step: 58/83, loss: 4.03193, accuracy: 0.56250\n","Epoch: 14/50, step: 59/83, loss: 4.00540, accuracy: 0.56356\n","Epoch: 14/50, step: 60/83, loss: 3.98164, accuracy: 0.56458\n","Epoch: 14/50, step: 61/83, loss: 3.95689, accuracy: 0.56762\n","Epoch: 14/50, step: 62/83, loss: 3.93314, accuracy: 0.57258\n","Epoch: 14/50, step: 63/83, loss: 3.93112, accuracy: 0.57341\n","Epoch: 14/50, step: 64/83, loss: 3.91330, accuracy: 0.57227\n","Epoch: 14/50, step: 65/83, loss: 3.90586, accuracy: 0.57500\n","Epoch: 14/50, step: 66/83, loss: 3.89674, accuracy: 0.57576\n","Epoch: 14/50, step: 67/83, loss: 3.90230, accuracy: 0.57836\n","Epoch: 14/50, step: 68/83, loss: 3.90413, accuracy: 0.57721\n","Epoch: 14/50, step: 69/83, loss: 3.89978, accuracy: 0.57971\n","Epoch: 14/50, step: 70/83, loss: 3.91403, accuracy: 0.57679\n","Epoch: 14/50, step: 71/83, loss: 3.89191, accuracy: 0.57746\n","Epoch: 14/50, step: 72/83, loss: 3.85988, accuracy: 0.58160\n","Epoch: 14/50, step: 73/83, loss: 3.86448, accuracy: 0.58390\n","Epoch: 14/50, step: 74/83, loss: 3.91942, accuracy: 0.58108\n","Epoch: 14/50, step: 75/83, loss: 3.93963, accuracy: 0.57833\n","Epoch: 14/50, step: 76/83, loss: 3.94681, accuracy: 0.57730\n","Epoch: 14/50, step: 77/83, loss: 3.96395, accuracy: 0.57468\n","Epoch: 14/50, step: 78/83, loss: 3.97919, accuracy: 0.57372\n","Epoch: 14/50, step: 79/83, loss: 3.98363, accuracy: 0.57278\n","Epoch: 14/50, step: 80/83, loss: 3.99652, accuracy: 0.57656\n","Epoch: 14/50, step: 81/83, loss: 4.00268, accuracy: 0.57407\n","Epoch: 14/50, step: 82/83, loss: 4.00552, accuracy: 0.57622\n","Epoch: 14/50, step: 83/83, loss: 4.04929, accuracy: 0.57686\n","Epoch: 14/50, train loss: 4.04929, train accuracy: 0.57686, valid loss: 10.18577, valid accuracy: 0.23077\n","Epoch: 15/50, step: 1/83, loss: 4.26792, accuracy: 0.75000\n","Epoch: 15/50, step: 2/83, loss: 3.32130, accuracy: 0.75000\n","Epoch: 15/50, step: 3/83, loss: 3.39160, accuracy: 0.70833\n","Epoch: 15/50, step: 4/83, loss: 3.33207, accuracy: 0.71875\n","Epoch: 15/50, step: 5/83, loss: 3.52451, accuracy: 0.70000\n","Epoch: 15/50, step: 6/83, loss: 3.77193, accuracy: 0.70833\n","Epoch: 15/50, step: 7/83, loss: 4.06320, accuracy: 0.66071\n","Epoch: 15/50, step: 8/83, loss: 3.79825, accuracy: 0.65625\n","Epoch: 15/50, step: 9/83, loss: 3.80047, accuracy: 0.61111\n","Epoch: 15/50, step: 10/83, loss: 3.66255, accuracy: 0.62500\n","Epoch: 15/50, step: 11/83, loss: 3.66366, accuracy: 0.61364\n","Epoch: 15/50, step: 12/83, loss: 3.80041, accuracy: 0.61458\n","Epoch: 15/50, step: 13/83, loss: 3.81816, accuracy: 0.63462\n","Epoch: 15/50, step: 14/83, loss: 3.79072, accuracy: 0.64286\n","Epoch: 15/50, step: 15/83, loss: 3.74227, accuracy: 0.65000\n","Epoch: 15/50, step: 16/83, loss: 3.77174, accuracy: 0.64844\n","Epoch: 15/50, step: 17/83, loss: 3.73123, accuracy: 0.64706\n","Epoch: 15/50, step: 18/83, loss: 3.72270, accuracy: 0.64583\n","Epoch: 15/50, step: 19/83, loss: 3.70081, accuracy: 0.64474\n","Epoch: 15/50, step: 20/83, loss: 3.71197, accuracy: 0.63125\n","Epoch: 15/50, step: 21/83, loss: 3.65921, accuracy: 0.63095\n","Epoch: 15/50, step: 22/83, loss: 3.60243, accuracy: 0.63636\n","Epoch: 15/50, step: 23/83, loss: 3.62938, accuracy: 0.63043\n","Epoch: 15/50, step: 24/83, loss: 3.69308, accuracy: 0.63021\n","Epoch: 15/50, step: 25/83, loss: 3.75942, accuracy: 0.62000\n","Epoch: 15/50, step: 26/83, loss: 3.74139, accuracy: 0.62019\n","Epoch: 15/50, step: 27/83, loss: 3.79282, accuracy: 0.62500\n","Epoch: 15/50, step: 28/83, loss: 3.82117, accuracy: 0.61607\n","Epoch: 15/50, step: 29/83, loss: 3.84583, accuracy: 0.61638\n","Epoch: 15/50, step: 30/83, loss: 3.83619, accuracy: 0.61667\n","Epoch: 15/50, step: 31/83, loss: 3.96118, accuracy: 0.60484\n","Epoch: 15/50, step: 32/83, loss: 3.91898, accuracy: 0.60547\n","Epoch: 15/50, step: 33/83, loss: 3.90792, accuracy: 0.60606\n","Epoch: 15/50, step: 34/83, loss: 3.92062, accuracy: 0.61029\n","Epoch: 15/50, step: 35/83, loss: 3.92849, accuracy: 0.60714\n","Epoch: 15/50, step: 36/83, loss: 3.91216, accuracy: 0.61111\n","Epoch: 15/50, step: 37/83, loss: 3.94912, accuracy: 0.61149\n","Epoch: 15/50, step: 38/83, loss: 3.94240, accuracy: 0.60197\n","Epoch: 15/50, step: 39/83, loss: 3.92743, accuracy: 0.60897\n","Epoch: 15/50, step: 40/83, loss: 3.92936, accuracy: 0.61250\n","Epoch: 15/50, step: 41/83, loss: 3.88983, accuracy: 0.61585\n","Epoch: 15/50, step: 42/83, loss: 3.84763, accuracy: 0.62202\n","Epoch: 15/50, step: 43/83, loss: 3.88129, accuracy: 0.61919\n","Epoch: 15/50, step: 44/83, loss: 3.83031, accuracy: 0.62216\n","Epoch: 15/50, step: 45/83, loss: 3.90446, accuracy: 0.61389\n","Epoch: 15/50, step: 46/83, loss: 3.93443, accuracy: 0.60870\n","Epoch: 15/50, step: 47/83, loss: 3.97979, accuracy: 0.60372\n","Epoch: 15/50, step: 48/83, loss: 3.97967, accuracy: 0.60417\n","Epoch: 15/50, step: 49/83, loss: 3.99832, accuracy: 0.60459\n","Epoch: 15/50, step: 50/83, loss: 4.00407, accuracy: 0.60250\n","Epoch: 15/50, step: 51/83, loss: 3.95828, accuracy: 0.60539\n","Epoch: 15/50, step: 52/83, loss: 4.02213, accuracy: 0.60096\n","Epoch: 15/50, step: 53/83, loss: 4.04398, accuracy: 0.60142\n","Epoch: 15/50, step: 54/83, loss: 4.05035, accuracy: 0.60185\n","Epoch: 15/50, step: 55/83, loss: 4.04027, accuracy: 0.60000\n","Epoch: 15/50, step: 56/83, loss: 3.99638, accuracy: 0.60268\n","Epoch: 15/50, step: 57/83, loss: 4.03797, accuracy: 0.59868\n","Epoch: 15/50, step: 58/83, loss: 3.99051, accuracy: 0.60345\n","Epoch: 15/50, step: 59/83, loss: 3.98150, accuracy: 0.60381\n","Epoch: 15/50, step: 60/83, loss: 3.95685, accuracy: 0.60625\n","Epoch: 15/50, step: 61/83, loss: 3.93686, accuracy: 0.60246\n","Epoch: 15/50, step: 62/83, loss: 3.92757, accuracy: 0.60081\n","Epoch: 15/50, step: 63/83, loss: 3.90452, accuracy: 0.60516\n","Epoch: 15/50, step: 64/83, loss: 3.90967, accuracy: 0.60352\n","Epoch: 15/50, step: 65/83, loss: 3.91242, accuracy: 0.60385\n","Epoch: 15/50, step: 66/83, loss: 3.89324, accuracy: 0.60227\n","Epoch: 15/50, step: 67/83, loss: 3.90304, accuracy: 0.59888\n","Epoch: 15/50, step: 68/83, loss: 3.89666, accuracy: 0.59743\n","Epoch: 15/50, step: 69/83, loss: 3.89299, accuracy: 0.59964\n","Epoch: 15/50, step: 70/83, loss: 3.90245, accuracy: 0.59821\n","Epoch: 15/50, step: 71/83, loss: 3.89610, accuracy: 0.59683\n","Epoch: 15/50, step: 72/83, loss: 3.88919, accuracy: 0.59722\n","Epoch: 15/50, step: 73/83, loss: 3.90936, accuracy: 0.59075\n","Epoch: 15/50, step: 74/83, loss: 3.88686, accuracy: 0.59459\n","Epoch: 15/50, step: 75/83, loss: 3.87811, accuracy: 0.59500\n","Epoch: 15/50, step: 76/83, loss: 3.87475, accuracy: 0.59211\n","Epoch: 15/50, step: 77/83, loss: 3.89016, accuracy: 0.59416\n","Epoch: 15/50, step: 78/83, loss: 3.88583, accuracy: 0.59295\n","Epoch: 15/50, step: 79/83, loss: 3.89316, accuracy: 0.59335\n","Epoch: 15/50, step: 80/83, loss: 3.92150, accuracy: 0.59062\n","Epoch: 15/50, step: 81/83, loss: 3.93825, accuracy: 0.59259\n","Epoch: 15/50, step: 82/83, loss: 3.95725, accuracy: 0.58841\n","Epoch: 15/50, step: 83/83, loss: 4.01403, accuracy: 0.58752\n","Epoch: 15/50, train loss: 4.01403, train accuracy: 0.58752, valid loss: 10.34300, valid accuracy: 0.21267\n","Epoch: 16/50, step: 1/83, loss: 3.49896, accuracy: 0.50000\n","Epoch: 16/50, step: 2/83, loss: 3.99692, accuracy: 0.56250\n","Epoch: 16/50, step: 3/83, loss: 4.03476, accuracy: 0.54167\n","Epoch: 16/50, step: 4/83, loss: 3.82014, accuracy: 0.59375\n","Epoch: 16/50, step: 5/83, loss: 3.73468, accuracy: 0.60000\n","Epoch: 16/50, step: 6/83, loss: 3.67778, accuracy: 0.60417\n","Epoch: 16/50, step: 7/83, loss: 3.74753, accuracy: 0.62500\n","Epoch: 16/50, step: 8/83, loss: 3.58750, accuracy: 0.62500\n","Epoch: 16/50, step: 9/83, loss: 3.85526, accuracy: 0.58333\n","Epoch: 16/50, step: 10/83, loss: 3.72487, accuracy: 0.60000\n","Epoch: 16/50, step: 11/83, loss: 3.87955, accuracy: 0.57955\n","Epoch: 16/50, step: 12/83, loss: 3.94150, accuracy: 0.56250\n","Epoch: 16/50, step: 13/83, loss: 3.82649, accuracy: 0.57692\n","Epoch: 16/50, step: 14/83, loss: 3.78899, accuracy: 0.59821\n","Epoch: 16/50, step: 15/83, loss: 3.64588, accuracy: 0.60833\n","Epoch: 16/50, step: 16/83, loss: 3.75812, accuracy: 0.58594\n","Epoch: 16/50, step: 17/83, loss: 3.78486, accuracy: 0.58824\n","Epoch: 16/50, step: 18/83, loss: 3.72260, accuracy: 0.58333\n","Epoch: 16/50, step: 19/83, loss: 3.75868, accuracy: 0.57237\n","Epoch: 16/50, step: 20/83, loss: 3.68816, accuracy: 0.57500\n","Epoch: 16/50, step: 21/83, loss: 3.68373, accuracy: 0.55952\n","Epoch: 16/50, step: 22/83, loss: 3.81132, accuracy: 0.54545\n","Epoch: 16/50, step: 23/83, loss: 3.78382, accuracy: 0.55978\n","Epoch: 16/50, step: 24/83, loss: 3.77298, accuracy: 0.56771\n","Epoch: 16/50, step: 25/83, loss: 3.82419, accuracy: 0.57500\n","Epoch: 16/50, step: 26/83, loss: 4.01968, accuracy: 0.56731\n","Epoch: 16/50, step: 27/83, loss: 4.02672, accuracy: 0.56481\n","Epoch: 16/50, step: 28/83, loss: 4.04257, accuracy: 0.56696\n","Epoch: 16/50, step: 29/83, loss: 3.98403, accuracy: 0.56034\n","Epoch: 16/50, step: 30/83, loss: 4.06270, accuracy: 0.56250\n","Epoch: 16/50, step: 31/83, loss: 4.07709, accuracy: 0.56452\n","Epoch: 16/50, step: 32/83, loss: 4.00397, accuracy: 0.57422\n","Epoch: 16/50, step: 33/83, loss: 3.96247, accuracy: 0.57197\n","Epoch: 16/50, step: 34/83, loss: 3.97940, accuracy: 0.56250\n","Epoch: 16/50, step: 35/83, loss: 3.93974, accuracy: 0.56071\n","Epoch: 16/50, step: 36/83, loss: 3.94668, accuracy: 0.56250\n","Epoch: 16/50, step: 37/83, loss: 3.98476, accuracy: 0.55743\n","Epoch: 16/50, step: 38/83, loss: 3.96583, accuracy: 0.56579\n","Epoch: 16/50, step: 39/83, loss: 3.97142, accuracy: 0.56731\n","Epoch: 16/50, step: 40/83, loss: 3.99781, accuracy: 0.57187\n","Epoch: 16/50, step: 41/83, loss: 4.00811, accuracy: 0.57012\n","Epoch: 16/50, step: 42/83, loss: 3.97040, accuracy: 0.56845\n","Epoch: 16/50, step: 43/83, loss: 4.03451, accuracy: 0.56395\n","Epoch: 16/50, step: 44/83, loss: 4.01797, accuracy: 0.56534\n","Epoch: 16/50, step: 45/83, loss: 4.02168, accuracy: 0.56944\n","Epoch: 16/50, step: 46/83, loss: 4.00552, accuracy: 0.57609\n","Epoch: 16/50, step: 47/83, loss: 3.98847, accuracy: 0.57979\n","Epoch: 16/50, step: 48/83, loss: 3.99632, accuracy: 0.57292\n","Epoch: 16/50, step: 49/83, loss: 3.96083, accuracy: 0.58163\n","Epoch: 16/50, step: 50/83, loss: 3.95403, accuracy: 0.57750\n","Epoch: 16/50, step: 51/83, loss: 3.92332, accuracy: 0.58088\n","Epoch: 16/50, step: 52/83, loss: 3.94471, accuracy: 0.58173\n","Epoch: 16/50, step: 53/83, loss: 3.98567, accuracy: 0.57547\n","Epoch: 16/50, step: 54/83, loss: 3.95443, accuracy: 0.57870\n","Epoch: 16/50, step: 55/83, loss: 3.98119, accuracy: 0.57500\n","Epoch: 16/50, step: 56/83, loss: 4.01146, accuracy: 0.56696\n","Epoch: 16/50, step: 57/83, loss: 3.99742, accuracy: 0.56579\n","Epoch: 16/50, step: 58/83, loss: 4.05274, accuracy: 0.56034\n","Epoch: 16/50, step: 59/83, loss: 4.07232, accuracy: 0.56356\n","Epoch: 16/50, step: 60/83, loss: 4.04735, accuracy: 0.56667\n","Epoch: 16/50, step: 61/83, loss: 4.00711, accuracy: 0.56967\n","Epoch: 16/50, step: 62/83, loss: 4.01641, accuracy: 0.56855\n","Epoch: 16/50, step: 63/83, loss: 4.04505, accuracy: 0.56548\n","Epoch: 16/50, step: 64/83, loss: 4.03540, accuracy: 0.56641\n","Epoch: 16/50, step: 65/83, loss: 4.01371, accuracy: 0.56731\n","Epoch: 16/50, step: 66/83, loss: 4.00338, accuracy: 0.56818\n","Epoch: 16/50, step: 67/83, loss: 3.96310, accuracy: 0.57090\n","Epoch: 16/50, step: 68/83, loss: 3.96962, accuracy: 0.57169\n","Epoch: 16/50, step: 69/83, loss: 4.01577, accuracy: 0.57065\n","Epoch: 16/50, step: 70/83, loss: 4.00814, accuracy: 0.57321\n","Epoch: 16/50, step: 71/83, loss: 3.97461, accuracy: 0.57746\n","Epoch: 16/50, step: 72/83, loss: 3.99629, accuracy: 0.57465\n","Epoch: 16/50, step: 73/83, loss: 3.97884, accuracy: 0.57363\n","Epoch: 16/50, step: 74/83, loss: 4.00707, accuracy: 0.57432\n","Epoch: 16/50, step: 75/83, loss: 4.01377, accuracy: 0.57167\n","Epoch: 16/50, step: 76/83, loss: 3.99689, accuracy: 0.57237\n","Epoch: 16/50, step: 77/83, loss: 3.97613, accuracy: 0.57143\n","Epoch: 16/50, step: 78/83, loss: 3.96421, accuracy: 0.57372\n","Epoch: 16/50, step: 79/83, loss: 3.97152, accuracy: 0.57437\n","Epoch: 16/50, step: 80/83, loss: 3.96382, accuracy: 0.57812\n","Epoch: 16/50, step: 81/83, loss: 4.00194, accuracy: 0.57716\n","Epoch: 16/50, step: 82/83, loss: 4.01870, accuracy: 0.57622\n","Epoch: 16/50, step: 83/83, loss: 4.06273, accuracy: 0.57686\n","Epoch: 16/50, train loss: 4.06273, train accuracy: 0.57686, valid loss: 10.30913, valid accuracy: 0.20814\n","Epoch: 17/50, step: 1/83, loss: 2.73351, accuracy: 0.50000\n","Epoch: 17/50, step: 2/83, loss: 3.58166, accuracy: 0.56250\n","Epoch: 17/50, step: 3/83, loss: 2.60503, accuracy: 0.62500\n","Epoch: 17/50, step: 4/83, loss: 3.44961, accuracy: 0.62500\n","Epoch: 17/50, step: 5/83, loss: 3.98973, accuracy: 0.67500\n","Epoch: 17/50, step: 6/83, loss: 4.08769, accuracy: 0.64583\n","Epoch: 17/50, step: 7/83, loss: 4.14831, accuracy: 0.62500\n","Epoch: 17/50, step: 8/83, loss: 4.21028, accuracy: 0.59375\n","Epoch: 17/50, step: 9/83, loss: 4.10518, accuracy: 0.62500\n","Epoch: 17/50, step: 10/83, loss: 4.03256, accuracy: 0.62500\n","Epoch: 17/50, step: 11/83, loss: 4.33341, accuracy: 0.59091\n","Epoch: 17/50, step: 12/83, loss: 4.37113, accuracy: 0.57292\n","Epoch: 17/50, step: 13/83, loss: 4.17575, accuracy: 0.56731\n","Epoch: 17/50, step: 14/83, loss: 4.38207, accuracy: 0.55357\n","Epoch: 17/50, step: 15/83, loss: 4.32353, accuracy: 0.55833\n","Epoch: 17/50, step: 16/83, loss: 4.11369, accuracy: 0.57031\n","Epoch: 17/50, step: 17/83, loss: 4.14579, accuracy: 0.57353\n","Epoch: 17/50, step: 18/83, loss: 4.19608, accuracy: 0.56250\n","Epoch: 17/50, step: 19/83, loss: 4.09916, accuracy: 0.56579\n","Epoch: 17/50, step: 20/83, loss: 4.08654, accuracy: 0.55625\n","Epoch: 17/50, step: 21/83, loss: 4.00325, accuracy: 0.57143\n","Epoch: 17/50, step: 22/83, loss: 3.97306, accuracy: 0.57955\n","Epoch: 17/50, step: 23/83, loss: 4.02268, accuracy: 0.57065\n","Epoch: 17/50, step: 24/83, loss: 4.02734, accuracy: 0.56771\n","Epoch: 17/50, step: 25/83, loss: 4.07515, accuracy: 0.56000\n","Epoch: 17/50, step: 26/83, loss: 4.10336, accuracy: 0.56731\n","Epoch: 17/50, step: 27/83, loss: 4.02131, accuracy: 0.57407\n","Epoch: 17/50, step: 28/83, loss: 3.93375, accuracy: 0.58482\n","Epoch: 17/50, step: 29/83, loss: 3.91492, accuracy: 0.59052\n","Epoch: 17/50, step: 30/83, loss: 3.84623, accuracy: 0.59167\n","Epoch: 17/50, step: 31/83, loss: 3.80359, accuracy: 0.59274\n","Epoch: 17/50, step: 32/83, loss: 3.73578, accuracy: 0.59766\n","Epoch: 17/50, step: 33/83, loss: 3.75313, accuracy: 0.59848\n","Epoch: 17/50, step: 34/83, loss: 3.76508, accuracy: 0.60294\n","Epoch: 17/50, step: 35/83, loss: 3.75273, accuracy: 0.60714\n","Epoch: 17/50, step: 36/83, loss: 3.73717, accuracy: 0.61111\n","Epoch: 17/50, step: 37/83, loss: 3.70761, accuracy: 0.60811\n","Epoch: 17/50, step: 38/83, loss: 3.69176, accuracy: 0.61513\n","Epoch: 17/50, step: 39/83, loss: 3.63743, accuracy: 0.61538\n","Epoch: 17/50, step: 40/83, loss: 3.65277, accuracy: 0.61563\n","Epoch: 17/50, step: 41/83, loss: 3.63897, accuracy: 0.61890\n","Epoch: 17/50, step: 42/83, loss: 3.62341, accuracy: 0.61607\n","Epoch: 17/50, step: 43/83, loss: 3.61947, accuracy: 0.61919\n","Epoch: 17/50, step: 44/83, loss: 3.70261, accuracy: 0.61648\n","Epoch: 17/50, step: 45/83, loss: 3.67659, accuracy: 0.61667\n","Epoch: 17/50, step: 46/83, loss: 3.66787, accuracy: 0.61413\n","Epoch: 17/50, step: 47/83, loss: 3.66594, accuracy: 0.61170\n","Epoch: 17/50, step: 48/83, loss: 3.64666, accuracy: 0.61198\n","Epoch: 17/50, step: 49/83, loss: 3.62221, accuracy: 0.61480\n","Epoch: 17/50, step: 50/83, loss: 3.63997, accuracy: 0.61250\n","Epoch: 17/50, step: 51/83, loss: 3.68932, accuracy: 0.60539\n","Epoch: 17/50, step: 52/83, loss: 3.69936, accuracy: 0.60337\n","Epoch: 17/50, step: 53/83, loss: 3.69216, accuracy: 0.60377\n","Epoch: 17/50, step: 54/83, loss: 3.67290, accuracy: 0.60185\n","Epoch: 17/50, step: 55/83, loss: 3.67138, accuracy: 0.60227\n","Epoch: 17/50, step: 56/83, loss: 3.65134, accuracy: 0.60045\n","Epoch: 17/50, step: 57/83, loss: 3.65016, accuracy: 0.60088\n","Epoch: 17/50, step: 58/83, loss: 3.66036, accuracy: 0.60345\n","Epoch: 17/50, step: 59/83, loss: 3.61670, accuracy: 0.60169\n","Epoch: 17/50, step: 60/83, loss: 3.59563, accuracy: 0.60208\n","Epoch: 17/50, step: 61/83, loss: 3.57625, accuracy: 0.60656\n","Epoch: 17/50, step: 62/83, loss: 3.57414, accuracy: 0.60685\n","Epoch: 17/50, step: 63/83, loss: 3.57013, accuracy: 0.60913\n","Epoch: 17/50, step: 64/83, loss: 3.54594, accuracy: 0.61328\n","Epoch: 17/50, step: 65/83, loss: 3.56808, accuracy: 0.61346\n","Epoch: 17/50, step: 66/83, loss: 3.57058, accuracy: 0.60985\n","Epoch: 17/50, step: 67/83, loss: 3.59259, accuracy: 0.61381\n","Epoch: 17/50, step: 68/83, loss: 3.58809, accuracy: 0.61029\n","Epoch: 17/50, step: 69/83, loss: 3.60681, accuracy: 0.61051\n","Epoch: 17/50, step: 70/83, loss: 3.59104, accuracy: 0.61071\n","Epoch: 17/50, step: 71/83, loss: 3.60316, accuracy: 0.61092\n","Epoch: 17/50, step: 72/83, loss: 3.57814, accuracy: 0.60938\n","Epoch: 17/50, step: 73/83, loss: 3.59092, accuracy: 0.60959\n","Epoch: 17/50, step: 74/83, loss: 3.58003, accuracy: 0.60473\n","Epoch: 17/50, step: 75/83, loss: 3.61788, accuracy: 0.60000\n","Epoch: 17/50, step: 76/83, loss: 3.64457, accuracy: 0.60197\n","Epoch: 17/50, step: 77/83, loss: 3.62970, accuracy: 0.60390\n","Epoch: 17/50, step: 78/83, loss: 3.62645, accuracy: 0.60096\n","Epoch: 17/50, step: 79/83, loss: 3.60100, accuracy: 0.60443\n","Epoch: 17/50, step: 80/83, loss: 3.59060, accuracy: 0.60312\n","Epoch: 17/50, step: 81/83, loss: 3.62308, accuracy: 0.60031\n","Epoch: 17/50, step: 82/83, loss: 3.63505, accuracy: 0.59909\n","Epoch: 17/50, step: 83/83, loss: 3.68680, accuracy: 0.59817\n","Epoch: 17/50, train loss: 3.68680, train accuracy: 0.59817, valid loss: 10.27011, valid accuracy: 0.20362\n","Epoch: 18/50, step: 1/83, loss: 4.49970, accuracy: 0.62500\n","Epoch: 18/50, step: 2/83, loss: 4.11895, accuracy: 0.56250\n","Epoch: 18/50, step: 3/83, loss: 3.65277, accuracy: 0.54167\n","Epoch: 18/50, step: 4/83, loss: 3.58611, accuracy: 0.59375\n","Epoch: 18/50, step: 5/83, loss: 3.95867, accuracy: 0.55000\n","Epoch: 18/50, step: 6/83, loss: 3.87047, accuracy: 0.58333\n","Epoch: 18/50, step: 7/83, loss: 3.96719, accuracy: 0.58929\n","Epoch: 18/50, step: 8/83, loss: 3.90564, accuracy: 0.57812\n","Epoch: 18/50, step: 9/83, loss: 3.86117, accuracy: 0.56944\n","Epoch: 18/50, step: 10/83, loss: 3.81763, accuracy: 0.56250\n","Epoch: 18/50, step: 11/83, loss: 3.61400, accuracy: 0.59091\n","Epoch: 18/50, step: 12/83, loss: 3.77610, accuracy: 0.57292\n","Epoch: 18/50, step: 13/83, loss: 3.53010, accuracy: 0.59615\n","Epoch: 18/50, step: 14/83, loss: 3.59761, accuracy: 0.58929\n","Epoch: 18/50, step: 15/83, loss: 3.64611, accuracy: 0.58333\n","Epoch: 18/50, step: 16/83, loss: 3.73733, accuracy: 0.58594\n","Epoch: 18/50, step: 17/83, loss: 3.88382, accuracy: 0.58824\n","Epoch: 18/50, step: 18/83, loss: 3.76238, accuracy: 0.59722\n","Epoch: 18/50, step: 19/83, loss: 3.78468, accuracy: 0.60526\n","Epoch: 18/50, step: 20/83, loss: 3.74065, accuracy: 0.61250\n","Epoch: 18/50, step: 21/83, loss: 3.67900, accuracy: 0.62500\n","Epoch: 18/50, step: 22/83, loss: 3.70143, accuracy: 0.63068\n","Epoch: 18/50, step: 23/83, loss: 3.71817, accuracy: 0.63587\n","Epoch: 18/50, step: 24/83, loss: 3.65369, accuracy: 0.64583\n","Epoch: 18/50, step: 25/83, loss: 3.61629, accuracy: 0.65000\n","Epoch: 18/50, step: 26/83, loss: 3.62439, accuracy: 0.64904\n","Epoch: 18/50, step: 27/83, loss: 3.72457, accuracy: 0.63889\n","Epoch: 18/50, step: 28/83, loss: 3.61120, accuracy: 0.64286\n","Epoch: 18/50, step: 29/83, loss: 3.64120, accuracy: 0.63793\n","Epoch: 18/50, step: 30/83, loss: 3.56845, accuracy: 0.65000\n","Epoch: 18/50, step: 31/83, loss: 3.53027, accuracy: 0.65323\n","Epoch: 18/50, step: 32/83, loss: 3.53625, accuracy: 0.65234\n","Epoch: 18/50, step: 33/83, loss: 3.53544, accuracy: 0.64773\n","Epoch: 18/50, step: 34/83, loss: 3.50457, accuracy: 0.64706\n","Epoch: 18/50, step: 35/83, loss: 3.47621, accuracy: 0.64643\n","Epoch: 18/50, step: 36/83, loss: 3.52849, accuracy: 0.63889\n","Epoch: 18/50, step: 37/83, loss: 3.50130, accuracy: 0.63851\n","Epoch: 18/50, step: 38/83, loss: 3.51537, accuracy: 0.63487\n","Epoch: 18/50, step: 39/83, loss: 3.60715, accuracy: 0.62821\n","Epoch: 18/50, step: 40/83, loss: 3.55471, accuracy: 0.63437\n","Epoch: 18/50, step: 41/83, loss: 3.50612, accuracy: 0.63720\n","Epoch: 18/50, step: 42/83, loss: 3.51766, accuracy: 0.63988\n","Epoch: 18/50, step: 43/83, loss: 3.51887, accuracy: 0.63663\n","Epoch: 18/50, step: 44/83, loss: 3.55472, accuracy: 0.63920\n","Epoch: 18/50, step: 45/83, loss: 3.55162, accuracy: 0.63611\n","Epoch: 18/50, step: 46/83, loss: 3.57003, accuracy: 0.63859\n","Epoch: 18/50, step: 47/83, loss: 3.61265, accuracy: 0.63032\n","Epoch: 18/50, step: 48/83, loss: 3.64325, accuracy: 0.62760\n","Epoch: 18/50, step: 49/83, loss: 3.65849, accuracy: 0.62245\n","Epoch: 18/50, step: 50/83, loss: 3.64985, accuracy: 0.62500\n","Epoch: 18/50, step: 51/83, loss: 3.65893, accuracy: 0.62255\n","Epoch: 18/50, step: 52/83, loss: 3.64201, accuracy: 0.61779\n","Epoch: 18/50, step: 53/83, loss: 3.65425, accuracy: 0.61321\n","Epoch: 18/50, step: 54/83, loss: 3.66709, accuracy: 0.61343\n","Epoch: 18/50, step: 55/83, loss: 3.71464, accuracy: 0.60682\n","Epoch: 18/50, step: 56/83, loss: 3.74462, accuracy: 0.60268\n","Epoch: 18/50, step: 57/83, loss: 3.75273, accuracy: 0.60088\n","Epoch: 18/50, step: 58/83, loss: 3.71452, accuracy: 0.60776\n","Epoch: 18/50, step: 59/83, loss: 3.75598, accuracy: 0.60805\n","Epoch: 18/50, step: 60/83, loss: 3.79417, accuracy: 0.60625\n","Epoch: 18/50, step: 61/83, loss: 3.76868, accuracy: 0.61066\n","Epoch: 18/50, step: 62/83, loss: 3.76220, accuracy: 0.61290\n","Epoch: 18/50, step: 63/83, loss: 3.74211, accuracy: 0.61508\n","Epoch: 18/50, step: 64/83, loss: 3.72083, accuracy: 0.61523\n","Epoch: 18/50, step: 65/83, loss: 3.68817, accuracy: 0.61346\n","Epoch: 18/50, step: 66/83, loss: 3.70638, accuracy: 0.61364\n","Epoch: 18/50, step: 67/83, loss: 3.72542, accuracy: 0.61381\n","Epoch: 18/50, step: 68/83, loss: 3.69411, accuracy: 0.61581\n","Epoch: 18/50, step: 69/83, loss: 3.70512, accuracy: 0.61232\n","Epoch: 18/50, step: 70/83, loss: 3.69545, accuracy: 0.61250\n","Epoch: 18/50, step: 71/83, loss: 3.68707, accuracy: 0.61444\n","Epoch: 18/50, step: 72/83, loss: 3.70964, accuracy: 0.61285\n","Epoch: 18/50, step: 73/83, loss: 3.72921, accuracy: 0.60959\n","Epoch: 18/50, step: 74/83, loss: 3.72758, accuracy: 0.60642\n","Epoch: 18/50, step: 75/83, loss: 3.72693, accuracy: 0.60500\n","Epoch: 18/50, step: 76/83, loss: 3.73495, accuracy: 0.60362\n","Epoch: 18/50, step: 77/83, loss: 3.73152, accuracy: 0.60390\n","Epoch: 18/50, step: 78/83, loss: 3.74089, accuracy: 0.60096\n","Epoch: 18/50, step: 79/83, loss: 3.72751, accuracy: 0.60285\n","Epoch: 18/50, step: 80/83, loss: 3.72583, accuracy: 0.60156\n","Epoch: 18/50, step: 81/83, loss: 3.73342, accuracy: 0.60185\n","Epoch: 18/50, step: 82/83, loss: 3.72041, accuracy: 0.60366\n","Epoch: 18/50, step: 83/83, loss: 3.77466, accuracy: 0.60426\n","Epoch: 18/50, train loss: 3.77466, train accuracy: 0.60426, valid loss: 10.25624, valid accuracy: 0.21719\n","Epoch: 19/50, step: 1/83, loss: 3.35208, accuracy: 0.75000\n","Epoch: 19/50, step: 2/83, loss: 4.25920, accuracy: 0.68750\n","Epoch: 19/50, step: 3/83, loss: 3.38454, accuracy: 0.66667\n","Epoch: 19/50, step: 4/83, loss: 3.37217, accuracy: 0.68750\n","Epoch: 19/50, step: 5/83, loss: 2.86857, accuracy: 0.65000\n","Epoch: 19/50, step: 6/83, loss: 3.24440, accuracy: 0.66667\n","Epoch: 19/50, step: 7/83, loss: 4.08555, accuracy: 0.60714\n","Epoch: 19/50, step: 8/83, loss: 4.15248, accuracy: 0.57812\n","Epoch: 19/50, step: 9/83, loss: 4.06496, accuracy: 0.58333\n","Epoch: 19/50, step: 10/83, loss: 3.91711, accuracy: 0.60000\n","Epoch: 19/50, step: 11/83, loss: 3.76937, accuracy: 0.61364\n","Epoch: 19/50, step: 12/83, loss: 3.78173, accuracy: 0.60417\n","Epoch: 19/50, step: 13/83, loss: 3.60498, accuracy: 0.62500\n","Epoch: 19/50, step: 14/83, loss: 3.72697, accuracy: 0.60714\n","Epoch: 19/50, step: 15/83, loss: 3.75648, accuracy: 0.60833\n","Epoch: 19/50, step: 16/83, loss: 3.77384, accuracy: 0.60938\n","Epoch: 19/50, step: 17/83, loss: 3.63404, accuracy: 0.61765\n","Epoch: 19/50, step: 18/83, loss: 3.62472, accuracy: 0.61111\n","Epoch: 19/50, step: 19/83, loss: 3.62135, accuracy: 0.60526\n","Epoch: 19/50, step: 20/83, loss: 3.69480, accuracy: 0.61875\n","Epoch: 19/50, step: 21/83, loss: 3.77767, accuracy: 0.60714\n","Epoch: 19/50, step: 22/83, loss: 3.82635, accuracy: 0.60227\n","Epoch: 19/50, step: 23/83, loss: 3.77627, accuracy: 0.60326\n","Epoch: 19/50, step: 24/83, loss: 3.81441, accuracy: 0.59896\n","Epoch: 19/50, step: 25/83, loss: 3.78639, accuracy: 0.60500\n","Epoch: 19/50, step: 26/83, loss: 3.84251, accuracy: 0.60096\n","Epoch: 19/50, step: 27/83, loss: 3.79914, accuracy: 0.60185\n","Epoch: 19/50, step: 28/83, loss: 3.83657, accuracy: 0.59375\n","Epoch: 19/50, step: 29/83, loss: 3.79258, accuracy: 0.59483\n","Epoch: 19/50, step: 30/83, loss: 3.76357, accuracy: 0.60000\n","Epoch: 19/50, step: 31/83, loss: 3.90004, accuracy: 0.59274\n","Epoch: 19/50, step: 32/83, loss: 3.88775, accuracy: 0.59375\n","Epoch: 19/50, step: 33/83, loss: 3.86509, accuracy: 0.60227\n","Epoch: 19/50, step: 34/83, loss: 3.86100, accuracy: 0.59926\n","Epoch: 19/50, step: 35/83, loss: 3.82621, accuracy: 0.60000\n","Epoch: 19/50, step: 36/83, loss: 3.79432, accuracy: 0.60417\n","Epoch: 19/50, step: 37/83, loss: 3.83243, accuracy: 0.59797\n","Epoch: 19/50, step: 38/83, loss: 3.86580, accuracy: 0.59211\n","Epoch: 19/50, step: 39/83, loss: 3.82578, accuracy: 0.59615\n","Epoch: 19/50, step: 40/83, loss: 3.86526, accuracy: 0.59062\n","Epoch: 19/50, step: 41/83, loss: 3.85072, accuracy: 0.59451\n","Epoch: 19/50, step: 42/83, loss: 3.78855, accuracy: 0.60417\n","Epoch: 19/50, step: 43/83, loss: 3.77796, accuracy: 0.61047\n","Epoch: 19/50, step: 44/83, loss: 3.80292, accuracy: 0.61080\n","Epoch: 19/50, step: 45/83, loss: 3.82768, accuracy: 0.61389\n","Epoch: 19/50, step: 46/83, loss: 3.81273, accuracy: 0.61957\n","Epoch: 19/50, step: 47/83, loss: 3.81938, accuracy: 0.61968\n","Epoch: 19/50, step: 48/83, loss: 3.82429, accuracy: 0.61979\n","Epoch: 19/50, step: 49/83, loss: 3.79754, accuracy: 0.62500\n","Epoch: 19/50, step: 50/83, loss: 3.79403, accuracy: 0.62000\n","Epoch: 19/50, step: 51/83, loss: 3.79051, accuracy: 0.62010\n","Epoch: 19/50, step: 52/83, loss: 3.79800, accuracy: 0.62260\n","Epoch: 19/50, step: 53/83, loss: 3.77340, accuracy: 0.62736\n","Epoch: 19/50, step: 54/83, loss: 3.75010, accuracy: 0.62731\n","Epoch: 19/50, step: 55/83, loss: 3.71443, accuracy: 0.62955\n","Epoch: 19/50, step: 56/83, loss: 3.71095, accuracy: 0.62946\n","Epoch: 19/50, step: 57/83, loss: 3.72085, accuracy: 0.62719\n","Epoch: 19/50, step: 58/83, loss: 3.73164, accuracy: 0.62500\n","Epoch: 19/50, step: 59/83, loss: 3.68241, accuracy: 0.62288\n","Epoch: 19/50, step: 60/83, loss: 3.67665, accuracy: 0.62292\n","Epoch: 19/50, step: 61/83, loss: 3.66741, accuracy: 0.62705\n","Epoch: 19/50, step: 62/83, loss: 3.66290, accuracy: 0.62702\n","Epoch: 19/50, step: 63/83, loss: 3.67348, accuracy: 0.62698\n","Epoch: 19/50, step: 64/83, loss: 3.66814, accuracy: 0.62891\n","Epoch: 19/50, step: 65/83, loss: 3.70324, accuracy: 0.62885\n","Epoch: 19/50, step: 66/83, loss: 3.67020, accuracy: 0.63258\n","Epoch: 19/50, step: 67/83, loss: 3.68088, accuracy: 0.63060\n","Epoch: 19/50, step: 68/83, loss: 3.66599, accuracy: 0.63051\n","Epoch: 19/50, step: 69/83, loss: 3.70333, accuracy: 0.62681\n","Epoch: 19/50, step: 70/83, loss: 3.70202, accuracy: 0.62679\n","Epoch: 19/50, step: 71/83, loss: 3.73521, accuracy: 0.62148\n","Epoch: 19/50, step: 72/83, loss: 3.70541, accuracy: 0.62326\n","Epoch: 19/50, step: 73/83, loss: 3.71650, accuracy: 0.62158\n","Epoch: 19/50, step: 74/83, loss: 3.71595, accuracy: 0.62162\n","Epoch: 19/50, step: 75/83, loss: 3.68606, accuracy: 0.62333\n","Epoch: 19/50, step: 76/83, loss: 3.69010, accuracy: 0.62171\n","Epoch: 19/50, step: 77/83, loss: 3.71027, accuracy: 0.62013\n","Epoch: 19/50, step: 78/83, loss: 3.68605, accuracy: 0.62340\n","Epoch: 19/50, step: 79/83, loss: 3.70741, accuracy: 0.62184\n","Epoch: 19/50, step: 80/83, loss: 3.71547, accuracy: 0.61875\n","Epoch: 19/50, step: 81/83, loss: 3.73611, accuracy: 0.61574\n","Epoch: 19/50, step: 82/83, loss: 3.75299, accuracy: 0.61585\n","Epoch: 19/50, step: 83/83, loss: 3.79957, accuracy: 0.61644\n","Epoch: 19/50, train loss: 3.79957, train accuracy: 0.61644, valid loss: 9.85130, valid accuracy: 0.20814\n","Epoch: 20/50, step: 1/83, loss: 2.54790, accuracy: 0.50000\n","Epoch: 20/50, step: 2/83, loss: 2.46921, accuracy: 0.62500\n","Epoch: 20/50, step: 3/83, loss: 2.54662, accuracy: 0.62500\n","Epoch: 20/50, step: 4/83, loss: 2.50320, accuracy: 0.65625\n","Epoch: 20/50, step: 5/83, loss: 3.01185, accuracy: 0.65000\n","Epoch: 20/50, step: 6/83, loss: 3.21567, accuracy: 0.66667\n","Epoch: 20/50, step: 7/83, loss: 3.38938, accuracy: 0.66071\n","Epoch: 20/50, step: 8/83, loss: 3.28444, accuracy: 0.67188\n","Epoch: 20/50, step: 9/83, loss: 3.28731, accuracy: 0.69444\n","Epoch: 20/50, step: 10/83, loss: 3.41507, accuracy: 0.66250\n","Epoch: 20/50, step: 11/83, loss: 3.34111, accuracy: 0.64773\n","Epoch: 20/50, step: 12/83, loss: 3.27337, accuracy: 0.64583\n","Epoch: 20/50, step: 13/83, loss: 3.21222, accuracy: 0.63462\n","Epoch: 20/50, step: 14/83, loss: 3.21992, accuracy: 0.64286\n","Epoch: 20/50, step: 15/83, loss: 3.13689, accuracy: 0.64167\n","Epoch: 20/50, step: 16/83, loss: 3.36285, accuracy: 0.63281\n","Epoch: 20/50, step: 17/83, loss: 3.35390, accuracy: 0.64706\n","Epoch: 20/50, step: 18/83, loss: 3.36545, accuracy: 0.63889\n","Epoch: 20/50, step: 19/83, loss: 3.51065, accuracy: 0.63158\n","Epoch: 20/50, step: 20/83, loss: 3.45856, accuracy: 0.63125\n","Epoch: 20/50, step: 21/83, loss: 3.41251, accuracy: 0.63690\n","Epoch: 20/50, step: 22/83, loss: 3.48989, accuracy: 0.63068\n","Epoch: 20/50, step: 23/83, loss: 3.44986, accuracy: 0.63587\n","Epoch: 20/50, step: 24/83, loss: 3.44524, accuracy: 0.64062\n","Epoch: 20/50, step: 25/83, loss: 3.44289, accuracy: 0.63500\n","Epoch: 20/50, step: 26/83, loss: 3.42981, accuracy: 0.63942\n","Epoch: 20/50, step: 27/83, loss: 3.38381, accuracy: 0.64352\n","Epoch: 20/50, step: 28/83, loss: 3.44734, accuracy: 0.63393\n","Epoch: 20/50, step: 29/83, loss: 3.47391, accuracy: 0.63793\n","Epoch: 20/50, step: 30/83, loss: 3.41607, accuracy: 0.64167\n","Epoch: 20/50, step: 31/83, loss: 3.44153, accuracy: 0.64113\n","Epoch: 20/50, step: 32/83, loss: 3.47169, accuracy: 0.64062\n","Epoch: 20/50, step: 33/83, loss: 3.47865, accuracy: 0.63636\n","Epoch: 20/50, step: 34/83, loss: 3.41724, accuracy: 0.64706\n","Epoch: 20/50, step: 35/83, loss: 3.46496, accuracy: 0.64643\n","Epoch: 20/50, step: 36/83, loss: 3.43270, accuracy: 0.64931\n","Epoch: 20/50, step: 37/83, loss: 3.45784, accuracy: 0.64527\n","Epoch: 20/50, step: 38/83, loss: 3.50689, accuracy: 0.64145\n","Epoch: 20/50, step: 39/83, loss: 3.45631, accuracy: 0.64744\n","Epoch: 20/50, step: 40/83, loss: 3.45825, accuracy: 0.64688\n","Epoch: 20/50, step: 41/83, loss: 3.45402, accuracy: 0.64634\n","Epoch: 20/50, step: 42/83, loss: 3.43260, accuracy: 0.64583\n","Epoch: 20/50, step: 43/83, loss: 3.38910, accuracy: 0.64826\n","Epoch: 20/50, step: 44/83, loss: 3.47803, accuracy: 0.63636\n","Epoch: 20/50, step: 45/83, loss: 3.48265, accuracy: 0.63889\n","Epoch: 20/50, step: 46/83, loss: 3.48382, accuracy: 0.63859\n","Epoch: 20/50, step: 47/83, loss: 3.52085, accuracy: 0.63298\n","Epoch: 20/50, step: 48/83, loss: 3.51283, accuracy: 0.63542\n","Epoch: 20/50, step: 49/83, loss: 3.50946, accuracy: 0.63520\n","Epoch: 20/50, step: 50/83, loss: 3.56716, accuracy: 0.62500\n","Epoch: 20/50, step: 51/83, loss: 3.56568, accuracy: 0.62500\n","Epoch: 20/50, step: 52/83, loss: 3.54703, accuracy: 0.62500\n","Epoch: 20/50, step: 53/83, loss: 3.52831, accuracy: 0.62264\n","Epoch: 20/50, step: 54/83, loss: 3.54049, accuracy: 0.62731\n","Epoch: 20/50, step: 55/83, loss: 3.51262, accuracy: 0.62955\n","Epoch: 20/50, step: 56/83, loss: 3.50810, accuracy: 0.62723\n","Epoch: 20/50, step: 57/83, loss: 3.51279, accuracy: 0.62281\n","Epoch: 20/50, step: 58/83, loss: 3.47539, accuracy: 0.62716\n","Epoch: 20/50, step: 59/83, loss: 3.47453, accuracy: 0.62712\n","Epoch: 20/50, step: 60/83, loss: 3.45148, accuracy: 0.63125\n","Epoch: 20/50, step: 61/83, loss: 3.43458, accuracy: 0.63320\n","Epoch: 20/50, step: 62/83, loss: 3.41999, accuracy: 0.63306\n","Epoch: 20/50, step: 63/83, loss: 3.40346, accuracy: 0.63095\n","Epoch: 20/50, step: 64/83, loss: 3.38816, accuracy: 0.63281\n","Epoch: 20/50, step: 65/83, loss: 3.39027, accuracy: 0.63269\n","Epoch: 20/50, step: 66/83, loss: 3.36132, accuracy: 0.63636\n","Epoch: 20/50, step: 67/83, loss: 3.37744, accuracy: 0.63433\n","Epoch: 20/50, step: 68/83, loss: 3.40078, accuracy: 0.63603\n","Epoch: 20/50, step: 69/83, loss: 3.41397, accuracy: 0.63406\n","Epoch: 20/50, step: 70/83, loss: 3.44056, accuracy: 0.63214\n","Epoch: 20/50, step: 71/83, loss: 3.47831, accuracy: 0.63204\n","Epoch: 20/50, step: 72/83, loss: 3.43744, accuracy: 0.63542\n","Epoch: 20/50, step: 73/83, loss: 3.41312, accuracy: 0.63527\n","Epoch: 20/50, step: 74/83, loss: 3.41827, accuracy: 0.63176\n","Epoch: 20/50, step: 75/83, loss: 3.42153, accuracy: 0.62833\n","Epoch: 20/50, step: 76/83, loss: 3.44164, accuracy: 0.62829\n","Epoch: 20/50, step: 77/83, loss: 3.45281, accuracy: 0.62825\n","Epoch: 20/50, step: 78/83, loss: 3.45455, accuracy: 0.62821\n","Epoch: 20/50, step: 79/83, loss: 3.46892, accuracy: 0.62500\n","Epoch: 20/50, step: 80/83, loss: 3.48834, accuracy: 0.62500\n","Epoch: 20/50, step: 81/83, loss: 3.47655, accuracy: 0.62500\n","Epoch: 20/50, step: 82/83, loss: 3.46481, accuracy: 0.62652\n","Epoch: 20/50, step: 83/83, loss: 3.51581, accuracy: 0.62709\n","Epoch: 20/50, train loss: 3.51581, train accuracy: 0.62709, valid loss: 10.63562, valid accuracy: 0.20362\n","Epoch: 21/50, step: 1/83, loss: 1.58813, accuracy: 0.75000\n","Epoch: 21/50, step: 2/83, loss: 2.86312, accuracy: 0.62500\n","Epoch: 21/50, step: 3/83, loss: 2.70112, accuracy: 0.62500\n","Epoch: 21/50, step: 4/83, loss: 2.92869, accuracy: 0.62500\n","Epoch: 21/50, step: 5/83, loss: 2.63912, accuracy: 0.65000\n","Epoch: 21/50, step: 6/83, loss: 2.77747, accuracy: 0.64583\n","Epoch: 21/50, step: 7/83, loss: 2.48944, accuracy: 0.66071\n","Epoch: 21/50, step: 8/83, loss: 2.49456, accuracy: 0.65625\n","Epoch: 21/50, step: 9/83, loss: 2.46165, accuracy: 0.69444\n","Epoch: 21/50, step: 10/83, loss: 2.47239, accuracy: 0.71250\n","Epoch: 21/50, step: 11/83, loss: 2.38882, accuracy: 0.70455\n","Epoch: 21/50, step: 12/83, loss: 2.38940, accuracy: 0.71875\n","Epoch: 21/50, step: 13/83, loss: 2.40273, accuracy: 0.71154\n","Epoch: 21/50, step: 14/83, loss: 2.46894, accuracy: 0.70536\n","Epoch: 21/50, step: 15/83, loss: 2.46578, accuracy: 0.69167\n","Epoch: 21/50, step: 16/83, loss: 2.70283, accuracy: 0.69531\n","Epoch: 21/50, step: 17/83, loss: 2.74778, accuracy: 0.69118\n","Epoch: 21/50, step: 18/83, loss: 2.79209, accuracy: 0.68056\n","Epoch: 21/50, step: 19/83, loss: 2.83278, accuracy: 0.67763\n","Epoch: 21/50, step: 20/83, loss: 2.77335, accuracy: 0.68125\n","Epoch: 21/50, step: 21/83, loss: 2.75718, accuracy: 0.68452\n","Epoch: 21/50, step: 22/83, loss: 2.75215, accuracy: 0.67614\n","Epoch: 21/50, step: 23/83, loss: 2.74965, accuracy: 0.66848\n","Epoch: 21/50, step: 24/83, loss: 2.78483, accuracy: 0.66146\n","Epoch: 21/50, step: 25/83, loss: 2.72920, accuracy: 0.66500\n","Epoch: 21/50, step: 26/83, loss: 2.78070, accuracy: 0.65865\n","Epoch: 21/50, step: 27/83, loss: 2.76983, accuracy: 0.66204\n","Epoch: 21/50, step: 28/83, loss: 2.85256, accuracy: 0.66518\n","Epoch: 21/50, step: 29/83, loss: 2.97296, accuracy: 0.65086\n","Epoch: 21/50, step: 30/83, loss: 2.95656, accuracy: 0.65000\n","Epoch: 21/50, step: 31/83, loss: 2.88238, accuracy: 0.65726\n","Epoch: 21/50, step: 32/83, loss: 2.97207, accuracy: 0.64844\n","Epoch: 21/50, step: 33/83, loss: 2.93625, accuracy: 0.64773\n","Epoch: 21/50, step: 34/83, loss: 2.99901, accuracy: 0.64706\n","Epoch: 21/50, step: 35/83, loss: 3.01255, accuracy: 0.64286\n","Epoch: 21/50, step: 36/83, loss: 2.99822, accuracy: 0.64583\n","Epoch: 21/50, step: 37/83, loss: 2.95951, accuracy: 0.65203\n","Epoch: 21/50, step: 38/83, loss: 3.07197, accuracy: 0.65132\n","Epoch: 21/50, step: 39/83, loss: 3.06976, accuracy: 0.64744\n","Epoch: 21/50, step: 40/83, loss: 3.06326, accuracy: 0.64375\n","Epoch: 21/50, step: 41/83, loss: 3.04674, accuracy: 0.65244\n","Epoch: 21/50, step: 42/83, loss: 3.09000, accuracy: 0.65179\n","Epoch: 21/50, step: 43/83, loss: 3.16110, accuracy: 0.65116\n","Epoch: 21/50, step: 44/83, loss: 3.18502, accuracy: 0.65057\n","Epoch: 21/50, step: 45/83, loss: 3.20641, accuracy: 0.63889\n","Epoch: 21/50, step: 46/83, loss: 3.19227, accuracy: 0.64130\n","Epoch: 21/50, step: 47/83, loss: 3.17482, accuracy: 0.64628\n","Epoch: 21/50, step: 48/83, loss: 3.20153, accuracy: 0.64062\n","Epoch: 21/50, step: 49/83, loss: 3.25993, accuracy: 0.64541\n","Epoch: 21/50, step: 50/83, loss: 3.30592, accuracy: 0.64000\n","Epoch: 21/50, step: 51/83, loss: 3.29376, accuracy: 0.63971\n","Epoch: 21/50, step: 52/83, loss: 3.25901, accuracy: 0.64423\n","Epoch: 21/50, step: 53/83, loss: 3.29748, accuracy: 0.64151\n","Epoch: 21/50, step: 54/83, loss: 3.31444, accuracy: 0.64120\n","Epoch: 21/50, step: 55/83, loss: 3.31270, accuracy: 0.64318\n","Epoch: 21/50, step: 56/83, loss: 3.39159, accuracy: 0.64062\n","Epoch: 21/50, step: 57/83, loss: 3.37860, accuracy: 0.64035\n","Epoch: 21/50, step: 58/83, loss: 3.34614, accuracy: 0.64655\n","Epoch: 21/50, step: 59/83, loss: 3.36746, accuracy: 0.64195\n","Epoch: 21/50, step: 60/83, loss: 3.41375, accuracy: 0.63750\n","Epoch: 21/50, step: 61/83, loss: 3.38656, accuracy: 0.63525\n","Epoch: 21/50, step: 62/83, loss: 3.36534, accuracy: 0.63911\n","Epoch: 21/50, step: 63/83, loss: 3.37891, accuracy: 0.63889\n","Epoch: 21/50, step: 64/83, loss: 3.41157, accuracy: 0.63672\n","Epoch: 21/50, step: 65/83, loss: 3.38089, accuracy: 0.64038\n","Epoch: 21/50, step: 66/83, loss: 3.37517, accuracy: 0.64015\n","Epoch: 21/50, step: 67/83, loss: 3.41766, accuracy: 0.63433\n","Epoch: 21/50, step: 68/83, loss: 3.39440, accuracy: 0.63603\n","Epoch: 21/50, step: 69/83, loss: 3.39331, accuracy: 0.63587\n","Epoch: 21/50, step: 70/83, loss: 3.40041, accuracy: 0.63036\n","Epoch: 21/50, step: 71/83, loss: 3.40480, accuracy: 0.62676\n","Epoch: 21/50, step: 72/83, loss: 3.45712, accuracy: 0.62500\n","Epoch: 21/50, step: 73/83, loss: 3.48457, accuracy: 0.62158\n","Epoch: 21/50, step: 74/83, loss: 3.51836, accuracy: 0.61824\n","Epoch: 21/50, step: 75/83, loss: 3.49539, accuracy: 0.62167\n","Epoch: 21/50, step: 76/83, loss: 3.51650, accuracy: 0.62500\n","Epoch: 21/50, step: 77/83, loss: 3.55209, accuracy: 0.62175\n","Epoch: 21/50, step: 78/83, loss: 3.56491, accuracy: 0.62019\n","Epoch: 21/50, step: 79/83, loss: 3.54982, accuracy: 0.62184\n","Epoch: 21/50, step: 80/83, loss: 3.54029, accuracy: 0.62031\n","Epoch: 21/50, step: 81/83, loss: 3.50472, accuracy: 0.62346\n","Epoch: 21/50, step: 82/83, loss: 3.47194, accuracy: 0.62500\n","Epoch: 21/50, step: 83/83, loss: 3.53474, accuracy: 0.62405\n","Epoch: 21/50, train loss: 3.53474, train accuracy: 0.62405, valid loss: 9.92260, valid accuracy: 0.18552\n","Epoch: 22/50, step: 1/83, loss: 3.74148, accuracy: 0.25000\n","Epoch: 22/50, step: 2/83, loss: 4.19092, accuracy: 0.31250\n","Epoch: 22/50, step: 3/83, loss: 3.33538, accuracy: 0.41667\n","Epoch: 22/50, step: 4/83, loss: 3.14788, accuracy: 0.46875\n","Epoch: 22/50, step: 5/83, loss: 3.19463, accuracy: 0.47500\n","Epoch: 22/50, step: 6/83, loss: 3.08834, accuracy: 0.47917\n","Epoch: 22/50, step: 7/83, loss: 3.14498, accuracy: 0.51786\n","Epoch: 22/50, step: 8/83, loss: 3.29298, accuracy: 0.53125\n","Epoch: 22/50, step: 9/83, loss: 3.50218, accuracy: 0.52778\n","Epoch: 22/50, step: 10/83, loss: 3.20598, accuracy: 0.57500\n","Epoch: 22/50, step: 11/83, loss: 3.30528, accuracy: 0.56818\n","Epoch: 22/50, step: 12/83, loss: 3.30102, accuracy: 0.58333\n","Epoch: 22/50, step: 13/83, loss: 3.32807, accuracy: 0.57692\n","Epoch: 22/50, step: 14/83, loss: 3.26648, accuracy: 0.58036\n","Epoch: 22/50, step: 15/83, loss: 3.21076, accuracy: 0.59167\n","Epoch: 22/50, step: 16/83, loss: 3.28312, accuracy: 0.58594\n","Epoch: 22/50, step: 17/83, loss: 3.33897, accuracy: 0.58824\n","Epoch: 22/50, step: 18/83, loss: 3.53329, accuracy: 0.59028\n","Epoch: 22/50, step: 19/83, loss: 3.66428, accuracy: 0.57895\n","Epoch: 22/50, step: 20/83, loss: 3.50763, accuracy: 0.60000\n","Epoch: 22/50, step: 21/83, loss: 3.45682, accuracy: 0.61310\n","Epoch: 22/50, step: 22/83, loss: 3.46281, accuracy: 0.60795\n","Epoch: 22/50, step: 23/83, loss: 3.54652, accuracy: 0.60870\n","Epoch: 22/50, step: 24/83, loss: 3.53888, accuracy: 0.61458\n","Epoch: 22/50, step: 25/83, loss: 3.49860, accuracy: 0.62000\n","Epoch: 22/50, step: 26/83, loss: 3.41420, accuracy: 0.63462\n","Epoch: 22/50, step: 27/83, loss: 3.57405, accuracy: 0.62963\n","Epoch: 22/50, step: 28/83, loss: 3.53256, accuracy: 0.63393\n","Epoch: 22/50, step: 29/83, loss: 3.49527, accuracy: 0.63793\n","Epoch: 22/50, step: 30/83, loss: 3.52549, accuracy: 0.64167\n","Epoch: 22/50, step: 31/83, loss: 3.55293, accuracy: 0.63306\n","Epoch: 22/50, step: 32/83, loss: 3.55046, accuracy: 0.63281\n","Epoch: 22/50, step: 33/83, loss: 3.57117, accuracy: 0.63258\n","Epoch: 22/50, step: 34/83, loss: 3.56665, accuracy: 0.63235\n","Epoch: 22/50, step: 35/83, loss: 3.53336, accuracy: 0.63929\n","Epoch: 22/50, step: 36/83, loss: 3.49950, accuracy: 0.64236\n","Epoch: 22/50, step: 37/83, loss: 3.51318, accuracy: 0.64189\n","Epoch: 22/50, step: 38/83, loss: 3.45931, accuracy: 0.64474\n","Epoch: 22/50, step: 39/83, loss: 3.45831, accuracy: 0.64423\n","Epoch: 22/50, step: 40/83, loss: 3.48025, accuracy: 0.64062\n","Epoch: 22/50, step: 41/83, loss: 3.45513, accuracy: 0.64024\n","Epoch: 22/50, step: 42/83, loss: 3.49691, accuracy: 0.63690\n","Epoch: 22/50, step: 43/83, loss: 3.49222, accuracy: 0.63372\n","Epoch: 22/50, step: 44/83, loss: 3.44629, accuracy: 0.63636\n","Epoch: 22/50, step: 45/83, loss: 3.43040, accuracy: 0.63889\n","Epoch: 22/50, step: 46/83, loss: 3.46560, accuracy: 0.63587\n","Epoch: 22/50, step: 47/83, loss: 3.46349, accuracy: 0.63032\n","Epoch: 22/50, step: 48/83, loss: 3.46047, accuracy: 0.63281\n","Epoch: 22/50, step: 49/83, loss: 3.45716, accuracy: 0.63776\n","Epoch: 22/50, step: 50/83, loss: 3.45832, accuracy: 0.63750\n","Epoch: 22/50, step: 51/83, loss: 3.45590, accuracy: 0.63971\n","Epoch: 22/50, step: 52/83, loss: 3.41879, accuracy: 0.63942\n","Epoch: 22/50, step: 53/83, loss: 3.36815, accuracy: 0.64151\n","Epoch: 22/50, step: 54/83, loss: 3.33480, accuracy: 0.64120\n","Epoch: 22/50, step: 55/83, loss: 3.38556, accuracy: 0.63864\n","Epoch: 22/50, step: 56/83, loss: 3.36771, accuracy: 0.64286\n","Epoch: 22/50, step: 57/83, loss: 3.36684, accuracy: 0.64035\n","Epoch: 22/50, step: 58/83, loss: 3.36807, accuracy: 0.63793\n","Epoch: 22/50, step: 59/83, loss: 3.33629, accuracy: 0.64195\n","Epoch: 22/50, step: 60/83, loss: 3.36518, accuracy: 0.63958\n","Epoch: 22/50, step: 61/83, loss: 3.36907, accuracy: 0.63934\n","Epoch: 22/50, step: 62/83, loss: 3.35377, accuracy: 0.63911\n","Epoch: 22/50, step: 63/83, loss: 3.37146, accuracy: 0.63889\n","Epoch: 22/50, step: 64/83, loss: 3.43272, accuracy: 0.63477\n","Epoch: 22/50, step: 65/83, loss: 3.41974, accuracy: 0.63462\n","Epoch: 22/50, step: 66/83, loss: 3.44742, accuracy: 0.63258\n","Epoch: 22/50, step: 67/83, loss: 3.42361, accuracy: 0.63619\n","Epoch: 22/50, step: 68/83, loss: 3.42472, accuracy: 0.63603\n","Epoch: 22/50, step: 69/83, loss: 3.39720, accuracy: 0.63768\n","Epoch: 22/50, step: 70/83, loss: 3.40096, accuracy: 0.63393\n","Epoch: 22/50, step: 71/83, loss: 3.43737, accuracy: 0.63028\n","Epoch: 22/50, step: 72/83, loss: 3.42431, accuracy: 0.63194\n","Epoch: 22/50, step: 73/83, loss: 3.42862, accuracy: 0.62842\n","Epoch: 22/50, step: 74/83, loss: 3.47463, accuracy: 0.62838\n","Epoch: 22/50, step: 75/83, loss: 3.46057, accuracy: 0.62833\n","Epoch: 22/50, step: 76/83, loss: 3.46168, accuracy: 0.62664\n","Epoch: 22/50, step: 77/83, loss: 3.45779, accuracy: 0.62662\n","Epoch: 22/50, step: 78/83, loss: 3.45718, accuracy: 0.62821\n","Epoch: 22/50, step: 79/83, loss: 3.44522, accuracy: 0.63133\n","Epoch: 22/50, step: 80/83, loss: 3.43407, accuracy: 0.63125\n","Epoch: 22/50, step: 81/83, loss: 3.42863, accuracy: 0.62809\n","Epoch: 22/50, step: 82/83, loss: 3.42723, accuracy: 0.62805\n","Epoch: 22/50, step: 83/83, loss: 3.39957, accuracy: 0.62709\n","Epoch: 22/50, train loss: 3.39957, train accuracy: 0.62709, valid loss: 10.03295, valid accuracy: 0.19005\n","Epoch: 23/50, step: 1/83, loss: 3.59469, accuracy: 0.62500\n","Epoch: 23/50, step: 2/83, loss: 3.46580, accuracy: 0.68750\n","Epoch: 23/50, step: 3/83, loss: 3.98401, accuracy: 0.66667\n","Epoch: 23/50, step: 4/83, loss: 3.36474, accuracy: 0.68750\n","Epoch: 23/50, step: 5/83, loss: 3.21148, accuracy: 0.67500\n","Epoch: 23/50, step: 6/83, loss: 3.37534, accuracy: 0.66667\n","Epoch: 23/50, step: 7/83, loss: 3.61360, accuracy: 0.62500\n","Epoch: 23/50, step: 8/83, loss: 4.03458, accuracy: 0.59375\n","Epoch: 23/50, step: 9/83, loss: 3.87135, accuracy: 0.61111\n","Epoch: 23/50, step: 10/83, loss: 3.76233, accuracy: 0.58750\n","Epoch: 23/50, step: 11/83, loss: 3.74967, accuracy: 0.57955\n","Epoch: 23/50, step: 12/83, loss: 3.64730, accuracy: 0.58333\n","Epoch: 23/50, step: 13/83, loss: 3.70370, accuracy: 0.56731\n","Epoch: 23/50, step: 14/83, loss: 3.68546, accuracy: 0.57143\n","Epoch: 23/50, step: 15/83, loss: 3.61194, accuracy: 0.59167\n","Epoch: 23/50, step: 16/83, loss: 3.50445, accuracy: 0.57812\n","Epoch: 23/50, step: 17/83, loss: 3.59168, accuracy: 0.58088\n","Epoch: 23/50, step: 18/83, loss: 3.53714, accuracy: 0.59028\n","Epoch: 23/50, step: 19/83, loss: 3.64707, accuracy: 0.57237\n","Epoch: 23/50, step: 20/83, loss: 3.63117, accuracy: 0.57500\n","Epoch: 23/50, step: 21/83, loss: 3.52716, accuracy: 0.58929\n","Epoch: 23/50, step: 22/83, loss: 3.48191, accuracy: 0.59091\n","Epoch: 23/50, step: 23/83, loss: 3.46221, accuracy: 0.59239\n","Epoch: 23/50, step: 24/83, loss: 3.47424, accuracy: 0.58854\n","Epoch: 23/50, step: 25/83, loss: 3.54211, accuracy: 0.59000\n","Epoch: 23/50, step: 26/83, loss: 3.50203, accuracy: 0.58654\n","Epoch: 23/50, step: 27/83, loss: 3.46580, accuracy: 0.58333\n","Epoch: 23/50, step: 28/83, loss: 3.46340, accuracy: 0.58482\n","Epoch: 23/50, step: 29/83, loss: 3.51581, accuracy: 0.58190\n","Epoch: 23/50, step: 30/83, loss: 3.58795, accuracy: 0.57917\n","Epoch: 23/50, step: 31/83, loss: 3.52278, accuracy: 0.58871\n","Epoch: 23/50, step: 32/83, loss: 3.53972, accuracy: 0.58594\n","Epoch: 23/50, step: 33/83, loss: 3.52544, accuracy: 0.58333\n","Epoch: 23/50, step: 34/83, loss: 3.54913, accuracy: 0.58456\n","Epoch: 23/50, step: 35/83, loss: 3.49183, accuracy: 0.58929\n","Epoch: 23/50, step: 36/83, loss: 3.49022, accuracy: 0.58681\n","Epoch: 23/50, step: 37/83, loss: 3.49137, accuracy: 0.58784\n","Epoch: 23/50, step: 38/83, loss: 3.48032, accuracy: 0.58882\n","Epoch: 23/50, step: 39/83, loss: 3.46113, accuracy: 0.58013\n","Epoch: 23/50, step: 40/83, loss: 3.46203, accuracy: 0.57812\n","Epoch: 23/50, step: 41/83, loss: 3.44547, accuracy: 0.57317\n","Epoch: 23/50, step: 42/83, loss: 3.44222, accuracy: 0.57738\n","Epoch: 23/50, step: 43/83, loss: 3.41786, accuracy: 0.58430\n","Epoch: 23/50, step: 44/83, loss: 3.39725, accuracy: 0.58239\n","Epoch: 23/50, step: 45/83, loss: 3.37317, accuracy: 0.58611\n","Epoch: 23/50, step: 46/83, loss: 3.35537, accuracy: 0.58696\n","Epoch: 23/50, step: 47/83, loss: 3.35838, accuracy: 0.58245\n","Epoch: 23/50, step: 48/83, loss: 3.33887, accuracy: 0.58333\n","Epoch: 23/50, step: 49/83, loss: 3.34029, accuracy: 0.58673\n","Epoch: 23/50, step: 50/83, loss: 3.33955, accuracy: 0.58500\n","Epoch: 23/50, step: 51/83, loss: 3.34121, accuracy: 0.58578\n","Epoch: 23/50, step: 52/83, loss: 3.34148, accuracy: 0.58894\n","Epoch: 23/50, step: 53/83, loss: 3.31210, accuracy: 0.59198\n","Epoch: 23/50, step: 54/83, loss: 3.29796, accuracy: 0.59491\n","Epoch: 23/50, step: 55/83, loss: 3.33032, accuracy: 0.59318\n","Epoch: 23/50, step: 56/83, loss: 3.29610, accuracy: 0.59821\n","Epoch: 23/50, step: 57/83, loss: 3.31025, accuracy: 0.59868\n","Epoch: 23/50, step: 58/83, loss: 3.27868, accuracy: 0.60345\n","Epoch: 23/50, step: 59/83, loss: 3.28299, accuracy: 0.60381\n","Epoch: 23/50, step: 60/83, loss: 3.29168, accuracy: 0.60417\n","Epoch: 23/50, step: 61/83, loss: 3.33577, accuracy: 0.60451\n","Epoch: 23/50, step: 62/83, loss: 3.35204, accuracy: 0.60484\n","Epoch: 23/50, step: 63/83, loss: 3.35842, accuracy: 0.60119\n","Epoch: 23/50, step: 64/83, loss: 3.35960, accuracy: 0.60156\n","Epoch: 23/50, step: 65/83, loss: 3.34412, accuracy: 0.60192\n","Epoch: 23/50, step: 66/83, loss: 3.36064, accuracy: 0.60038\n","Epoch: 23/50, step: 67/83, loss: 3.38152, accuracy: 0.59515\n","Epoch: 23/50, step: 68/83, loss: 3.39394, accuracy: 0.59375\n","Epoch: 23/50, step: 69/83, loss: 3.39642, accuracy: 0.59420\n","Epoch: 23/50, step: 70/83, loss: 3.37924, accuracy: 0.59643\n","Epoch: 23/50, step: 71/83, loss: 3.35280, accuracy: 0.59859\n","Epoch: 23/50, step: 72/83, loss: 3.36659, accuracy: 0.59722\n","Epoch: 23/50, step: 73/83, loss: 3.38021, accuracy: 0.59589\n","Epoch: 23/50, step: 74/83, loss: 3.39505, accuracy: 0.59628\n","Epoch: 23/50, step: 75/83, loss: 3.40353, accuracy: 0.59667\n","Epoch: 23/50, step: 76/83, loss: 3.38869, accuracy: 0.60197\n","Epoch: 23/50, step: 77/83, loss: 3.36651, accuracy: 0.60390\n","Epoch: 23/50, step: 78/83, loss: 3.35706, accuracy: 0.60256\n","Epoch: 23/50, step: 79/83, loss: 3.33772, accuracy: 0.60601\n","Epoch: 23/50, step: 80/83, loss: 3.32623, accuracy: 0.60938\n","Epoch: 23/50, step: 81/83, loss: 3.34944, accuracy: 0.60802\n","Epoch: 23/50, step: 82/83, loss: 3.33781, accuracy: 0.60976\n","Epoch: 23/50, step: 83/83, loss: 3.38980, accuracy: 0.61035\n","Epoch: 23/50, train loss: 3.38980, train accuracy: 0.61035, valid loss: 10.71155, valid accuracy: 0.17195\n","Epoch: 24/50, step: 1/83, loss: 3.50985, accuracy: 0.50000\n","Epoch: 24/50, step: 2/83, loss: 3.33715, accuracy: 0.56250\n","Epoch: 24/50, step: 3/83, loss: 3.05541, accuracy: 0.58333\n","Epoch: 24/50, step: 4/83, loss: 2.69750, accuracy: 0.65625\n","Epoch: 24/50, step: 5/83, loss: 2.46900, accuracy: 0.70000\n","Epoch: 24/50, step: 6/83, loss: 2.61680, accuracy: 0.66667\n","Epoch: 24/50, step: 7/83, loss: 2.66602, accuracy: 0.69643\n","Epoch: 24/50, step: 8/83, loss: 2.65472, accuracy: 0.71875\n","Epoch: 24/50, step: 9/83, loss: 2.54286, accuracy: 0.72222\n","Epoch: 24/50, step: 10/83, loss: 2.72587, accuracy: 0.70000\n","Epoch: 24/50, step: 11/83, loss: 2.68232, accuracy: 0.71591\n","Epoch: 24/50, step: 12/83, loss: 2.97041, accuracy: 0.68750\n","Epoch: 24/50, step: 13/83, loss: 2.94004, accuracy: 0.68269\n","Epoch: 24/50, step: 14/83, loss: 2.83506, accuracy: 0.69643\n","Epoch: 24/50, step: 15/83, loss: 2.88424, accuracy: 0.68333\n","Epoch: 24/50, step: 16/83, loss: 2.90199, accuracy: 0.67969\n","Epoch: 24/50, step: 17/83, loss: 2.82935, accuracy: 0.67647\n","Epoch: 24/50, step: 18/83, loss: 2.92314, accuracy: 0.66667\n","Epoch: 24/50, step: 19/83, loss: 3.04862, accuracy: 0.65132\n","Epoch: 24/50, step: 20/83, loss: 2.97941, accuracy: 0.65625\n","Epoch: 24/50, step: 21/83, loss: 2.99593, accuracy: 0.66071\n","Epoch: 24/50, step: 22/83, loss: 3.00589, accuracy: 0.66477\n","Epoch: 24/50, step: 23/83, loss: 2.90301, accuracy: 0.66848\n","Epoch: 24/50, step: 24/83, loss: 2.88593, accuracy: 0.67188\n","Epoch: 24/50, step: 25/83, loss: 2.98651, accuracy: 0.66000\n","Epoch: 24/50, step: 26/83, loss: 2.99919, accuracy: 0.66346\n","Epoch: 24/50, step: 27/83, loss: 3.10773, accuracy: 0.64815\n","Epoch: 24/50, step: 28/83, loss: 3.08456, accuracy: 0.64732\n","Epoch: 24/50, step: 29/83, loss: 3.15561, accuracy: 0.64224\n","Epoch: 24/50, step: 30/83, loss: 3.21354, accuracy: 0.64167\n","Epoch: 24/50, step: 31/83, loss: 3.16182, accuracy: 0.64516\n","Epoch: 24/50, step: 32/83, loss: 3.17769, accuracy: 0.63672\n","Epoch: 24/50, step: 33/83, loss: 3.15728, accuracy: 0.64394\n","Epoch: 24/50, step: 34/83, loss: 3.10164, accuracy: 0.64706\n","Epoch: 24/50, step: 35/83, loss: 3.12029, accuracy: 0.63929\n","Epoch: 24/50, step: 36/83, loss: 3.11450, accuracy: 0.64236\n","Epoch: 24/50, step: 37/83, loss: 3.09507, accuracy: 0.63851\n","Epoch: 24/50, step: 38/83, loss: 3.08463, accuracy: 0.63487\n","Epoch: 24/50, step: 39/83, loss: 3.07700, accuracy: 0.63782\n","Epoch: 24/50, step: 40/83, loss: 3.03936, accuracy: 0.64375\n","Epoch: 24/50, step: 41/83, loss: 3.04290, accuracy: 0.64329\n","Epoch: 24/50, step: 42/83, loss: 3.05452, accuracy: 0.64286\n","Epoch: 24/50, step: 43/83, loss: 3.08800, accuracy: 0.63953\n","Epoch: 24/50, step: 44/83, loss: 3.09454, accuracy: 0.63636\n","Epoch: 24/50, step: 45/83, loss: 3.03741, accuracy: 0.64444\n","Epoch: 24/50, step: 46/83, loss: 3.02607, accuracy: 0.64674\n","Epoch: 24/50, step: 47/83, loss: 3.05915, accuracy: 0.63830\n","Epoch: 24/50, step: 48/83, loss: 3.11584, accuracy: 0.63281\n","Epoch: 24/50, step: 49/83, loss: 3.14041, accuracy: 0.63010\n","Epoch: 24/50, step: 50/83, loss: 3.16802, accuracy: 0.63000\n","Epoch: 24/50, step: 51/83, loss: 3.13974, accuracy: 0.63235\n","Epoch: 24/50, step: 52/83, loss: 3.13338, accuracy: 0.63221\n","Epoch: 24/50, step: 53/83, loss: 3.10417, accuracy: 0.63443\n","Epoch: 24/50, step: 54/83, loss: 3.10728, accuracy: 0.63657\n","Epoch: 24/50, step: 55/83, loss: 3.12492, accuracy: 0.63864\n","Epoch: 24/50, step: 56/83, loss: 3.12438, accuracy: 0.63839\n","Epoch: 24/50, step: 57/83, loss: 3.14054, accuracy: 0.63596\n","Epoch: 24/50, step: 58/83, loss: 3.18021, accuracy: 0.63147\n","Epoch: 24/50, step: 59/83, loss: 3.17062, accuracy: 0.62924\n","Epoch: 24/50, step: 60/83, loss: 3.19430, accuracy: 0.62708\n","Epoch: 24/50, step: 61/83, loss: 3.16829, accuracy: 0.62910\n","Epoch: 24/50, step: 62/83, loss: 3.16394, accuracy: 0.62500\n","Epoch: 24/50, step: 63/83, loss: 3.16988, accuracy: 0.62500\n","Epoch: 24/50, step: 64/83, loss: 3.16258, accuracy: 0.62500\n","Epoch: 24/50, step: 65/83, loss: 3.18654, accuracy: 0.62500\n","Epoch: 24/50, step: 66/83, loss: 3.16228, accuracy: 0.62689\n","Epoch: 24/50, step: 67/83, loss: 3.12588, accuracy: 0.63060\n","Epoch: 24/50, step: 68/83, loss: 3.12718, accuracy: 0.63051\n","Epoch: 24/50, step: 69/83, loss: 3.10262, accuracy: 0.63406\n","Epoch: 24/50, step: 70/83, loss: 3.09495, accuracy: 0.63393\n","Epoch: 24/50, step: 71/83, loss: 3.07314, accuracy: 0.63556\n","Epoch: 24/50, step: 72/83, loss: 3.09069, accuracy: 0.63542\n","Epoch: 24/50, step: 73/83, loss: 3.08180, accuracy: 0.63699\n","Epoch: 24/50, step: 74/83, loss: 3.06139, accuracy: 0.63851\n","Epoch: 24/50, step: 75/83, loss: 3.06985, accuracy: 0.63500\n","Epoch: 24/50, step: 76/83, loss: 3.07375, accuracy: 0.63322\n","Epoch: 24/50, step: 77/83, loss: 3.06909, accuracy: 0.63312\n","Epoch: 24/50, step: 78/83, loss: 3.08391, accuracy: 0.63141\n","Epoch: 24/50, step: 79/83, loss: 3.07938, accuracy: 0.63133\n","Epoch: 24/50, step: 80/83, loss: 3.10154, accuracy: 0.63125\n","Epoch: 24/50, step: 81/83, loss: 3.10660, accuracy: 0.63272\n","Epoch: 24/50, step: 82/83, loss: 3.12357, accuracy: 0.63262\n","Epoch: 24/50, step: 83/83, loss: 3.17756, accuracy: 0.63318\n","Epoch: 24/50, train loss: 3.17756, train accuracy: 0.63318, valid loss: 10.42199, valid accuracy: 0.18100\n","Epoch: 25/50, step: 1/83, loss: 5.09049, accuracy: 0.37500\n","Epoch: 25/50, step: 2/83, loss: 3.87069, accuracy: 0.50000\n","Epoch: 25/50, step: 3/83, loss: 3.08705, accuracy: 0.62500\n","Epoch: 25/50, step: 4/83, loss: 3.19957, accuracy: 0.59375\n","Epoch: 25/50, step: 5/83, loss: 2.87330, accuracy: 0.65000\n","Epoch: 25/50, step: 6/83, loss: 3.12895, accuracy: 0.62500\n","Epoch: 25/50, step: 7/83, loss: 2.90745, accuracy: 0.64286\n","Epoch: 25/50, step: 8/83, loss: 2.84660, accuracy: 0.65625\n","Epoch: 25/50, step: 9/83, loss: 2.84462, accuracy: 0.63889\n","Epoch: 25/50, step: 10/83, loss: 2.91484, accuracy: 0.63750\n","Epoch: 25/50, step: 11/83, loss: 2.86064, accuracy: 0.65909\n","Epoch: 25/50, step: 12/83, loss: 2.75632, accuracy: 0.66667\n","Epoch: 25/50, step: 13/83, loss: 2.80070, accuracy: 0.68269\n","Epoch: 25/50, step: 14/83, loss: 2.83731, accuracy: 0.68750\n","Epoch: 25/50, step: 15/83, loss: 2.82015, accuracy: 0.67500\n","Epoch: 25/50, step: 16/83, loss: 2.92226, accuracy: 0.65625\n","Epoch: 25/50, step: 17/83, loss: 2.85204, accuracy: 0.66176\n","Epoch: 25/50, step: 18/83, loss: 2.86072, accuracy: 0.66667\n","Epoch: 25/50, step: 19/83, loss: 2.87753, accuracy: 0.66447\n","Epoch: 25/50, step: 20/83, loss: 2.86377, accuracy: 0.66250\n","Epoch: 25/50, step: 21/83, loss: 2.89015, accuracy: 0.65476\n","Epoch: 25/50, step: 22/83, loss: 2.96853, accuracy: 0.64205\n","Epoch: 25/50, step: 23/83, loss: 2.96002, accuracy: 0.63587\n","Epoch: 25/50, step: 24/83, loss: 2.98388, accuracy: 0.63021\n","Epoch: 25/50, step: 25/83, loss: 2.92464, accuracy: 0.64000\n","Epoch: 25/50, step: 26/83, loss: 2.94018, accuracy: 0.63942\n","Epoch: 25/50, step: 27/83, loss: 2.89998, accuracy: 0.63426\n","Epoch: 25/50, step: 28/83, loss: 2.98089, accuracy: 0.63839\n","Epoch: 25/50, step: 29/83, loss: 2.99394, accuracy: 0.63793\n","Epoch: 25/50, step: 30/83, loss: 3.00817, accuracy: 0.63750\n","Epoch: 25/50, step: 31/83, loss: 3.05296, accuracy: 0.62903\n","Epoch: 25/50, step: 32/83, loss: 3.10387, accuracy: 0.61328\n","Epoch: 25/50, step: 33/83, loss: 3.07724, accuracy: 0.62121\n","Epoch: 25/50, step: 34/83, loss: 3.02926, accuracy: 0.62868\n","Epoch: 25/50, step: 35/83, loss: 2.98863, accuracy: 0.63214\n","Epoch: 25/50, step: 36/83, loss: 2.96960, accuracy: 0.63542\n","Epoch: 25/50, step: 37/83, loss: 3.00972, accuracy: 0.63176\n","Epoch: 25/50, step: 38/83, loss: 2.97235, accuracy: 0.63816\n","Epoch: 25/50, step: 39/83, loss: 2.93764, accuracy: 0.63462\n","Epoch: 25/50, step: 40/83, loss: 2.92609, accuracy: 0.64062\n","Epoch: 25/50, step: 41/83, loss: 2.87855, accuracy: 0.63720\n","Epoch: 25/50, step: 42/83, loss: 2.95928, accuracy: 0.62798\n","Epoch: 25/50, step: 43/83, loss: 2.94805, accuracy: 0.62500\n","Epoch: 25/50, step: 44/83, loss: 2.97666, accuracy: 0.62784\n","Epoch: 25/50, step: 45/83, loss: 3.00286, accuracy: 0.62778\n","Epoch: 25/50, step: 46/83, loss: 2.99877, accuracy: 0.62500\n","Epoch: 25/50, step: 47/83, loss: 2.98660, accuracy: 0.62234\n","Epoch: 25/50, step: 48/83, loss: 2.97731, accuracy: 0.62500\n","Epoch: 25/50, step: 49/83, loss: 3.00402, accuracy: 0.62500\n","Epoch: 25/50, step: 50/83, loss: 2.99037, accuracy: 0.62250\n","Epoch: 25/50, step: 51/83, loss: 3.00146, accuracy: 0.61765\n","Epoch: 25/50, step: 52/83, loss: 2.96987, accuracy: 0.62260\n","Epoch: 25/50, step: 53/83, loss: 2.92860, accuracy: 0.62500\n","Epoch: 25/50, step: 54/83, loss: 2.96935, accuracy: 0.62500\n","Epoch: 25/50, step: 55/83, loss: 2.95602, accuracy: 0.62955\n","Epoch: 25/50, step: 56/83, loss: 2.91528, accuracy: 0.63393\n","Epoch: 25/50, step: 57/83, loss: 2.92100, accuracy: 0.63377\n","Epoch: 25/50, step: 58/83, loss: 2.92639, accuracy: 0.63578\n","Epoch: 25/50, step: 59/83, loss: 2.95263, accuracy: 0.63136\n","Epoch: 25/50, step: 60/83, loss: 2.95928, accuracy: 0.63125\n","Epoch: 25/50, step: 61/83, loss: 2.93855, accuracy: 0.63115\n","Epoch: 25/50, step: 62/83, loss: 2.96633, accuracy: 0.62702\n","Epoch: 25/50, step: 63/83, loss: 2.98746, accuracy: 0.62500\n","Epoch: 25/50, step: 64/83, loss: 2.99632, accuracy: 0.62500\n","Epoch: 25/50, step: 65/83, loss: 2.96105, accuracy: 0.62885\n","Epoch: 25/50, step: 66/83, loss: 2.96888, accuracy: 0.62689\n","Epoch: 25/50, step: 67/83, loss: 2.95789, accuracy: 0.62873\n","Epoch: 25/50, step: 68/83, loss: 2.94419, accuracy: 0.63235\n","Epoch: 25/50, step: 69/83, loss: 2.94917, accuracy: 0.63225\n","Epoch: 25/50, step: 70/83, loss: 2.91843, accuracy: 0.63214\n","Epoch: 25/50, step: 71/83, loss: 2.96308, accuracy: 0.62676\n","Epoch: 25/50, step: 72/83, loss: 2.95718, accuracy: 0.62674\n","Epoch: 25/50, step: 73/83, loss: 2.94949, accuracy: 0.62842\n","Epoch: 25/50, step: 74/83, loss: 2.94357, accuracy: 0.62669\n","Epoch: 25/50, step: 75/83, loss: 2.96322, accuracy: 0.62500\n","Epoch: 25/50, step: 76/83, loss: 2.95546, accuracy: 0.62829\n","Epoch: 25/50, step: 77/83, loss: 2.93756, accuracy: 0.62825\n","Epoch: 25/50, step: 78/83, loss: 2.96768, accuracy: 0.62660\n","Epoch: 25/50, step: 79/83, loss: 2.97312, accuracy: 0.62342\n","Epoch: 25/50, step: 80/83, loss: 3.01245, accuracy: 0.61875\n","Epoch: 25/50, step: 81/83, loss: 2.99550, accuracy: 0.62037\n","Epoch: 25/50, step: 82/83, loss: 3.03393, accuracy: 0.61738\n","Epoch: 25/50, step: 83/83, loss: 3.09899, accuracy: 0.61644\n","Epoch: 25/50, train loss: 3.09899, train accuracy: 0.61644, valid loss: 10.05379, valid accuracy: 0.17647\n","Epoch: 26/50, step: 1/83, loss: 2.74316, accuracy: 0.62500\n","Epoch: 26/50, step: 2/83, loss: 1.77217, accuracy: 0.68750\n","Epoch: 26/50, step: 3/83, loss: 2.31429, accuracy: 0.70833\n","Epoch: 26/50, step: 4/83, loss: 2.55857, accuracy: 0.65625\n","Epoch: 26/50, step: 5/83, loss: 3.07006, accuracy: 0.60000\n","Epoch: 26/50, step: 6/83, loss: 3.39520, accuracy: 0.52083\n","Epoch: 26/50, step: 7/83, loss: 3.42463, accuracy: 0.50000\n","Epoch: 26/50, step: 8/83, loss: 3.24579, accuracy: 0.54688\n","Epoch: 26/50, step: 9/83, loss: 3.19320, accuracy: 0.54167\n","Epoch: 26/50, step: 10/83, loss: 3.19978, accuracy: 0.56250\n","Epoch: 26/50, step: 11/83, loss: 3.44181, accuracy: 0.56818\n","Epoch: 26/50, step: 12/83, loss: 3.36708, accuracy: 0.57292\n","Epoch: 26/50, step: 13/83, loss: 3.50611, accuracy: 0.57692\n","Epoch: 26/50, step: 14/83, loss: 3.44404, accuracy: 0.58036\n","Epoch: 26/50, step: 15/83, loss: 3.38841, accuracy: 0.58333\n","Epoch: 26/50, step: 16/83, loss: 3.48889, accuracy: 0.57812\n","Epoch: 26/50, step: 17/83, loss: 3.41431, accuracy: 0.59559\n","Epoch: 26/50, step: 18/83, loss: 3.39291, accuracy: 0.59722\n","Epoch: 26/50, step: 19/83, loss: 3.34834, accuracy: 0.59868\n","Epoch: 26/50, step: 20/83, loss: 3.30845, accuracy: 0.60625\n","Epoch: 26/50, step: 21/83, loss: 3.22835, accuracy: 0.61905\n","Epoch: 26/50, step: 22/83, loss: 3.19716, accuracy: 0.62500\n","Epoch: 26/50, step: 23/83, loss: 3.08667, accuracy: 0.63587\n","Epoch: 26/50, step: 24/83, loss: 3.17515, accuracy: 0.62500\n","Epoch: 26/50, step: 25/83, loss: 3.15097, accuracy: 0.62500\n","Epoch: 26/50, step: 26/83, loss: 3.09864, accuracy: 0.62500\n","Epoch: 26/50, step: 27/83, loss: 3.07875, accuracy: 0.62500\n","Epoch: 26/50, step: 28/83, loss: 3.05381, accuracy: 0.62946\n","Epoch: 26/50, step: 29/83, loss: 2.97997, accuracy: 0.62500\n","Epoch: 26/50, step: 30/83, loss: 3.00135, accuracy: 0.62083\n","Epoch: 26/50, step: 31/83, loss: 3.06911, accuracy: 0.61694\n","Epoch: 26/50, step: 32/83, loss: 3.07903, accuracy: 0.61719\n","Epoch: 26/50, step: 33/83, loss: 3.11267, accuracy: 0.61742\n","Epoch: 26/50, step: 34/83, loss: 3.17084, accuracy: 0.62132\n","Epoch: 26/50, step: 35/83, loss: 3.15665, accuracy: 0.62143\n","Epoch: 26/50, step: 36/83, loss: 3.15846, accuracy: 0.61806\n","Epoch: 26/50, step: 37/83, loss: 3.14248, accuracy: 0.61149\n","Epoch: 26/50, step: 38/83, loss: 3.15126, accuracy: 0.61184\n","Epoch: 26/50, step: 39/83, loss: 3.18275, accuracy: 0.61218\n","Epoch: 26/50, step: 40/83, loss: 3.21153, accuracy: 0.60938\n","Epoch: 26/50, step: 41/83, loss: 3.23695, accuracy: 0.60671\n","Epoch: 26/50, step: 42/83, loss: 3.21494, accuracy: 0.61012\n","Epoch: 26/50, step: 43/83, loss: 3.23987, accuracy: 0.61337\n","Epoch: 26/50, step: 44/83, loss: 3.28731, accuracy: 0.61080\n","Epoch: 26/50, step: 45/83, loss: 3.35204, accuracy: 0.60833\n","Epoch: 26/50, step: 46/83, loss: 3.35577, accuracy: 0.60598\n","Epoch: 26/50, step: 47/83, loss: 3.31988, accuracy: 0.60106\n","Epoch: 26/50, step: 48/83, loss: 3.32152, accuracy: 0.60417\n","Epoch: 26/50, step: 49/83, loss: 3.30478, accuracy: 0.60459\n","Epoch: 26/50, step: 50/83, loss: 3.26841, accuracy: 0.61000\n","Epoch: 26/50, step: 51/83, loss: 3.27210, accuracy: 0.60784\n","Epoch: 26/50, step: 52/83, loss: 3.28622, accuracy: 0.60096\n","Epoch: 26/50, step: 53/83, loss: 3.25373, accuracy: 0.60142\n","Epoch: 26/50, step: 54/83, loss: 3.22396, accuracy: 0.60417\n","Epoch: 26/50, step: 55/83, loss: 3.24914, accuracy: 0.60227\n","Epoch: 26/50, step: 56/83, loss: 3.23734, accuracy: 0.60268\n","Epoch: 26/50, step: 57/83, loss: 3.25181, accuracy: 0.60526\n","Epoch: 26/50, step: 58/83, loss: 3.22637, accuracy: 0.60560\n","Epoch: 26/50, step: 59/83, loss: 3.18355, accuracy: 0.61017\n","Epoch: 26/50, step: 60/83, loss: 3.15522, accuracy: 0.61458\n","Epoch: 26/50, step: 61/83, loss: 3.15795, accuracy: 0.61885\n","Epoch: 26/50, step: 62/83, loss: 3.16182, accuracy: 0.62097\n","Epoch: 26/50, step: 63/83, loss: 3.17918, accuracy: 0.62103\n","Epoch: 26/50, step: 64/83, loss: 3.16515, accuracy: 0.62305\n","Epoch: 26/50, step: 65/83, loss: 3.15439, accuracy: 0.62500\n","Epoch: 26/50, step: 66/83, loss: 3.14378, accuracy: 0.62689\n","Epoch: 26/50, step: 67/83, loss: 3.10512, accuracy: 0.63246\n","Epoch: 26/50, step: 68/83, loss: 3.16492, accuracy: 0.63051\n","Epoch: 26/50, step: 69/83, loss: 3.20698, accuracy: 0.62862\n","Epoch: 26/50, step: 70/83, loss: 3.21495, accuracy: 0.62500\n","Epoch: 26/50, step: 71/83, loss: 3.21791, accuracy: 0.62500\n","Epoch: 26/50, step: 72/83, loss: 3.22167, accuracy: 0.62500\n","Epoch: 26/50, step: 73/83, loss: 3.21154, accuracy: 0.62500\n","Epoch: 26/50, step: 74/83, loss: 3.21386, accuracy: 0.62162\n","Epoch: 26/50, step: 75/83, loss: 3.23759, accuracy: 0.62000\n","Epoch: 26/50, step: 76/83, loss: 3.22715, accuracy: 0.62007\n","Epoch: 26/50, step: 77/83, loss: 3.20447, accuracy: 0.62175\n","Epoch: 26/50, step: 78/83, loss: 3.20673, accuracy: 0.62340\n","Epoch: 26/50, step: 79/83, loss: 3.21255, accuracy: 0.62342\n","Epoch: 26/50, step: 80/83, loss: 3.20125, accuracy: 0.62344\n","Epoch: 26/50, step: 81/83, loss: 3.19342, accuracy: 0.62500\n","Epoch: 26/50, step: 82/83, loss: 3.19494, accuracy: 0.62500\n","Epoch: 26/50, step: 83/83, loss: 3.25680, accuracy: 0.62405\n","Epoch: 26/50, train loss: 3.25680, train accuracy: 0.62405, valid loss: 10.46681, valid accuracy: 0.17195\n","Epoch: 27/50, step: 1/83, loss: 2.49854, accuracy: 0.75000\n","Epoch: 27/50, step: 2/83, loss: 2.94025, accuracy: 0.62500\n","Epoch: 27/50, step: 3/83, loss: 2.72609, accuracy: 0.70833\n","Epoch: 27/50, step: 4/83, loss: 2.65718, accuracy: 0.65625\n","Epoch: 27/50, step: 5/83, loss: 2.45531, accuracy: 0.65000\n","Epoch: 27/50, step: 6/83, loss: 2.31624, accuracy: 0.66667\n","Epoch: 27/50, step: 7/83, loss: 2.58354, accuracy: 0.62500\n","Epoch: 27/50, step: 8/83, loss: 2.82426, accuracy: 0.60938\n","Epoch: 27/50, step: 9/83, loss: 2.87568, accuracy: 0.62500\n","Epoch: 27/50, step: 10/83, loss: 2.82989, accuracy: 0.63750\n","Epoch: 27/50, step: 11/83, loss: 2.75116, accuracy: 0.64773\n","Epoch: 27/50, step: 12/83, loss: 2.81479, accuracy: 0.64583\n","Epoch: 27/50, step: 13/83, loss: 2.85270, accuracy: 0.64423\n","Epoch: 27/50, step: 14/83, loss: 2.74558, accuracy: 0.66964\n","Epoch: 27/50, step: 15/83, loss: 2.78369, accuracy: 0.67500\n","Epoch: 27/50, step: 16/83, loss: 2.76110, accuracy: 0.66406\n","Epoch: 27/50, step: 17/83, loss: 2.75216, accuracy: 0.66912\n","Epoch: 27/50, step: 18/83, loss: 2.78183, accuracy: 0.65972\n","Epoch: 27/50, step: 19/83, loss: 2.98736, accuracy: 0.65132\n","Epoch: 27/50, step: 20/83, loss: 3.05810, accuracy: 0.64375\n","Epoch: 27/50, step: 21/83, loss: 3.03583, accuracy: 0.63690\n","Epoch: 27/50, step: 22/83, loss: 3.09549, accuracy: 0.63068\n","Epoch: 27/50, step: 23/83, loss: 3.02213, accuracy: 0.64130\n","Epoch: 27/50, step: 24/83, loss: 3.05369, accuracy: 0.63542\n","Epoch: 27/50, step: 25/83, loss: 3.09526, accuracy: 0.63500\n","Epoch: 27/50, step: 26/83, loss: 3.11748, accuracy: 0.63462\n","Epoch: 27/50, step: 27/83, loss: 3.12890, accuracy: 0.62963\n","Epoch: 27/50, step: 28/83, loss: 3.26619, accuracy: 0.61607\n","Epoch: 27/50, step: 29/83, loss: 3.27467, accuracy: 0.61207\n","Epoch: 27/50, step: 30/83, loss: 3.24590, accuracy: 0.61667\n","Epoch: 27/50, step: 31/83, loss: 3.16830, accuracy: 0.61694\n","Epoch: 27/50, step: 32/83, loss: 3.14945, accuracy: 0.62109\n","Epoch: 27/50, step: 33/83, loss: 3.09973, accuracy: 0.62500\n","Epoch: 27/50, step: 34/83, loss: 3.02251, accuracy: 0.63603\n","Epoch: 27/50, step: 35/83, loss: 3.05763, accuracy: 0.63571\n","Epoch: 27/50, step: 36/83, loss: 3.04256, accuracy: 0.63542\n","Epoch: 27/50, step: 37/83, loss: 3.02970, accuracy: 0.63514\n","Epoch: 27/50, step: 38/83, loss: 3.06853, accuracy: 0.63487\n","Epoch: 27/50, step: 39/83, loss: 3.13909, accuracy: 0.63462\n","Epoch: 27/50, step: 40/83, loss: 3.09774, accuracy: 0.63750\n","Epoch: 27/50, step: 41/83, loss: 3.17775, accuracy: 0.62805\n","Epoch: 27/50, step: 42/83, loss: 3.13948, accuracy: 0.63393\n","Epoch: 27/50, step: 43/83, loss: 3.17237, accuracy: 0.63081\n","Epoch: 27/50, step: 44/83, loss: 3.15804, accuracy: 0.63352\n","Epoch: 27/50, step: 45/83, loss: 3.14387, accuracy: 0.63333\n","Epoch: 27/50, step: 46/83, loss: 3.08627, accuracy: 0.64130\n","Epoch: 27/50, step: 47/83, loss: 3.09781, accuracy: 0.63830\n","Epoch: 27/50, step: 48/83, loss: 3.08496, accuracy: 0.64062\n","Epoch: 27/50, step: 49/83, loss: 3.05138, accuracy: 0.64541\n","Epoch: 27/50, step: 50/83, loss: 3.07587, accuracy: 0.64500\n","Epoch: 27/50, step: 51/83, loss: 3.08020, accuracy: 0.64461\n","Epoch: 27/50, step: 52/83, loss: 3.07723, accuracy: 0.64904\n","Epoch: 27/50, step: 53/83, loss: 3.11668, accuracy: 0.64858\n","Epoch: 27/50, step: 54/83, loss: 3.13517, accuracy: 0.65046\n","Epoch: 27/50, step: 55/83, loss: 3.14007, accuracy: 0.64773\n","Epoch: 27/50, step: 56/83, loss: 3.12678, accuracy: 0.64509\n","Epoch: 27/50, step: 57/83, loss: 3.13384, accuracy: 0.64035\n","Epoch: 27/50, step: 58/83, loss: 3.14719, accuracy: 0.63793\n","Epoch: 27/50, step: 59/83, loss: 3.11883, accuracy: 0.63983\n","Epoch: 27/50, step: 60/83, loss: 3.15162, accuracy: 0.63750\n","Epoch: 27/50, step: 61/83, loss: 3.12515, accuracy: 0.64139\n","Epoch: 27/50, step: 62/83, loss: 3.13241, accuracy: 0.63911\n","Epoch: 27/50, step: 63/83, loss: 3.16867, accuracy: 0.63690\n","Epoch: 27/50, step: 64/83, loss: 3.17540, accuracy: 0.63477\n","Epoch: 27/50, step: 65/83, loss: 3.16268, accuracy: 0.63654\n","Epoch: 27/50, step: 66/83, loss: 3.13808, accuracy: 0.63826\n","Epoch: 27/50, step: 67/83, loss: 3.12599, accuracy: 0.64179\n","Epoch: 27/50, step: 68/83, loss: 3.12708, accuracy: 0.64154\n","Epoch: 27/50, step: 69/83, loss: 3.11159, accuracy: 0.64493\n","Epoch: 27/50, step: 70/83, loss: 3.07789, accuracy: 0.64643\n","Epoch: 27/50, step: 71/83, loss: 3.07607, accuracy: 0.64437\n","Epoch: 27/50, step: 72/83, loss: 3.05572, accuracy: 0.64583\n","Epoch: 27/50, step: 73/83, loss: 3.03963, accuracy: 0.64726\n","Epoch: 27/50, step: 74/83, loss: 3.02753, accuracy: 0.65034\n","Epoch: 27/50, step: 75/83, loss: 2.99766, accuracy: 0.65167\n","Epoch: 27/50, step: 76/83, loss: 2.97701, accuracy: 0.65625\n","Epoch: 27/50, step: 77/83, loss: 2.96977, accuracy: 0.65747\n","Epoch: 27/50, step: 78/83, loss: 3.01155, accuracy: 0.65224\n","Epoch: 27/50, step: 79/83, loss: 3.02549, accuracy: 0.65190\n","Epoch: 27/50, step: 80/83, loss: 3.06498, accuracy: 0.65156\n","Epoch: 27/50, step: 81/83, loss: 3.09030, accuracy: 0.64969\n","Epoch: 27/50, step: 82/83, loss: 3.13941, accuracy: 0.64482\n","Epoch: 27/50, step: 83/83, loss: 3.19914, accuracy: 0.64384\n","Epoch: 27/50, train loss: 3.19914, train accuracy: 0.64384, valid loss: 10.19001, valid accuracy: 0.16742\n","Epoch: 28/50, step: 1/83, loss: 1.91127, accuracy: 0.62500\n","Epoch: 28/50, step: 2/83, loss: 1.71192, accuracy: 0.75000\n","Epoch: 28/50, step: 3/83, loss: 1.88636, accuracy: 0.75000\n","Epoch: 28/50, step: 4/83, loss: 2.51077, accuracy: 0.65625\n","Epoch: 28/50, step: 5/83, loss: 2.48386, accuracy: 0.67500\n","Epoch: 28/50, step: 6/83, loss: 2.72815, accuracy: 0.58333\n","Epoch: 28/50, step: 7/83, loss: 2.86839, accuracy: 0.58929\n","Epoch: 28/50, step: 8/83, loss: 2.58576, accuracy: 0.62500\n","Epoch: 28/50, step: 9/83, loss: 2.56095, accuracy: 0.63889\n","Epoch: 28/50, step: 10/83, loss: 2.42405, accuracy: 0.66250\n","Epoch: 28/50, step: 11/83, loss: 2.58647, accuracy: 0.65909\n","Epoch: 28/50, step: 12/83, loss: 2.71901, accuracy: 0.65625\n","Epoch: 28/50, step: 13/83, loss: 2.69580, accuracy: 0.64423\n","Epoch: 28/50, step: 14/83, loss: 2.59931, accuracy: 0.66964\n","Epoch: 28/50, step: 15/83, loss: 2.60518, accuracy: 0.65833\n","Epoch: 28/50, step: 16/83, loss: 2.72621, accuracy: 0.64844\n","Epoch: 28/50, step: 17/83, loss: 2.71221, accuracy: 0.66176\n","Epoch: 28/50, step: 18/83, loss: 2.68167, accuracy: 0.66667\n","Epoch: 28/50, step: 19/83, loss: 2.81799, accuracy: 0.64474\n","Epoch: 28/50, step: 20/83, loss: 2.80183, accuracy: 0.64375\n","Epoch: 28/50, step: 21/83, loss: 2.83666, accuracy: 0.63690\n","Epoch: 28/50, step: 22/83, loss: 2.87092, accuracy: 0.63068\n","Epoch: 28/50, step: 23/83, loss: 2.81881, accuracy: 0.63043\n","Epoch: 28/50, step: 24/83, loss: 2.76839, accuracy: 0.63021\n","Epoch: 28/50, step: 25/83, loss: 2.79075, accuracy: 0.63000\n","Epoch: 28/50, step: 26/83, loss: 2.74185, accuracy: 0.62981\n","Epoch: 28/50, step: 27/83, loss: 2.71690, accuracy: 0.62500\n","Epoch: 28/50, step: 28/83, loss: 2.73961, accuracy: 0.62500\n","Epoch: 28/50, step: 29/83, loss: 2.73575, accuracy: 0.62069\n","Epoch: 28/50, step: 30/83, loss: 2.85097, accuracy: 0.61250\n","Epoch: 28/50, step: 31/83, loss: 2.96043, accuracy: 0.60081\n","Epoch: 28/50, step: 32/83, loss: 2.88707, accuracy: 0.60938\n","Epoch: 28/50, step: 33/83, loss: 2.90149, accuracy: 0.60985\n","Epoch: 28/50, step: 34/83, loss: 2.93703, accuracy: 0.61029\n","Epoch: 28/50, step: 35/83, loss: 2.98152, accuracy: 0.60714\n","Epoch: 28/50, step: 36/83, loss: 2.96652, accuracy: 0.60417\n","Epoch: 28/50, step: 37/83, loss: 3.00154, accuracy: 0.60135\n","Epoch: 28/50, step: 38/83, loss: 2.99029, accuracy: 0.60526\n","Epoch: 28/50, step: 39/83, loss: 3.04553, accuracy: 0.60577\n","Epoch: 28/50, step: 40/83, loss: 3.04731, accuracy: 0.60625\n","Epoch: 28/50, step: 41/83, loss: 3.06062, accuracy: 0.60061\n","Epoch: 28/50, step: 42/83, loss: 3.04162, accuracy: 0.60119\n","Epoch: 28/50, step: 43/83, loss: 3.02933, accuracy: 0.60174\n","Epoch: 28/50, step: 44/83, loss: 3.01076, accuracy: 0.60795\n","Epoch: 28/50, step: 45/83, loss: 3.00076, accuracy: 0.60556\n","Epoch: 28/50, step: 46/83, loss: 2.98633, accuracy: 0.61141\n","Epoch: 28/50, step: 47/83, loss: 2.99361, accuracy: 0.61170\n","Epoch: 28/50, step: 48/83, loss: 2.94552, accuracy: 0.61719\n","Epoch: 28/50, step: 49/83, loss: 2.97442, accuracy: 0.61480\n","Epoch: 28/50, step: 50/83, loss: 2.95991, accuracy: 0.61750\n","Epoch: 28/50, step: 51/83, loss: 3.00442, accuracy: 0.61765\n","Epoch: 28/50, step: 52/83, loss: 2.99517, accuracy: 0.62019\n","Epoch: 28/50, step: 53/83, loss: 2.96741, accuracy: 0.62500\n","Epoch: 28/50, step: 54/83, loss: 2.97698, accuracy: 0.62731\n","Epoch: 28/50, step: 55/83, loss: 2.97378, accuracy: 0.62500\n","Epoch: 28/50, step: 56/83, loss: 2.97886, accuracy: 0.62723\n","Epoch: 28/50, step: 57/83, loss: 2.97293, accuracy: 0.62719\n","Epoch: 28/50, step: 58/83, loss: 2.96428, accuracy: 0.62931\n","Epoch: 28/50, step: 59/83, loss: 2.97334, accuracy: 0.62712\n","Epoch: 28/50, step: 60/83, loss: 3.01453, accuracy: 0.62500\n","Epoch: 28/50, step: 61/83, loss: 3.02185, accuracy: 0.62295\n","Epoch: 28/50, step: 62/83, loss: 3.01352, accuracy: 0.62298\n","Epoch: 28/50, step: 63/83, loss: 3.03128, accuracy: 0.62302\n","Epoch: 28/50, step: 64/83, loss: 3.05559, accuracy: 0.61914\n","Epoch: 28/50, step: 65/83, loss: 3.06218, accuracy: 0.61731\n","Epoch: 28/50, step: 66/83, loss: 3.05114, accuracy: 0.61742\n","Epoch: 28/50, step: 67/83, loss: 3.06941, accuracy: 0.62127\n","Epoch: 28/50, step: 68/83, loss: 3.03373, accuracy: 0.62684\n","Epoch: 28/50, step: 69/83, loss: 3.05335, accuracy: 0.62500\n","Epoch: 28/50, step: 70/83, loss: 3.03436, accuracy: 0.62679\n","Epoch: 28/50, step: 71/83, loss: 3.02755, accuracy: 0.62676\n","Epoch: 28/50, step: 72/83, loss: 3.03440, accuracy: 0.62674\n","Epoch: 28/50, step: 73/83, loss: 3.05588, accuracy: 0.62671\n","Epoch: 28/50, step: 74/83, loss: 3.07457, accuracy: 0.62500\n","Epoch: 28/50, step: 75/83, loss: 3.09090, accuracy: 0.62667\n","Epoch: 28/50, step: 76/83, loss: 3.09651, accuracy: 0.62336\n","Epoch: 28/50, step: 77/83, loss: 3.08978, accuracy: 0.62338\n","Epoch: 28/50, step: 78/83, loss: 3.06882, accuracy: 0.62660\n","Epoch: 28/50, step: 79/83, loss: 3.06162, accuracy: 0.62658\n","Epoch: 28/50, step: 80/83, loss: 3.05407, accuracy: 0.62813\n","Epoch: 28/50, step: 81/83, loss: 3.03689, accuracy: 0.62963\n","Epoch: 28/50, step: 82/83, loss: 3.05521, accuracy: 0.62652\n","Epoch: 28/50, step: 83/83, loss: 3.11030, accuracy: 0.62709\n","Epoch: 28/50, train loss: 3.11030, train accuracy: 0.62709, valid loss: 9.85076, valid accuracy: 0.16742\n","Epoch: 29/50, step: 1/83, loss: 3.07069, accuracy: 0.62500\n","Epoch: 29/50, step: 2/83, loss: 3.27940, accuracy: 0.56250\n","Epoch: 29/50, step: 3/83, loss: 2.80525, accuracy: 0.54167\n","Epoch: 29/50, step: 4/83, loss: 3.36544, accuracy: 0.50000\n","Epoch: 29/50, step: 5/83, loss: 3.02158, accuracy: 0.55000\n","Epoch: 29/50, step: 6/83, loss: 3.11120, accuracy: 0.52083\n","Epoch: 29/50, step: 7/83, loss: 3.02974, accuracy: 0.51786\n","Epoch: 29/50, step: 8/83, loss: 3.06613, accuracy: 0.53125\n","Epoch: 29/50, step: 9/83, loss: 3.01368, accuracy: 0.54167\n","Epoch: 29/50, step: 10/83, loss: 3.05941, accuracy: 0.55000\n","Epoch: 29/50, step: 11/83, loss: 3.30130, accuracy: 0.54545\n","Epoch: 29/50, step: 12/83, loss: 3.47133, accuracy: 0.54167\n","Epoch: 29/50, step: 13/83, loss: 3.53559, accuracy: 0.53846\n","Epoch: 29/50, step: 14/83, loss: 3.51796, accuracy: 0.55357\n","Epoch: 29/50, step: 15/83, loss: 3.46188, accuracy: 0.55000\n","Epoch: 29/50, step: 16/83, loss: 3.45440, accuracy: 0.53906\n","Epoch: 29/50, step: 17/83, loss: 3.37929, accuracy: 0.55147\n","Epoch: 29/50, step: 18/83, loss: 3.26577, accuracy: 0.56944\n","Epoch: 29/50, step: 19/83, loss: 3.21657, accuracy: 0.58553\n","Epoch: 29/50, step: 20/83, loss: 3.08914, accuracy: 0.59375\n","Epoch: 29/50, step: 21/83, loss: 2.96850, accuracy: 0.61310\n","Epoch: 29/50, step: 22/83, loss: 2.95012, accuracy: 0.61364\n","Epoch: 29/50, step: 23/83, loss: 3.04266, accuracy: 0.61413\n","Epoch: 29/50, step: 24/83, loss: 3.20604, accuracy: 0.60417\n","Epoch: 29/50, step: 25/83, loss: 3.24655, accuracy: 0.61000\n","Epoch: 29/50, step: 26/83, loss: 3.28843, accuracy: 0.60577\n","Epoch: 29/50, step: 27/83, loss: 3.28674, accuracy: 0.60185\n","Epoch: 29/50, step: 28/83, loss: 3.28773, accuracy: 0.60268\n","Epoch: 29/50, step: 29/83, loss: 3.27009, accuracy: 0.60345\n","Epoch: 29/50, step: 30/83, loss: 3.21361, accuracy: 0.60417\n","Epoch: 29/50, step: 31/83, loss: 3.23550, accuracy: 0.60887\n","Epoch: 29/50, step: 32/83, loss: 3.26936, accuracy: 0.61328\n","Epoch: 29/50, step: 33/83, loss: 3.25756, accuracy: 0.61742\n","Epoch: 29/50, step: 34/83, loss: 3.27679, accuracy: 0.60662\n","Epoch: 29/50, step: 35/83, loss: 3.27998, accuracy: 0.61071\n","Epoch: 29/50, step: 36/83, loss: 3.30463, accuracy: 0.61111\n","Epoch: 29/50, step: 37/83, loss: 3.23509, accuracy: 0.61486\n","Epoch: 29/50, step: 38/83, loss: 3.27509, accuracy: 0.61513\n","Epoch: 29/50, step: 39/83, loss: 3.28128, accuracy: 0.61538\n","Epoch: 29/50, step: 40/83, loss: 3.23805, accuracy: 0.61563\n","Epoch: 29/50, step: 41/83, loss: 3.24146, accuracy: 0.61585\n","Epoch: 29/50, step: 42/83, loss: 3.24605, accuracy: 0.61905\n","Epoch: 29/50, step: 43/83, loss: 3.20945, accuracy: 0.61628\n","Epoch: 29/50, step: 44/83, loss: 3.17251, accuracy: 0.61932\n","Epoch: 29/50, step: 45/83, loss: 3.14111, accuracy: 0.61667\n","Epoch: 29/50, step: 46/83, loss: 3.14585, accuracy: 0.61685\n","Epoch: 29/50, step: 47/83, loss: 3.14744, accuracy: 0.61702\n","Epoch: 29/50, step: 48/83, loss: 3.17051, accuracy: 0.61458\n","Epoch: 29/50, step: 49/83, loss: 3.16059, accuracy: 0.61735\n","Epoch: 29/50, step: 50/83, loss: 3.14700, accuracy: 0.62000\n","Epoch: 29/50, step: 51/83, loss: 3.15285, accuracy: 0.61765\n","Epoch: 29/50, step: 52/83, loss: 3.11990, accuracy: 0.62260\n","Epoch: 29/50, step: 53/83, loss: 3.14374, accuracy: 0.61792\n","Epoch: 29/50, step: 54/83, loss: 3.14699, accuracy: 0.61806\n","Epoch: 29/50, step: 55/83, loss: 3.18415, accuracy: 0.61364\n","Epoch: 29/50, step: 56/83, loss: 3.15420, accuracy: 0.61830\n","Epoch: 29/50, step: 57/83, loss: 3.17961, accuracy: 0.61623\n","Epoch: 29/50, step: 58/83, loss: 3.18276, accuracy: 0.61638\n","Epoch: 29/50, step: 59/83, loss: 3.21707, accuracy: 0.61653\n","Epoch: 29/50, step: 60/83, loss: 3.18657, accuracy: 0.61875\n","Epoch: 29/50, step: 61/83, loss: 3.14344, accuracy: 0.62090\n","Epoch: 29/50, step: 62/83, loss: 3.10353, accuracy: 0.62298\n","Epoch: 29/50, step: 63/83, loss: 3.08363, accuracy: 0.62302\n","Epoch: 29/50, step: 64/83, loss: 3.06022, accuracy: 0.62500\n","Epoch: 29/50, step: 65/83, loss: 3.03691, accuracy: 0.62692\n","Epoch: 29/50, step: 66/83, loss: 3.02642, accuracy: 0.62879\n","Epoch: 29/50, step: 67/83, loss: 3.04702, accuracy: 0.62500\n","Epoch: 29/50, step: 68/83, loss: 3.13324, accuracy: 0.61949\n","Epoch: 29/50, step: 69/83, loss: 3.12451, accuracy: 0.61957\n","Epoch: 29/50, step: 70/83, loss: 3.15526, accuracy: 0.61964\n","Epoch: 29/50, step: 71/83, loss: 3.14558, accuracy: 0.61972\n","Epoch: 29/50, step: 72/83, loss: 3.12447, accuracy: 0.62153\n","Epoch: 29/50, step: 73/83, loss: 3.12657, accuracy: 0.62158\n","Epoch: 29/50, step: 74/83, loss: 3.13926, accuracy: 0.61824\n","Epoch: 29/50, step: 75/83, loss: 3.11389, accuracy: 0.62333\n","Epoch: 29/50, step: 76/83, loss: 3.10563, accuracy: 0.62336\n","Epoch: 29/50, step: 77/83, loss: 3.10509, accuracy: 0.62500\n","Epoch: 29/50, step: 78/83, loss: 3.11022, accuracy: 0.62019\n","Epoch: 29/50, step: 79/83, loss: 3.10195, accuracy: 0.62184\n","Epoch: 29/50, step: 80/83, loss: 3.11777, accuracy: 0.62031\n","Epoch: 29/50, step: 81/83, loss: 3.10333, accuracy: 0.62037\n","Epoch: 29/50, step: 82/83, loss: 3.09678, accuracy: 0.62195\n","Epoch: 29/50, step: 83/83, loss: 3.16066, accuracy: 0.62100\n","Epoch: 29/50, train loss: 3.16066, train accuracy: 0.62100, valid loss: 9.77068, valid accuracy: 0.16742\n","Epoch: 30/50, step: 1/83, loss: 4.29990, accuracy: 0.62500\n","Epoch: 30/50, step: 2/83, loss: 3.45204, accuracy: 0.62500\n","Epoch: 30/50, step: 3/83, loss: 3.11304, accuracy: 0.58333\n","Epoch: 30/50, step: 4/83, loss: 3.16320, accuracy: 0.59375\n","Epoch: 30/50, step: 5/83, loss: 3.21068, accuracy: 0.57500\n","Epoch: 30/50, step: 6/83, loss: 3.41125, accuracy: 0.56250\n","Epoch: 30/50, step: 7/83, loss: 3.13060, accuracy: 0.60714\n","Epoch: 30/50, step: 8/83, loss: 3.16546, accuracy: 0.60938\n","Epoch: 30/50, step: 9/83, loss: 3.06989, accuracy: 0.63889\n","Epoch: 30/50, step: 10/83, loss: 2.82995, accuracy: 0.66250\n","Epoch: 30/50, step: 11/83, loss: 2.94333, accuracy: 0.64773\n","Epoch: 30/50, step: 12/83, loss: 2.82500, accuracy: 0.66667\n","Epoch: 30/50, step: 13/83, loss: 2.80187, accuracy: 0.66346\n","Epoch: 30/50, step: 14/83, loss: 2.85227, accuracy: 0.66071\n","Epoch: 30/50, step: 15/83, loss: 2.77068, accuracy: 0.65833\n","Epoch: 30/50, step: 16/83, loss: 2.80598, accuracy: 0.64844\n","Epoch: 30/50, step: 17/83, loss: 2.95015, accuracy: 0.64706\n","Epoch: 30/50, step: 18/83, loss: 2.98172, accuracy: 0.64583\n","Epoch: 30/50, step: 19/83, loss: 2.95912, accuracy: 0.65132\n","Epoch: 30/50, step: 20/83, loss: 3.02289, accuracy: 0.65000\n","Epoch: 30/50, step: 21/83, loss: 3.00216, accuracy: 0.64286\n","Epoch: 30/50, step: 22/83, loss: 2.98252, accuracy: 0.64205\n","Epoch: 30/50, step: 23/83, loss: 2.96322, accuracy: 0.63587\n","Epoch: 30/50, step: 24/83, loss: 2.94106, accuracy: 0.64062\n","Epoch: 30/50, step: 25/83, loss: 3.03291, accuracy: 0.63000\n","Epoch: 30/50, step: 26/83, loss: 3.01010, accuracy: 0.63942\n","Epoch: 30/50, step: 27/83, loss: 3.01849, accuracy: 0.64352\n","Epoch: 30/50, step: 28/83, loss: 3.03612, accuracy: 0.63839\n","Epoch: 30/50, step: 29/83, loss: 3.08940, accuracy: 0.63362\n","Epoch: 30/50, step: 30/83, loss: 3.15854, accuracy: 0.62917\n","Epoch: 30/50, step: 31/83, loss: 3.07536, accuracy: 0.63306\n","Epoch: 30/50, step: 32/83, loss: 3.05862, accuracy: 0.63281\n","Epoch: 30/50, step: 33/83, loss: 3.07147, accuracy: 0.63258\n","Epoch: 30/50, step: 34/83, loss: 3.03280, accuracy: 0.63235\n","Epoch: 30/50, step: 35/83, loss: 2.98524, accuracy: 0.63571\n","Epoch: 30/50, step: 36/83, loss: 2.96221, accuracy: 0.64236\n","Epoch: 30/50, step: 37/83, loss: 2.97945, accuracy: 0.63851\n","Epoch: 30/50, step: 38/83, loss: 2.96437, accuracy: 0.64145\n","Epoch: 30/50, step: 39/83, loss: 2.97614, accuracy: 0.64423\n","Epoch: 30/50, step: 40/83, loss: 2.99669, accuracy: 0.63750\n","Epoch: 30/50, step: 41/83, loss: 3.01571, accuracy: 0.63415\n","Epoch: 30/50, step: 42/83, loss: 3.04918, accuracy: 0.63095\n","Epoch: 30/50, step: 43/83, loss: 3.01495, accuracy: 0.63663\n","Epoch: 30/50, step: 44/83, loss: 3.04650, accuracy: 0.63068\n","Epoch: 30/50, step: 45/83, loss: 3.03619, accuracy: 0.63333\n","Epoch: 30/50, step: 46/83, loss: 3.03869, accuracy: 0.63587\n","Epoch: 30/50, step: 47/83, loss: 3.08333, accuracy: 0.63032\n","Epoch: 30/50, step: 48/83, loss: 3.08762, accuracy: 0.63281\n","Epoch: 30/50, step: 49/83, loss: 3.05997, accuracy: 0.63520\n","Epoch: 30/50, step: 50/83, loss: 3.03148, accuracy: 0.63750\n","Epoch: 30/50, step: 51/83, loss: 3.01344, accuracy: 0.64216\n","Epoch: 30/50, step: 52/83, loss: 3.00668, accuracy: 0.63942\n","Epoch: 30/50, step: 53/83, loss: 2.99924, accuracy: 0.64151\n","Epoch: 30/50, step: 54/83, loss: 3.00291, accuracy: 0.64120\n","Epoch: 30/50, step: 55/83, loss: 3.02381, accuracy: 0.64091\n","Epoch: 30/50, step: 56/83, loss: 3.04437, accuracy: 0.64062\n","Epoch: 30/50, step: 57/83, loss: 3.03150, accuracy: 0.64254\n","Epoch: 30/50, step: 58/83, loss: 3.05630, accuracy: 0.63578\n","Epoch: 30/50, step: 59/83, loss: 3.06685, accuracy: 0.63347\n","Epoch: 30/50, step: 60/83, loss: 3.04054, accuracy: 0.63750\n","Epoch: 30/50, step: 61/83, loss: 3.05978, accuracy: 0.64139\n","Epoch: 30/50, step: 62/83, loss: 3.04973, accuracy: 0.63508\n","Epoch: 30/50, step: 63/83, loss: 3.01421, accuracy: 0.64087\n","Epoch: 30/50, step: 64/83, loss: 3.00666, accuracy: 0.64062\n","Epoch: 30/50, step: 65/83, loss: 2.98550, accuracy: 0.64038\n","Epoch: 30/50, step: 66/83, loss: 3.02118, accuracy: 0.63636\n","Epoch: 30/50, step: 67/83, loss: 3.01292, accuracy: 0.63806\n","Epoch: 30/50, step: 68/83, loss: 3.06101, accuracy: 0.63419\n","Epoch: 30/50, step: 69/83, loss: 3.05248, accuracy: 0.63768\n","Epoch: 30/50, step: 70/83, loss: 3.03076, accuracy: 0.63929\n","Epoch: 30/50, step: 71/83, loss: 3.00723, accuracy: 0.64261\n","Epoch: 30/50, step: 72/83, loss: 2.99749, accuracy: 0.64583\n","Epoch: 30/50, step: 73/83, loss: 2.99659, accuracy: 0.64384\n","Epoch: 30/50, step: 74/83, loss: 2.96366, accuracy: 0.64696\n","Epoch: 30/50, step: 75/83, loss: 3.01989, accuracy: 0.64333\n","Epoch: 30/50, step: 76/83, loss: 3.05135, accuracy: 0.63980\n","Epoch: 30/50, step: 77/83, loss: 3.05492, accuracy: 0.63799\n","Epoch: 30/50, step: 78/83, loss: 3.06605, accuracy: 0.64103\n","Epoch: 30/50, step: 79/83, loss: 3.07259, accuracy: 0.63924\n","Epoch: 30/50, step: 80/83, loss: 3.05184, accuracy: 0.64219\n","Epoch: 30/50, step: 81/83, loss: 3.05386, accuracy: 0.63735\n","Epoch: 30/50, step: 82/83, loss: 3.04648, accuracy: 0.63720\n","Epoch: 30/50, step: 83/83, loss: 3.10180, accuracy: 0.63775\n","Epoch: 30/50, train loss: 3.10180, train accuracy: 0.63775, valid loss: 9.66183, valid accuracy: 0.16742\n","Epoch: 31/50, step: 1/83, loss: 3.36951, accuracy: 0.62500\n","Epoch: 31/50, step: 2/83, loss: 3.03707, accuracy: 0.56250\n","Epoch: 31/50, step: 3/83, loss: 3.47750, accuracy: 0.54167\n","Epoch: 31/50, step: 4/83, loss: 3.16150, accuracy: 0.59375\n","Epoch: 31/50, step: 5/83, loss: 3.54103, accuracy: 0.55000\n","Epoch: 31/50, step: 6/83, loss: 3.53307, accuracy: 0.52083\n","Epoch: 31/50, step: 7/83, loss: 3.48846, accuracy: 0.55357\n","Epoch: 31/50, step: 8/83, loss: 3.34181, accuracy: 0.57812\n","Epoch: 31/50, step: 9/83, loss: 3.46282, accuracy: 0.55556\n","Epoch: 31/50, step: 10/83, loss: 3.46275, accuracy: 0.57500\n","Epoch: 31/50, step: 11/83, loss: 3.43965, accuracy: 0.57955\n","Epoch: 31/50, step: 12/83, loss: 3.36183, accuracy: 0.58333\n","Epoch: 31/50, step: 13/83, loss: 3.34717, accuracy: 0.58654\n","Epoch: 31/50, step: 14/83, loss: 3.45684, accuracy: 0.58036\n","Epoch: 31/50, step: 15/83, loss: 3.44470, accuracy: 0.59167\n","Epoch: 31/50, step: 16/83, loss: 3.43258, accuracy: 0.59375\n","Epoch: 31/50, step: 17/83, loss: 3.37240, accuracy: 0.59559\n","Epoch: 31/50, step: 18/83, loss: 3.38287, accuracy: 0.59722\n","Epoch: 31/50, step: 19/83, loss: 3.53836, accuracy: 0.57237\n","Epoch: 31/50, step: 20/83, loss: 3.43911, accuracy: 0.58125\n","Epoch: 31/50, step: 21/83, loss: 3.51767, accuracy: 0.58333\n","Epoch: 31/50, step: 22/83, loss: 3.52067, accuracy: 0.57386\n","Epoch: 31/50, step: 23/83, loss: 3.43174, accuracy: 0.58152\n","Epoch: 31/50, step: 24/83, loss: 3.48211, accuracy: 0.57292\n","Epoch: 31/50, step: 25/83, loss: 3.44339, accuracy: 0.57500\n","Epoch: 31/50, step: 26/83, loss: 3.54059, accuracy: 0.56250\n","Epoch: 31/50, step: 27/83, loss: 3.50743, accuracy: 0.56481\n","Epoch: 31/50, step: 28/83, loss: 3.43374, accuracy: 0.57589\n","Epoch: 31/50, step: 29/83, loss: 3.40410, accuracy: 0.58190\n","Epoch: 31/50, step: 30/83, loss: 3.55280, accuracy: 0.57083\n","Epoch: 31/50, step: 31/83, loss: 3.53521, accuracy: 0.57661\n","Epoch: 31/50, step: 32/83, loss: 3.58819, accuracy: 0.57422\n","Epoch: 31/50, step: 33/83, loss: 3.49735, accuracy: 0.58333\n","Epoch: 31/50, step: 34/83, loss: 3.55189, accuracy: 0.58088\n","Epoch: 31/50, step: 35/83, loss: 3.53906, accuracy: 0.58929\n","Epoch: 31/50, step: 36/83, loss: 3.46153, accuracy: 0.59722\n","Epoch: 31/50, step: 37/83, loss: 3.43628, accuracy: 0.60135\n","Epoch: 31/50, step: 38/83, loss: 3.36324, accuracy: 0.60526\n","Epoch: 31/50, step: 39/83, loss: 3.31325, accuracy: 0.61218\n","Epoch: 31/50, step: 40/83, loss: 3.27146, accuracy: 0.61563\n","Epoch: 31/50, step: 41/83, loss: 3.28353, accuracy: 0.60671\n","Epoch: 31/50, step: 42/83, loss: 3.26516, accuracy: 0.60417\n","Epoch: 31/50, step: 43/83, loss: 3.25182, accuracy: 0.59884\n","Epoch: 31/50, step: 44/83, loss: 3.25724, accuracy: 0.59659\n","Epoch: 31/50, step: 45/83, loss: 3.28048, accuracy: 0.59444\n","Epoch: 31/50, step: 46/83, loss: 3.28654, accuracy: 0.59511\n","Epoch: 31/50, step: 47/83, loss: 3.25031, accuracy: 0.59574\n","Epoch: 31/50, step: 48/83, loss: 3.23484, accuracy: 0.59896\n","Epoch: 31/50, step: 49/83, loss: 3.24020, accuracy: 0.59694\n","Epoch: 31/50, step: 50/83, loss: 3.24425, accuracy: 0.59750\n","Epoch: 31/50, step: 51/83, loss: 3.24503, accuracy: 0.59804\n","Epoch: 31/50, step: 52/83, loss: 3.24594, accuracy: 0.59856\n","Epoch: 31/50, step: 53/83, loss: 3.19670, accuracy: 0.60377\n","Epoch: 31/50, step: 54/83, loss: 3.18911, accuracy: 0.60185\n","Epoch: 31/50, step: 55/83, loss: 3.21505, accuracy: 0.59773\n","Epoch: 31/50, step: 56/83, loss: 3.19910, accuracy: 0.60268\n","Epoch: 31/50, step: 57/83, loss: 3.15224, accuracy: 0.60965\n","Epoch: 31/50, step: 58/83, loss: 3.13785, accuracy: 0.61207\n","Epoch: 31/50, step: 59/83, loss: 3.18369, accuracy: 0.61017\n","Epoch: 31/50, step: 60/83, loss: 3.21800, accuracy: 0.60833\n","Epoch: 31/50, step: 61/83, loss: 3.20317, accuracy: 0.61066\n","Epoch: 31/50, step: 62/83, loss: 3.19378, accuracy: 0.61089\n","Epoch: 31/50, step: 63/83, loss: 3.18050, accuracy: 0.61310\n","Epoch: 31/50, step: 64/83, loss: 3.17280, accuracy: 0.60938\n","Epoch: 31/50, step: 65/83, loss: 3.16743, accuracy: 0.60769\n","Epoch: 31/50, step: 66/83, loss: 3.18538, accuracy: 0.60606\n","Epoch: 31/50, step: 67/83, loss: 3.18938, accuracy: 0.60634\n","Epoch: 31/50, step: 68/83, loss: 3.17699, accuracy: 0.60846\n","Epoch: 31/50, step: 69/83, loss: 3.16841, accuracy: 0.61051\n","Epoch: 31/50, step: 70/83, loss: 3.15602, accuracy: 0.61071\n","Epoch: 31/50, step: 71/83, loss: 3.13160, accuracy: 0.61444\n","Epoch: 31/50, step: 72/83, loss: 3.09546, accuracy: 0.61979\n","Epoch: 31/50, step: 73/83, loss: 3.09670, accuracy: 0.61986\n","Epoch: 31/50, step: 74/83, loss: 3.10330, accuracy: 0.61824\n","Epoch: 31/50, step: 75/83, loss: 3.10992, accuracy: 0.61667\n","Epoch: 31/50, step: 76/83, loss: 3.08959, accuracy: 0.62007\n","Epoch: 31/50, step: 77/83, loss: 3.07784, accuracy: 0.62175\n","Epoch: 31/50, step: 78/83, loss: 3.05869, accuracy: 0.62340\n","Epoch: 31/50, step: 79/83, loss: 3.05015, accuracy: 0.62500\n","Epoch: 31/50, step: 80/83, loss: 3.06716, accuracy: 0.62344\n","Epoch: 31/50, step: 81/83, loss: 3.05871, accuracy: 0.62654\n","Epoch: 31/50, step: 82/83, loss: 3.09428, accuracy: 0.62652\n","Epoch: 31/50, step: 83/83, loss: 3.15538, accuracy: 0.62557\n","Epoch: 31/50, train loss: 3.15538, train accuracy: 0.62557, valid loss: 9.77733, valid accuracy: 0.18100\n","Epoch: 32/50, step: 1/83, loss: 3.26447, accuracy: 0.62500\n","Epoch: 32/50, step: 2/83, loss: 2.00227, accuracy: 0.75000\n","Epoch: 32/50, step: 3/83, loss: 2.44976, accuracy: 0.70833\n","Epoch: 32/50, step: 4/83, loss: 2.46989, accuracy: 0.68750\n","Epoch: 32/50, step: 5/83, loss: 2.69406, accuracy: 0.67500\n","Epoch: 32/50, step: 6/83, loss: 2.51751, accuracy: 0.64583\n","Epoch: 32/50, step: 7/83, loss: 2.52257, accuracy: 0.64286\n","Epoch: 32/50, step: 8/83, loss: 2.42987, accuracy: 0.65625\n","Epoch: 32/50, step: 9/83, loss: 2.54573, accuracy: 0.63889\n","Epoch: 32/50, step: 10/83, loss: 2.46045, accuracy: 0.62500\n","Epoch: 32/50, step: 11/83, loss: 2.47117, accuracy: 0.62500\n","Epoch: 32/50, step: 12/83, loss: 2.47990, accuracy: 0.63542\n","Epoch: 32/50, step: 13/83, loss: 2.41959, accuracy: 0.63462\n","Epoch: 32/50, step: 14/83, loss: 2.55944, accuracy: 0.62500\n","Epoch: 32/50, step: 15/83, loss: 2.69469, accuracy: 0.60000\n","Epoch: 32/50, step: 16/83, loss: 2.67447, accuracy: 0.60938\n","Epoch: 32/50, step: 17/83, loss: 2.76427, accuracy: 0.61029\n","Epoch: 32/50, step: 18/83, loss: 2.78934, accuracy: 0.61111\n","Epoch: 32/50, step: 19/83, loss: 2.77265, accuracy: 0.61184\n","Epoch: 32/50, step: 20/83, loss: 2.80546, accuracy: 0.61875\n","Epoch: 32/50, step: 21/83, loss: 3.02985, accuracy: 0.61310\n","Epoch: 32/50, step: 22/83, loss: 3.00604, accuracy: 0.61364\n","Epoch: 32/50, step: 23/83, loss: 2.93116, accuracy: 0.63043\n","Epoch: 32/50, step: 24/83, loss: 2.94672, accuracy: 0.63021\n","Epoch: 32/50, step: 25/83, loss: 2.89129, accuracy: 0.64000\n","Epoch: 32/50, step: 26/83, loss: 2.84000, accuracy: 0.64904\n","Epoch: 32/50, step: 27/83, loss: 2.85356, accuracy: 0.64815\n","Epoch: 32/50, step: 28/83, loss: 2.87349, accuracy: 0.64286\n","Epoch: 32/50, step: 29/83, loss: 2.86062, accuracy: 0.63793\n","Epoch: 32/50, step: 30/83, loss: 2.84398, accuracy: 0.64167\n","Epoch: 32/50, step: 31/83, loss: 2.85896, accuracy: 0.64516\n","Epoch: 32/50, step: 32/83, loss: 2.92981, accuracy: 0.64062\n","Epoch: 32/50, step: 33/83, loss: 3.00003, accuracy: 0.63258\n","Epoch: 32/50, step: 34/83, loss: 3.04630, accuracy: 0.62500\n","Epoch: 32/50, step: 35/83, loss: 3.02882, accuracy: 0.62857\n","Epoch: 32/50, step: 36/83, loss: 3.06570, accuracy: 0.62153\n","Epoch: 32/50, step: 37/83, loss: 3.08201, accuracy: 0.61824\n","Epoch: 32/50, step: 38/83, loss: 3.10988, accuracy: 0.61513\n","Epoch: 32/50, step: 39/83, loss: 3.12228, accuracy: 0.61218\n","Epoch: 32/50, step: 40/83, loss: 3.14075, accuracy: 0.60938\n","Epoch: 32/50, step: 41/83, loss: 3.10024, accuracy: 0.61280\n","Epoch: 32/50, step: 42/83, loss: 3.08836, accuracy: 0.61310\n","Epoch: 32/50, step: 43/83, loss: 3.07232, accuracy: 0.61628\n","Epoch: 32/50, step: 44/83, loss: 3.04058, accuracy: 0.61648\n","Epoch: 32/50, step: 45/83, loss: 2.99259, accuracy: 0.61944\n","Epoch: 32/50, step: 46/83, loss: 2.99516, accuracy: 0.61957\n","Epoch: 32/50, step: 47/83, loss: 2.98241, accuracy: 0.61968\n","Epoch: 32/50, step: 48/83, loss: 2.94536, accuracy: 0.62500\n","Epoch: 32/50, step: 49/83, loss: 2.93120, accuracy: 0.62755\n","Epoch: 32/50, step: 50/83, loss: 2.90639, accuracy: 0.62250\n","Epoch: 32/50, step: 51/83, loss: 2.94334, accuracy: 0.62010\n","Epoch: 32/50, step: 52/83, loss: 2.93518, accuracy: 0.62260\n","Epoch: 32/50, step: 53/83, loss: 2.96037, accuracy: 0.62028\n","Epoch: 32/50, step: 54/83, loss: 2.93410, accuracy: 0.62269\n","Epoch: 32/50, step: 55/83, loss: 2.96568, accuracy: 0.61591\n","Epoch: 32/50, step: 56/83, loss: 2.98987, accuracy: 0.60938\n","Epoch: 32/50, step: 57/83, loss: 2.96475, accuracy: 0.61184\n","Epoch: 32/50, step: 58/83, loss: 2.93878, accuracy: 0.61638\n","Epoch: 32/50, step: 59/83, loss: 2.94483, accuracy: 0.61653\n","Epoch: 32/50, step: 60/83, loss: 2.96696, accuracy: 0.61458\n","Epoch: 32/50, step: 61/83, loss: 2.93116, accuracy: 0.61680\n","Epoch: 32/50, step: 62/83, loss: 2.94118, accuracy: 0.61492\n","Epoch: 32/50, step: 63/83, loss: 2.90326, accuracy: 0.62103\n","Epoch: 32/50, step: 64/83, loss: 2.92459, accuracy: 0.61914\n","Epoch: 32/50, step: 65/83, loss: 2.89837, accuracy: 0.62115\n","Epoch: 32/50, step: 66/83, loss: 2.90572, accuracy: 0.62121\n","Epoch: 32/50, step: 67/83, loss: 2.91262, accuracy: 0.62127\n","Epoch: 32/50, step: 68/83, loss: 2.90215, accuracy: 0.62500\n","Epoch: 32/50, step: 69/83, loss: 2.93176, accuracy: 0.62681\n","Epoch: 32/50, step: 70/83, loss: 2.93562, accuracy: 0.62679\n","Epoch: 32/50, step: 71/83, loss: 2.91693, accuracy: 0.63028\n","Epoch: 32/50, step: 72/83, loss: 2.88464, accuracy: 0.63368\n","Epoch: 32/50, step: 73/83, loss: 2.85282, accuracy: 0.63699\n","Epoch: 32/50, step: 74/83, loss: 2.82361, accuracy: 0.64020\n","Epoch: 32/50, step: 75/83, loss: 2.84427, accuracy: 0.63833\n","Epoch: 32/50, step: 76/83, loss: 2.81378, accuracy: 0.64309\n","Epoch: 32/50, step: 77/83, loss: 2.79549, accuracy: 0.64773\n","Epoch: 32/50, step: 78/83, loss: 2.80080, accuracy: 0.65064\n","Epoch: 32/50, step: 79/83, loss: 2.81988, accuracy: 0.64873\n","Epoch: 32/50, step: 80/83, loss: 2.80036, accuracy: 0.65156\n","Epoch: 32/50, step: 81/83, loss: 2.77268, accuracy: 0.65432\n","Epoch: 32/50, step: 82/83, loss: 2.76714, accuracy: 0.65549\n","Epoch: 32/50, step: 83/83, loss: 2.82743, accuracy: 0.65449\n","Epoch: 32/50, train loss: 2.82743, train accuracy: 0.65449, valid loss: 10.17291, valid accuracy: 0.17195\n","Epoch: 33/50, step: 1/83, loss: 4.16926, accuracy: 0.62500\n","Epoch: 33/50, step: 2/83, loss: 4.16498, accuracy: 0.56250\n","Epoch: 33/50, step: 3/83, loss: 3.93649, accuracy: 0.58333\n","Epoch: 33/50, step: 4/83, loss: 3.86728, accuracy: 0.56250\n","Epoch: 33/50, step: 5/83, loss: 3.22534, accuracy: 0.62500\n","Epoch: 33/50, step: 6/83, loss: 3.12462, accuracy: 0.62500\n","Epoch: 33/50, step: 7/83, loss: 2.77011, accuracy: 0.64286\n","Epoch: 33/50, step: 8/83, loss: 2.99461, accuracy: 0.57812\n","Epoch: 33/50, step: 9/83, loss: 3.06252, accuracy: 0.56944\n","Epoch: 33/50, step: 10/83, loss: 2.98202, accuracy: 0.58750\n","Epoch: 33/50, step: 11/83, loss: 2.84411, accuracy: 0.62500\n","Epoch: 33/50, step: 12/83, loss: 2.74060, accuracy: 0.63542\n","Epoch: 33/50, step: 13/83, loss: 2.77405, accuracy: 0.64423\n","Epoch: 33/50, step: 14/83, loss: 2.68317, accuracy: 0.63393\n","Epoch: 33/50, step: 15/83, loss: 2.72831, accuracy: 0.64167\n","Epoch: 33/50, step: 16/83, loss: 2.71015, accuracy: 0.64844\n","Epoch: 33/50, step: 17/83, loss: 2.63562, accuracy: 0.66176\n","Epoch: 33/50, step: 18/83, loss: 2.57567, accuracy: 0.66667\n","Epoch: 33/50, step: 19/83, loss: 2.57383, accuracy: 0.66447\n","Epoch: 33/50, step: 20/83, loss: 2.56796, accuracy: 0.66875\n","Epoch: 33/50, step: 21/83, loss: 2.67857, accuracy: 0.65476\n","Epoch: 33/50, step: 22/83, loss: 2.75195, accuracy: 0.64773\n","Epoch: 33/50, step: 23/83, loss: 2.74781, accuracy: 0.64674\n","Epoch: 33/50, step: 24/83, loss: 2.78479, accuracy: 0.63542\n","Epoch: 33/50, step: 25/83, loss: 2.77422, accuracy: 0.64500\n","Epoch: 33/50, step: 26/83, loss: 2.83939, accuracy: 0.63942\n","Epoch: 33/50, step: 27/83, loss: 2.83159, accuracy: 0.63889\n","Epoch: 33/50, step: 28/83, loss: 2.86826, accuracy: 0.63393\n","Epoch: 33/50, step: 29/83, loss: 2.87849, accuracy: 0.63793\n","Epoch: 33/50, step: 30/83, loss: 2.88615, accuracy: 0.64583\n","Epoch: 33/50, step: 31/83, loss: 2.84035, accuracy: 0.64516\n","Epoch: 33/50, step: 32/83, loss: 2.83456, accuracy: 0.64844\n","Epoch: 33/50, step: 33/83, loss: 2.79394, accuracy: 0.65909\n","Epoch: 33/50, step: 34/83, loss: 2.77982, accuracy: 0.66176\n","Epoch: 33/50, step: 35/83, loss: 2.74668, accuracy: 0.66429\n","Epoch: 33/50, step: 36/83, loss: 2.77055, accuracy: 0.66319\n","Epoch: 33/50, step: 37/83, loss: 2.76254, accuracy: 0.65878\n","Epoch: 33/50, step: 38/83, loss: 2.73060, accuracy: 0.66118\n","Epoch: 33/50, step: 39/83, loss: 2.72098, accuracy: 0.66667\n","Epoch: 33/50, step: 40/83, loss: 2.78828, accuracy: 0.65938\n","Epoch: 33/50, step: 41/83, loss: 2.78303, accuracy: 0.66159\n","Epoch: 33/50, step: 42/83, loss: 2.83922, accuracy: 0.65774\n","Epoch: 33/50, step: 43/83, loss: 2.87304, accuracy: 0.65407\n","Epoch: 33/50, step: 44/83, loss: 2.86319, accuracy: 0.65625\n","Epoch: 33/50, step: 45/83, loss: 2.89852, accuracy: 0.65556\n","Epoch: 33/50, step: 46/83, loss: 2.88601, accuracy: 0.65761\n","Epoch: 33/50, step: 47/83, loss: 2.89444, accuracy: 0.65957\n","Epoch: 33/50, step: 48/83, loss: 2.88032, accuracy: 0.66406\n","Epoch: 33/50, step: 49/83, loss: 2.89363, accuracy: 0.66071\n","Epoch: 33/50, step: 50/83, loss: 2.90144, accuracy: 0.65750\n","Epoch: 33/50, step: 51/83, loss: 2.91323, accuracy: 0.65441\n","Epoch: 33/50, step: 52/83, loss: 2.86753, accuracy: 0.65865\n","Epoch: 33/50, step: 53/83, loss: 2.86401, accuracy: 0.65566\n","Epoch: 33/50, step: 54/83, loss: 2.85480, accuracy: 0.65741\n","Epoch: 33/50, step: 55/83, loss: 2.84675, accuracy: 0.65682\n","Epoch: 33/50, step: 56/83, loss: 2.85531, accuracy: 0.65625\n","Epoch: 33/50, step: 57/83, loss: 2.83155, accuracy: 0.65789\n","Epoch: 33/50, step: 58/83, loss: 2.81405, accuracy: 0.65948\n","Epoch: 33/50, step: 59/83, loss: 2.80856, accuracy: 0.65678\n","Epoch: 33/50, step: 60/83, loss: 2.80109, accuracy: 0.66042\n","Epoch: 33/50, step: 61/83, loss: 2.81003, accuracy: 0.65984\n","Epoch: 33/50, step: 62/83, loss: 2.82049, accuracy: 0.65927\n","Epoch: 33/50, step: 63/83, loss: 2.82869, accuracy: 0.65873\n","Epoch: 33/50, step: 64/83, loss: 2.83987, accuracy: 0.65820\n","Epoch: 33/50, step: 65/83, loss: 2.80556, accuracy: 0.66154\n","Epoch: 33/50, step: 66/83, loss: 2.82849, accuracy: 0.65909\n","Epoch: 33/50, step: 67/83, loss: 2.80790, accuracy: 0.66231\n","Epoch: 33/50, step: 68/83, loss: 2.82968, accuracy: 0.66176\n","Epoch: 33/50, step: 69/83, loss: 2.86581, accuracy: 0.65580\n","Epoch: 33/50, step: 70/83, loss: 2.84747, accuracy: 0.65893\n","Epoch: 33/50, step: 71/83, loss: 2.81461, accuracy: 0.66373\n","Epoch: 33/50, step: 72/83, loss: 2.82587, accuracy: 0.66146\n","Epoch: 33/50, step: 73/83, loss: 2.82163, accuracy: 0.66096\n","Epoch: 33/50, step: 74/83, loss: 2.85092, accuracy: 0.66385\n","Epoch: 33/50, step: 75/83, loss: 2.83680, accuracy: 0.66500\n","Epoch: 33/50, step: 76/83, loss: 2.84443, accuracy: 0.66447\n","Epoch: 33/50, step: 77/83, loss: 2.82767, accuracy: 0.66721\n","Epoch: 33/50, step: 78/83, loss: 2.81133, accuracy: 0.66827\n","Epoch: 33/50, step: 79/83, loss: 2.80463, accuracy: 0.66772\n","Epoch: 33/50, step: 80/83, loss: 2.80389, accuracy: 0.66875\n","Epoch: 33/50, step: 81/83, loss: 2.80267, accuracy: 0.66821\n","Epoch: 33/50, step: 82/83, loss: 2.79104, accuracy: 0.66463\n","Epoch: 33/50, step: 83/83, loss: 2.86175, accuracy: 0.66362\n","Epoch: 33/50, train loss: 2.86175, train accuracy: 0.66362, valid loss: 10.26204, valid accuracy: 0.16742\n","Epoch: 34/50, step: 1/83, loss: 3.26715, accuracy: 0.62500\n","Epoch: 34/50, step: 2/83, loss: 3.28653, accuracy: 0.56250\n","Epoch: 34/50, step: 3/83, loss: 3.02798, accuracy: 0.62500\n","Epoch: 34/50, step: 4/83, loss: 2.44567, accuracy: 0.68750\n","Epoch: 34/50, step: 5/83, loss: 2.81203, accuracy: 0.65000\n","Epoch: 34/50, step: 6/83, loss: 3.33430, accuracy: 0.62500\n","Epoch: 34/50, step: 7/83, loss: 3.49824, accuracy: 0.60714\n","Epoch: 34/50, step: 8/83, loss: 3.60133, accuracy: 0.60938\n","Epoch: 34/50, step: 9/83, loss: 3.35690, accuracy: 0.63889\n","Epoch: 34/50, step: 10/83, loss: 3.30371, accuracy: 0.62500\n","Epoch: 34/50, step: 11/83, loss: 3.32514, accuracy: 0.60227\n","Epoch: 34/50, step: 12/83, loss: 3.27452, accuracy: 0.59375\n","Epoch: 34/50, step: 13/83, loss: 3.24067, accuracy: 0.57692\n","Epoch: 34/50, step: 14/83, loss: 3.31259, accuracy: 0.58036\n","Epoch: 34/50, step: 15/83, loss: 3.24349, accuracy: 0.59167\n","Epoch: 34/50, step: 16/83, loss: 3.08469, accuracy: 0.60938\n","Epoch: 34/50, step: 17/83, loss: 3.00465, accuracy: 0.61029\n","Epoch: 34/50, step: 18/83, loss: 2.92734, accuracy: 0.62500\n","Epoch: 34/50, step: 19/83, loss: 2.89976, accuracy: 0.63816\n","Epoch: 34/50, step: 20/83, loss: 2.95298, accuracy: 0.64375\n","Epoch: 34/50, step: 21/83, loss: 2.88222, accuracy: 0.65476\n","Epoch: 34/50, step: 22/83, loss: 2.94541, accuracy: 0.65341\n","Epoch: 34/50, step: 23/83, loss: 2.88459, accuracy: 0.66304\n","Epoch: 34/50, step: 24/83, loss: 2.82392, accuracy: 0.67188\n","Epoch: 34/50, step: 25/83, loss: 2.77177, accuracy: 0.67000\n","Epoch: 34/50, step: 26/83, loss: 2.68921, accuracy: 0.68269\n","Epoch: 34/50, step: 27/83, loss: 2.76111, accuracy: 0.67593\n","Epoch: 34/50, step: 28/83, loss: 2.75875, accuracy: 0.67411\n","Epoch: 34/50, step: 29/83, loss: 2.75174, accuracy: 0.66379\n","Epoch: 34/50, step: 30/83, loss: 2.77431, accuracy: 0.65833\n","Epoch: 34/50, step: 31/83, loss: 2.77461, accuracy: 0.65726\n","Epoch: 34/50, step: 32/83, loss: 2.76110, accuracy: 0.66016\n","Epoch: 34/50, step: 33/83, loss: 2.72485, accuracy: 0.66667\n","Epoch: 34/50, step: 34/83, loss: 2.82050, accuracy: 0.66176\n","Epoch: 34/50, step: 35/83, loss: 2.82362, accuracy: 0.66429\n","Epoch: 34/50, step: 36/83, loss: 2.88799, accuracy: 0.65625\n","Epoch: 34/50, step: 37/83, loss: 2.87641, accuracy: 0.65878\n","Epoch: 34/50, step: 38/83, loss: 2.84225, accuracy: 0.66118\n","Epoch: 34/50, step: 39/83, loss: 2.82732, accuracy: 0.66346\n","Epoch: 34/50, step: 40/83, loss: 2.82600, accuracy: 0.66562\n","Epoch: 34/50, step: 41/83, loss: 2.82091, accuracy: 0.66463\n","Epoch: 34/50, step: 42/83, loss: 2.83235, accuracy: 0.66369\n","Epoch: 34/50, step: 43/83, loss: 2.80114, accuracy: 0.66860\n","Epoch: 34/50, step: 44/83, loss: 2.85144, accuracy: 0.66477\n","Epoch: 34/50, step: 45/83, loss: 2.88657, accuracy: 0.65833\n","Epoch: 34/50, step: 46/83, loss: 2.88655, accuracy: 0.65489\n","Epoch: 34/50, step: 47/83, loss: 2.89534, accuracy: 0.65160\n","Epoch: 34/50, step: 48/83, loss: 2.94237, accuracy: 0.64844\n","Epoch: 34/50, step: 49/83, loss: 2.99681, accuracy: 0.64031\n","Epoch: 34/50, step: 50/83, loss: 3.02928, accuracy: 0.63500\n","Epoch: 34/50, step: 51/83, loss: 3.04000, accuracy: 0.63235\n","Epoch: 34/50, step: 52/83, loss: 3.04942, accuracy: 0.62740\n","Epoch: 34/50, step: 53/83, loss: 3.02070, accuracy: 0.62972\n","Epoch: 34/50, step: 54/83, loss: 2.98158, accuracy: 0.63194\n","Epoch: 34/50, step: 55/83, loss: 2.97303, accuracy: 0.63182\n","Epoch: 34/50, step: 56/83, loss: 2.99667, accuracy: 0.63170\n","Epoch: 34/50, step: 57/83, loss: 3.01919, accuracy: 0.63158\n","Epoch: 34/50, step: 58/83, loss: 3.03769, accuracy: 0.63578\n","Epoch: 34/50, step: 59/83, loss: 3.04060, accuracy: 0.63771\n","Epoch: 34/50, step: 60/83, loss: 3.05266, accuracy: 0.63125\n","Epoch: 34/50, step: 61/83, loss: 3.04236, accuracy: 0.62910\n","Epoch: 34/50, step: 62/83, loss: 3.03057, accuracy: 0.62903\n","Epoch: 34/50, step: 63/83, loss: 3.00706, accuracy: 0.63095\n","Epoch: 34/50, step: 64/83, loss: 2.99814, accuracy: 0.63477\n","Epoch: 34/50, step: 65/83, loss: 3.00598, accuracy: 0.63269\n","Epoch: 34/50, step: 66/83, loss: 2.99762, accuracy: 0.63258\n","Epoch: 34/50, step: 67/83, loss: 2.99105, accuracy: 0.63060\n","Epoch: 34/50, step: 68/83, loss: 2.98421, accuracy: 0.63051\n","Epoch: 34/50, step: 69/83, loss: 2.97891, accuracy: 0.62862\n","Epoch: 34/50, step: 70/83, loss: 2.99489, accuracy: 0.62857\n","Epoch: 34/50, step: 71/83, loss: 2.98496, accuracy: 0.63028\n","Epoch: 34/50, step: 72/83, loss: 2.96473, accuracy: 0.63368\n","Epoch: 34/50, step: 73/83, loss: 2.98484, accuracy: 0.63185\n","Epoch: 34/50, step: 74/83, loss: 2.99187, accuracy: 0.63007\n","Epoch: 34/50, step: 75/83, loss: 2.99789, accuracy: 0.63000\n","Epoch: 34/50, step: 76/83, loss: 3.00152, accuracy: 0.62993\n","Epoch: 34/50, step: 77/83, loss: 2.97199, accuracy: 0.63312\n","Epoch: 34/50, step: 78/83, loss: 2.96676, accuracy: 0.63462\n","Epoch: 34/50, step: 79/83, loss: 2.96313, accuracy: 0.63291\n","Epoch: 34/50, step: 80/83, loss: 2.95771, accuracy: 0.63281\n","Epoch: 34/50, step: 81/83, loss: 2.93884, accuracy: 0.63426\n","Epoch: 34/50, step: 82/83, loss: 2.93200, accuracy: 0.63720\n","Epoch: 34/50, step: 83/83, loss: 2.98846, accuracy: 0.63775\n","Epoch: 34/50, train loss: 2.98846, train accuracy: 0.63775, valid loss: 10.75458, valid accuracy: 0.16290\n","Epoch: 35/50, step: 1/83, loss: 2.42423, accuracy: 0.37500\n","Epoch: 35/50, step: 2/83, loss: 3.94249, accuracy: 0.31250\n","Epoch: 35/50, step: 3/83, loss: 3.83001, accuracy: 0.29167\n","Epoch: 35/50, step: 4/83, loss: 3.27679, accuracy: 0.43750\n","Epoch: 35/50, step: 5/83, loss: 3.19331, accuracy: 0.45000\n","Epoch: 35/50, step: 6/83, loss: 2.75523, accuracy: 0.50000\n","Epoch: 35/50, step: 7/83, loss: 3.33926, accuracy: 0.53571\n","Epoch: 35/50, step: 8/83, loss: 3.45530, accuracy: 0.53125\n","Epoch: 35/50, step: 9/83, loss: 3.13164, accuracy: 0.56944\n","Epoch: 35/50, step: 10/83, loss: 2.97062, accuracy: 0.60000\n","Epoch: 35/50, step: 11/83, loss: 3.08311, accuracy: 0.60227\n","Epoch: 35/50, step: 12/83, loss: 2.97705, accuracy: 0.61458\n","Epoch: 35/50, step: 13/83, loss: 3.01525, accuracy: 0.60577\n","Epoch: 35/50, step: 14/83, loss: 2.96749, accuracy: 0.60714\n","Epoch: 35/50, step: 15/83, loss: 2.94395, accuracy: 0.60000\n","Epoch: 35/50, step: 16/83, loss: 2.91178, accuracy: 0.60938\n","Epoch: 35/50, step: 17/83, loss: 2.88918, accuracy: 0.61765\n","Epoch: 35/50, step: 18/83, loss: 2.77152, accuracy: 0.62500\n","Epoch: 35/50, step: 19/83, loss: 2.85525, accuracy: 0.62500\n","Epoch: 35/50, step: 20/83, loss: 2.73887, accuracy: 0.63750\n","Epoch: 35/50, step: 21/83, loss: 2.76577, accuracy: 0.64286\n","Epoch: 35/50, step: 22/83, loss: 2.66617, accuracy: 0.64205\n","Epoch: 35/50, step: 23/83, loss: 2.73120, accuracy: 0.63587\n","Epoch: 35/50, step: 24/83, loss: 2.71653, accuracy: 0.64583\n","Epoch: 35/50, step: 25/83, loss: 2.75238, accuracy: 0.63000\n","Epoch: 35/50, step: 26/83, loss: 2.90742, accuracy: 0.61538\n","Epoch: 35/50, step: 27/83, loss: 2.92437, accuracy: 0.62037\n","Epoch: 35/50, step: 28/83, loss: 2.96810, accuracy: 0.62500\n","Epoch: 35/50, step: 29/83, loss: 3.05142, accuracy: 0.61207\n","Epoch: 35/50, step: 30/83, loss: 2.98832, accuracy: 0.61250\n","Epoch: 35/50, step: 31/83, loss: 2.94598, accuracy: 0.61694\n","Epoch: 35/50, step: 32/83, loss: 2.95818, accuracy: 0.62109\n","Epoch: 35/50, step: 33/83, loss: 2.88402, accuracy: 0.63258\n","Epoch: 35/50, step: 34/83, loss: 2.86467, accuracy: 0.63235\n","Epoch: 35/50, step: 35/83, loss: 2.88221, accuracy: 0.62857\n","Epoch: 35/50, step: 36/83, loss: 2.90015, accuracy: 0.62153\n","Epoch: 35/50, step: 37/83, loss: 2.88864, accuracy: 0.62838\n","Epoch: 35/50, step: 38/83, loss: 2.92841, accuracy: 0.62500\n","Epoch: 35/50, step: 39/83, loss: 2.94062, accuracy: 0.62821\n","Epoch: 35/50, step: 40/83, loss: 2.95129, accuracy: 0.63125\n","Epoch: 35/50, step: 41/83, loss: 2.90958, accuracy: 0.64024\n","Epoch: 35/50, step: 42/83, loss: 2.93324, accuracy: 0.63988\n","Epoch: 35/50, step: 43/83, loss: 2.92431, accuracy: 0.63372\n","Epoch: 35/50, step: 44/83, loss: 2.89838, accuracy: 0.63636\n","Epoch: 35/50, step: 45/83, loss: 2.84693, accuracy: 0.63889\n","Epoch: 35/50, step: 46/83, loss: 2.87952, accuracy: 0.63315\n","Epoch: 35/50, step: 47/83, loss: 2.88999, accuracy: 0.63032\n","Epoch: 35/50, step: 48/83, loss: 2.93113, accuracy: 0.63021\n","Epoch: 35/50, step: 49/83, loss: 2.90377, accuracy: 0.63265\n","Epoch: 35/50, step: 50/83, loss: 2.87874, accuracy: 0.63000\n","Epoch: 35/50, step: 51/83, loss: 2.90641, accuracy: 0.62745\n","Epoch: 35/50, step: 52/83, loss: 2.94097, accuracy: 0.62500\n","Epoch: 35/50, step: 53/83, loss: 2.91485, accuracy: 0.62500\n","Epoch: 35/50, step: 54/83, loss: 2.95769, accuracy: 0.62269\n","Epoch: 35/50, step: 55/83, loss: 3.01116, accuracy: 0.61818\n","Epoch: 35/50, step: 56/83, loss: 3.00477, accuracy: 0.62277\n","Epoch: 35/50, step: 57/83, loss: 3.02939, accuracy: 0.62061\n","Epoch: 35/50, step: 58/83, loss: 3.02360, accuracy: 0.61853\n","Epoch: 35/50, step: 59/83, loss: 3.01745, accuracy: 0.61653\n","Epoch: 35/50, step: 60/83, loss: 3.00905, accuracy: 0.61875\n","Epoch: 35/50, step: 61/83, loss: 2.98698, accuracy: 0.62090\n","Epoch: 35/50, step: 62/83, loss: 2.96254, accuracy: 0.62298\n","Epoch: 35/50, step: 63/83, loss: 2.97049, accuracy: 0.61706\n","Epoch: 35/50, step: 64/83, loss: 2.94678, accuracy: 0.62109\n","Epoch: 35/50, step: 65/83, loss: 2.92552, accuracy: 0.62500\n","Epoch: 35/50, step: 66/83, loss: 2.94310, accuracy: 0.62500\n","Epoch: 35/50, step: 67/83, loss: 2.93723, accuracy: 0.62687\n","Epoch: 35/50, step: 68/83, loss: 2.90206, accuracy: 0.63051\n","Epoch: 35/50, step: 69/83, loss: 2.89627, accuracy: 0.63043\n","Epoch: 35/50, step: 70/83, loss: 2.93944, accuracy: 0.62857\n","Epoch: 35/50, step: 71/83, loss: 2.93066, accuracy: 0.62852\n","Epoch: 35/50, step: 72/83, loss: 2.91038, accuracy: 0.63021\n","Epoch: 35/50, step: 73/83, loss: 2.88225, accuracy: 0.63356\n","Epoch: 35/50, step: 74/83, loss: 2.87794, accuracy: 0.63514\n","Epoch: 35/50, step: 75/83, loss: 2.88597, accuracy: 0.63333\n","Epoch: 35/50, step: 76/83, loss: 2.89213, accuracy: 0.62993\n","Epoch: 35/50, step: 77/83, loss: 2.87347, accuracy: 0.63312\n","Epoch: 35/50, step: 78/83, loss: 2.87761, accuracy: 0.63462\n","Epoch: 35/50, step: 79/83, loss: 2.87395, accuracy: 0.63449\n","Epoch: 35/50, step: 80/83, loss: 2.87854, accuracy: 0.63437\n","Epoch: 35/50, step: 81/83, loss: 2.87334, accuracy: 0.63580\n","Epoch: 35/50, step: 82/83, loss: 2.85548, accuracy: 0.63872\n","Epoch: 35/50, step: 83/83, loss: 2.91847, accuracy: 0.63775\n","Epoch: 35/50, train loss: 2.91847, train accuracy: 0.63775, valid loss: 10.45210, valid accuracy: 0.16290\n","Epoch: 36/50, step: 1/83, loss: 2.46260, accuracy: 0.75000\n","Epoch: 36/50, step: 2/83, loss: 1.53297, accuracy: 0.68750\n","Epoch: 36/50, step: 3/83, loss: 2.19108, accuracy: 0.66667\n","Epoch: 36/50, step: 4/83, loss: 2.22224, accuracy: 0.68750\n","Epoch: 36/50, step: 5/83, loss: 2.08714, accuracy: 0.72500\n","Epoch: 36/50, step: 6/83, loss: 2.18081, accuracy: 0.72917\n","Epoch: 36/50, step: 7/83, loss: 2.50820, accuracy: 0.69643\n","Epoch: 36/50, step: 8/83, loss: 2.37604, accuracy: 0.70312\n","Epoch: 36/50, step: 9/83, loss: 2.39505, accuracy: 0.69444\n","Epoch: 36/50, step: 10/83, loss: 2.39693, accuracy: 0.68750\n","Epoch: 36/50, step: 11/83, loss: 2.38613, accuracy: 0.69318\n","Epoch: 36/50, step: 12/83, loss: 2.23186, accuracy: 0.71875\n","Epoch: 36/50, step: 13/83, loss: 2.16967, accuracy: 0.72115\n","Epoch: 36/50, step: 14/83, loss: 2.19677, accuracy: 0.72321\n","Epoch: 36/50, step: 15/83, loss: 2.21656, accuracy: 0.70833\n","Epoch: 36/50, step: 16/83, loss: 2.36779, accuracy: 0.68750\n","Epoch: 36/50, step: 17/83, loss: 2.42825, accuracy: 0.68382\n","Epoch: 36/50, step: 18/83, loss: 2.33159, accuracy: 0.69444\n","Epoch: 36/50, step: 19/83, loss: 2.24285, accuracy: 0.71053\n","Epoch: 36/50, step: 20/83, loss: 2.30610, accuracy: 0.70000\n","Epoch: 36/50, step: 21/83, loss: 2.30848, accuracy: 0.70238\n","Epoch: 36/50, step: 22/83, loss: 2.34834, accuracy: 0.69318\n","Epoch: 36/50, step: 23/83, loss: 2.45276, accuracy: 0.66848\n","Epoch: 36/50, step: 24/83, loss: 2.56373, accuracy: 0.66146\n","Epoch: 36/50, step: 25/83, loss: 2.48191, accuracy: 0.67000\n","Epoch: 36/50, step: 26/83, loss: 2.44841, accuracy: 0.67308\n","Epoch: 36/50, step: 27/83, loss: 2.41655, accuracy: 0.67593\n","Epoch: 36/50, step: 28/83, loss: 2.35638, accuracy: 0.67411\n","Epoch: 36/50, step: 29/83, loss: 2.32484, accuracy: 0.67672\n","Epoch: 36/50, step: 30/83, loss: 2.32642, accuracy: 0.68333\n","Epoch: 36/50, step: 31/83, loss: 2.31057, accuracy: 0.67339\n","Epoch: 36/50, step: 32/83, loss: 2.28826, accuracy: 0.67969\n","Epoch: 36/50, step: 33/83, loss: 2.26686, accuracy: 0.68182\n","Epoch: 36/50, step: 34/83, loss: 2.36383, accuracy: 0.66912\n","Epoch: 36/50, step: 35/83, loss: 2.34325, accuracy: 0.67143\n","Epoch: 36/50, step: 36/83, loss: 2.40389, accuracy: 0.66667\n","Epoch: 36/50, step: 37/83, loss: 2.45586, accuracy: 0.65878\n","Epoch: 36/50, step: 38/83, loss: 2.46109, accuracy: 0.65461\n","Epoch: 36/50, step: 39/83, loss: 2.52856, accuracy: 0.64744\n","Epoch: 36/50, step: 40/83, loss: 2.50042, accuracy: 0.65000\n","Epoch: 36/50, step: 41/83, loss: 2.50384, accuracy: 0.64939\n","Epoch: 36/50, step: 42/83, loss: 2.53981, accuracy: 0.64583\n","Epoch: 36/50, step: 43/83, loss: 2.53407, accuracy: 0.65116\n","Epoch: 36/50, step: 44/83, loss: 2.57422, accuracy: 0.65057\n","Epoch: 36/50, step: 45/83, loss: 2.58694, accuracy: 0.65278\n","Epoch: 36/50, step: 46/83, loss: 2.56249, accuracy: 0.65489\n","Epoch: 36/50, step: 47/83, loss: 2.58182, accuracy: 0.65426\n","Epoch: 36/50, step: 48/83, loss: 2.56448, accuracy: 0.65365\n","Epoch: 36/50, step: 49/83, loss: 2.52320, accuracy: 0.66071\n","Epoch: 36/50, step: 50/83, loss: 2.55487, accuracy: 0.66000\n","Epoch: 36/50, step: 51/83, loss: 2.55288, accuracy: 0.66176\n","Epoch: 36/50, step: 52/83, loss: 2.58548, accuracy: 0.66106\n","Epoch: 36/50, step: 53/83, loss: 2.60909, accuracy: 0.66038\n","Epoch: 36/50, step: 54/83, loss: 2.63378, accuracy: 0.65741\n","Epoch: 36/50, step: 55/83, loss: 2.62572, accuracy: 0.65909\n","Epoch: 36/50, step: 56/83, loss: 2.63630, accuracy: 0.65848\n","Epoch: 36/50, step: 57/83, loss: 2.66146, accuracy: 0.65351\n","Epoch: 36/50, step: 58/83, loss: 2.65617, accuracy: 0.65517\n","Epoch: 36/50, step: 59/83, loss: 2.67351, accuracy: 0.65466\n","Epoch: 36/50, step: 60/83, loss: 2.69928, accuracy: 0.65625\n","Epoch: 36/50, step: 61/83, loss: 2.70924, accuracy: 0.65574\n","Epoch: 36/50, step: 62/83, loss: 2.70295, accuracy: 0.65927\n","Epoch: 36/50, step: 63/83, loss: 2.71669, accuracy: 0.65675\n","Epoch: 36/50, step: 64/83, loss: 2.71277, accuracy: 0.65820\n","Epoch: 36/50, step: 65/83, loss: 2.70667, accuracy: 0.65962\n","Epoch: 36/50, step: 66/83, loss: 2.68911, accuracy: 0.66288\n","Epoch: 36/50, step: 67/83, loss: 2.72288, accuracy: 0.66045\n","Epoch: 36/50, step: 68/83, loss: 2.70436, accuracy: 0.66360\n","Epoch: 36/50, step: 69/83, loss: 2.70290, accuracy: 0.66486\n","Epoch: 36/50, step: 70/83, loss: 2.68471, accuracy: 0.66786\n","Epoch: 36/50, step: 71/83, loss: 2.70812, accuracy: 0.66549\n","Epoch: 36/50, step: 72/83, loss: 2.72074, accuracy: 0.66319\n","Epoch: 36/50, step: 73/83, loss: 2.71689, accuracy: 0.66438\n","Epoch: 36/50, step: 74/83, loss: 2.73610, accuracy: 0.66216\n","Epoch: 36/50, step: 75/83, loss: 2.73460, accuracy: 0.66000\n","Epoch: 36/50, step: 76/83, loss: 2.72973, accuracy: 0.65954\n","Epoch: 36/50, step: 77/83, loss: 2.70237, accuracy: 0.66234\n","Epoch: 36/50, step: 78/83, loss: 2.72450, accuracy: 0.65865\n","Epoch: 36/50, step: 79/83, loss: 2.74562, accuracy: 0.65665\n","Epoch: 36/50, step: 80/83, loss: 2.73015, accuracy: 0.65938\n","Epoch: 36/50, step: 81/83, loss: 2.72348, accuracy: 0.66204\n","Epoch: 36/50, step: 82/83, loss: 2.72229, accuracy: 0.66159\n","Epoch: 36/50, step: 83/83, loss: 2.78540, accuracy: 0.66058\n","Epoch: 36/50, train loss: 2.78540, train accuracy: 0.66058, valid loss: 10.45571, valid accuracy: 0.17647\n","Epoch: 37/50, step: 1/83, loss: 5.33786, accuracy: 0.37500\n","Epoch: 37/50, step: 2/83, loss: 2.91681, accuracy: 0.68750\n","Epoch: 37/50, step: 3/83, loss: 3.37463, accuracy: 0.66667\n","Epoch: 37/50, step: 4/83, loss: 3.40835, accuracy: 0.59375\n","Epoch: 37/50, step: 5/83, loss: 3.20120, accuracy: 0.62500\n","Epoch: 37/50, step: 6/83, loss: 3.21921, accuracy: 0.64583\n","Epoch: 37/50, step: 7/83, loss: 3.12687, accuracy: 0.64286\n","Epoch: 37/50, step: 8/83, loss: 3.05828, accuracy: 0.65625\n","Epoch: 37/50, step: 9/83, loss: 2.88951, accuracy: 0.68056\n","Epoch: 37/50, step: 10/83, loss: 2.85533, accuracy: 0.67500\n","Epoch: 37/50, step: 11/83, loss: 2.73140, accuracy: 0.69318\n","Epoch: 37/50, step: 12/83, loss: 2.59657, accuracy: 0.67708\n","Epoch: 37/50, step: 13/83, loss: 2.67967, accuracy: 0.66346\n","Epoch: 37/50, step: 14/83, loss: 2.53583, accuracy: 0.66071\n","Epoch: 37/50, step: 15/83, loss: 2.40145, accuracy: 0.67500\n","Epoch: 37/50, step: 16/83, loss: 2.39392, accuracy: 0.68750\n","Epoch: 37/50, step: 17/83, loss: 2.28489, accuracy: 0.70588\n","Epoch: 37/50, step: 18/83, loss: 2.24259, accuracy: 0.70139\n","Epoch: 37/50, step: 19/83, loss: 2.24777, accuracy: 0.70395\n","Epoch: 37/50, step: 20/83, loss: 2.35704, accuracy: 0.69375\n","Epoch: 37/50, step: 21/83, loss: 2.41408, accuracy: 0.68452\n","Epoch: 37/50, step: 22/83, loss: 2.50504, accuracy: 0.67614\n","Epoch: 37/50, step: 23/83, loss: 2.56359, accuracy: 0.68478\n","Epoch: 37/50, step: 24/83, loss: 2.67785, accuracy: 0.66667\n","Epoch: 37/50, step: 25/83, loss: 2.77562, accuracy: 0.65500\n","Epoch: 37/50, step: 26/83, loss: 2.84995, accuracy: 0.63942\n","Epoch: 37/50, step: 27/83, loss: 2.94275, accuracy: 0.62963\n","Epoch: 37/50, step: 28/83, loss: 3.01454, accuracy: 0.62946\n","Epoch: 37/50, step: 29/83, loss: 2.94484, accuracy: 0.63362\n","Epoch: 37/50, step: 30/83, loss: 2.91637, accuracy: 0.63750\n","Epoch: 37/50, step: 31/83, loss: 2.90041, accuracy: 0.64113\n","Epoch: 37/50, step: 32/83, loss: 2.94737, accuracy: 0.63672\n","Epoch: 37/50, step: 33/83, loss: 2.98665, accuracy: 0.64394\n","Epoch: 37/50, step: 34/83, loss: 3.08501, accuracy: 0.62868\n","Epoch: 37/50, step: 35/83, loss: 3.09119, accuracy: 0.62500\n","Epoch: 37/50, step: 36/83, loss: 3.07635, accuracy: 0.62153\n","Epoch: 37/50, step: 37/83, loss: 3.11525, accuracy: 0.61486\n","Epoch: 37/50, step: 38/83, loss: 3.10035, accuracy: 0.61513\n","Epoch: 37/50, step: 39/83, loss: 3.13317, accuracy: 0.61538\n","Epoch: 37/50, step: 40/83, loss: 3.14583, accuracy: 0.61250\n","Epoch: 37/50, step: 41/83, loss: 3.12878, accuracy: 0.61280\n","Epoch: 37/50, step: 42/83, loss: 3.16370, accuracy: 0.60714\n","Epoch: 37/50, step: 43/83, loss: 3.15837, accuracy: 0.60174\n","Epoch: 37/50, step: 44/83, loss: 3.16242, accuracy: 0.60227\n","Epoch: 37/50, step: 45/83, loss: 3.18979, accuracy: 0.60000\n","Epoch: 37/50, step: 46/83, loss: 3.13208, accuracy: 0.60870\n","Epoch: 37/50, step: 47/83, loss: 3.08041, accuracy: 0.61436\n","Epoch: 37/50, step: 48/83, loss: 3.08758, accuracy: 0.61458\n","Epoch: 37/50, step: 49/83, loss: 3.09275, accuracy: 0.61224\n","Epoch: 37/50, step: 50/83, loss: 3.10371, accuracy: 0.61000\n","Epoch: 37/50, step: 51/83, loss: 3.08758, accuracy: 0.61029\n","Epoch: 37/50, step: 52/83, loss: 3.13818, accuracy: 0.60817\n","Epoch: 37/50, step: 53/83, loss: 3.10814, accuracy: 0.61085\n","Epoch: 37/50, step: 54/83, loss: 3.11320, accuracy: 0.61111\n","Epoch: 37/50, step: 55/83, loss: 3.09916, accuracy: 0.61136\n","Epoch: 37/50, step: 56/83, loss: 3.11421, accuracy: 0.61161\n","Epoch: 37/50, step: 57/83, loss: 3.11654, accuracy: 0.61623\n","Epoch: 37/50, step: 58/83, loss: 3.10523, accuracy: 0.61638\n","Epoch: 37/50, step: 59/83, loss: 3.06431, accuracy: 0.61864\n","Epoch: 37/50, step: 60/83, loss: 3.08097, accuracy: 0.61667\n","Epoch: 37/50, step: 61/83, loss: 3.06141, accuracy: 0.61475\n","Epoch: 37/50, step: 62/83, loss: 3.07742, accuracy: 0.61492\n","Epoch: 37/50, step: 63/83, loss: 3.08313, accuracy: 0.61508\n","Epoch: 37/50, step: 64/83, loss: 3.05954, accuracy: 0.61719\n","Epoch: 37/50, step: 65/83, loss: 3.06614, accuracy: 0.61731\n","Epoch: 37/50, step: 66/83, loss: 3.07081, accuracy: 0.61742\n","Epoch: 37/50, step: 67/83, loss: 3.04861, accuracy: 0.62127\n","Epoch: 37/50, step: 68/83, loss: 3.01242, accuracy: 0.62500\n","Epoch: 37/50, step: 69/83, loss: 3.01750, accuracy: 0.62681\n","Epoch: 37/50, step: 70/83, loss: 2.98321, accuracy: 0.62857\n","Epoch: 37/50, step: 71/83, loss: 2.99751, accuracy: 0.62852\n","Epoch: 37/50, step: 72/83, loss: 2.99217, accuracy: 0.62847\n","Epoch: 37/50, step: 73/83, loss: 3.01082, accuracy: 0.62671\n","Epoch: 37/50, step: 74/83, loss: 3.00511, accuracy: 0.62500\n","Epoch: 37/50, step: 75/83, loss: 2.98779, accuracy: 0.62833\n","Epoch: 37/50, step: 76/83, loss: 2.96886, accuracy: 0.63158\n","Epoch: 37/50, step: 77/83, loss: 2.98311, accuracy: 0.63149\n","Epoch: 37/50, step: 78/83, loss: 2.96247, accuracy: 0.63462\n","Epoch: 37/50, step: 79/83, loss: 2.96303, accuracy: 0.63766\n","Epoch: 37/50, step: 80/83, loss: 2.97925, accuracy: 0.63750\n","Epoch: 37/50, step: 81/83, loss: 2.95243, accuracy: 0.64043\n","Epoch: 37/50, step: 82/83, loss: 2.95935, accuracy: 0.63872\n","Epoch: 37/50, step: 83/83, loss: 2.93075, accuracy: 0.63927\n","Epoch: 37/50, train loss: 2.93075, train accuracy: 0.63927, valid loss: 10.40112, valid accuracy: 0.15385\n","Epoch: 38/50, step: 1/83, loss: 2.54945, accuracy: 0.75000\n","Epoch: 38/50, step: 2/83, loss: 3.04852, accuracy: 0.62500\n","Epoch: 38/50, step: 3/83, loss: 3.15859, accuracy: 0.58333\n","Epoch: 38/50, step: 4/83, loss: 2.52484, accuracy: 0.65625\n","Epoch: 38/50, step: 5/83, loss: 2.28646, accuracy: 0.67500\n","Epoch: 38/50, step: 6/83, loss: 2.16726, accuracy: 0.66667\n","Epoch: 38/50, step: 7/83, loss: 1.93990, accuracy: 0.67857\n","Epoch: 38/50, step: 8/83, loss: 2.03306, accuracy: 0.67188\n","Epoch: 38/50, step: 9/83, loss: 2.13158, accuracy: 0.65278\n","Epoch: 38/50, step: 10/83, loss: 1.97985, accuracy: 0.67500\n","Epoch: 38/50, step: 11/83, loss: 1.93872, accuracy: 0.69318\n","Epoch: 38/50, step: 12/83, loss: 1.90952, accuracy: 0.69792\n","Epoch: 38/50, step: 13/83, loss: 2.07780, accuracy: 0.69231\n","Epoch: 38/50, step: 14/83, loss: 2.05848, accuracy: 0.67857\n","Epoch: 38/50, step: 15/83, loss: 1.96322, accuracy: 0.68333\n","Epoch: 38/50, step: 16/83, loss: 2.10299, accuracy: 0.65625\n","Epoch: 38/50, step: 17/83, loss: 2.14432, accuracy: 0.63971\n","Epoch: 38/50, step: 18/83, loss: 2.16989, accuracy: 0.63194\n","Epoch: 38/50, step: 19/83, loss: 2.13789, accuracy: 0.64474\n","Epoch: 38/50, step: 20/83, loss: 2.14364, accuracy: 0.65000\n","Epoch: 38/50, step: 21/83, loss: 2.19877, accuracy: 0.64881\n","Epoch: 38/50, step: 22/83, loss: 2.27638, accuracy: 0.64205\n","Epoch: 38/50, step: 23/83, loss: 2.32236, accuracy: 0.63587\n","Epoch: 38/50, step: 24/83, loss: 2.40613, accuracy: 0.63021\n","Epoch: 38/50, step: 25/83, loss: 2.40931, accuracy: 0.63000\n","Epoch: 38/50, step: 26/83, loss: 2.41494, accuracy: 0.62981\n","Epoch: 38/50, step: 27/83, loss: 2.42065, accuracy: 0.62963\n","Epoch: 38/50, step: 28/83, loss: 2.53480, accuracy: 0.62054\n","Epoch: 38/50, step: 29/83, loss: 2.53152, accuracy: 0.62069\n","Epoch: 38/50, step: 30/83, loss: 2.56400, accuracy: 0.61667\n","Epoch: 38/50, step: 31/83, loss: 2.64945, accuracy: 0.60887\n","Epoch: 38/50, step: 32/83, loss: 2.67861, accuracy: 0.60547\n","Epoch: 38/50, step: 33/83, loss: 2.69180, accuracy: 0.59470\n","Epoch: 38/50, step: 34/83, loss: 2.65597, accuracy: 0.60294\n","Epoch: 38/50, step: 35/83, loss: 2.62409, accuracy: 0.61071\n","Epoch: 38/50, step: 36/83, loss: 2.62402, accuracy: 0.60764\n","Epoch: 38/50, step: 37/83, loss: 2.64190, accuracy: 0.60811\n","Epoch: 38/50, step: 38/83, loss: 2.66160, accuracy: 0.60855\n","Epoch: 38/50, step: 39/83, loss: 2.63357, accuracy: 0.61218\n","Epoch: 38/50, step: 40/83, loss: 2.65711, accuracy: 0.60938\n","Epoch: 38/50, step: 41/83, loss: 2.62933, accuracy: 0.60976\n","Epoch: 38/50, step: 42/83, loss: 2.64983, accuracy: 0.61012\n","Epoch: 38/50, step: 43/83, loss: 2.66841, accuracy: 0.60756\n","Epoch: 38/50, step: 44/83, loss: 2.68417, accuracy: 0.60795\n","Epoch: 38/50, step: 45/83, loss: 2.72383, accuracy: 0.60278\n","Epoch: 38/50, step: 46/83, loss: 2.75997, accuracy: 0.60326\n","Epoch: 38/50, step: 47/83, loss: 2.73340, accuracy: 0.60904\n","Epoch: 38/50, step: 48/83, loss: 2.72796, accuracy: 0.61198\n","Epoch: 38/50, step: 49/83, loss: 2.71322, accuracy: 0.61735\n","Epoch: 38/50, step: 50/83, loss: 2.70979, accuracy: 0.62000\n","Epoch: 38/50, step: 51/83, loss: 2.75706, accuracy: 0.61765\n","Epoch: 38/50, step: 52/83, loss: 2.80062, accuracy: 0.61538\n","Epoch: 38/50, step: 53/83, loss: 2.77665, accuracy: 0.62028\n","Epoch: 38/50, step: 54/83, loss: 2.81923, accuracy: 0.62037\n","Epoch: 38/50, step: 55/83, loss: 2.83171, accuracy: 0.62045\n","Epoch: 38/50, step: 56/83, loss: 2.80773, accuracy: 0.62277\n","Epoch: 38/50, step: 57/83, loss: 2.78522, accuracy: 0.62500\n","Epoch: 38/50, step: 58/83, loss: 2.78281, accuracy: 0.62500\n","Epoch: 38/50, step: 59/83, loss: 2.80907, accuracy: 0.62500\n","Epoch: 38/50, step: 60/83, loss: 2.81502, accuracy: 0.62292\n","Epoch: 38/50, step: 61/83, loss: 2.79430, accuracy: 0.62295\n","Epoch: 38/50, step: 62/83, loss: 2.77338, accuracy: 0.62702\n","Epoch: 38/50, step: 63/83, loss: 2.75366, accuracy: 0.63095\n","Epoch: 38/50, step: 64/83, loss: 2.76601, accuracy: 0.62891\n","Epoch: 38/50, step: 65/83, loss: 2.73304, accuracy: 0.63077\n","Epoch: 38/50, step: 66/83, loss: 2.74442, accuracy: 0.62879\n","Epoch: 38/50, step: 67/83, loss: 2.72870, accuracy: 0.62687\n","Epoch: 38/50, step: 68/83, loss: 2.73917, accuracy: 0.62500\n","Epoch: 38/50, step: 69/83, loss: 2.72370, accuracy: 0.62681\n","Epoch: 38/50, step: 70/83, loss: 2.74623, accuracy: 0.62500\n","Epoch: 38/50, step: 71/83, loss: 2.74366, accuracy: 0.62324\n","Epoch: 38/50, step: 72/83, loss: 2.74173, accuracy: 0.62500\n","Epoch: 38/50, step: 73/83, loss: 2.77452, accuracy: 0.61986\n","Epoch: 38/50, step: 74/83, loss: 2.78342, accuracy: 0.61824\n","Epoch: 38/50, step: 75/83, loss: 2.78012, accuracy: 0.61833\n","Epoch: 38/50, step: 76/83, loss: 2.79933, accuracy: 0.61678\n","Epoch: 38/50, step: 77/83, loss: 2.79516, accuracy: 0.61688\n","Epoch: 38/50, step: 78/83, loss: 2.79410, accuracy: 0.61538\n","Epoch: 38/50, step: 79/83, loss: 2.77904, accuracy: 0.61709\n","Epoch: 38/50, step: 80/83, loss: 2.76408, accuracy: 0.61719\n","Epoch: 38/50, step: 81/83, loss: 2.76167, accuracy: 0.61883\n","Epoch: 38/50, step: 82/83, loss: 2.73568, accuracy: 0.62043\n","Epoch: 38/50, step: 83/83, loss: 2.80583, accuracy: 0.61948\n","Epoch: 38/50, train loss: 2.80583, train accuracy: 0.61948, valid loss: 10.24275, valid accuracy: 0.16290\n","Epoch: 39/50, step: 1/83, loss: 1.72089, accuracy: 0.62500\n","Epoch: 39/50, step: 2/83, loss: 3.00848, accuracy: 0.56250\n","Epoch: 39/50, step: 3/83, loss: 2.47231, accuracy: 0.66667\n","Epoch: 39/50, step: 4/83, loss: 3.37296, accuracy: 0.56250\n","Epoch: 39/50, step: 5/83, loss: 3.40229, accuracy: 0.55000\n","Epoch: 39/50, step: 6/83, loss: 3.35144, accuracy: 0.58333\n","Epoch: 39/50, step: 7/83, loss: 2.94010, accuracy: 0.64286\n","Epoch: 39/50, step: 8/83, loss: 3.08118, accuracy: 0.65625\n","Epoch: 39/50, step: 9/83, loss: 3.11993, accuracy: 0.62500\n","Epoch: 39/50, step: 10/83, loss: 3.05972, accuracy: 0.63750\n","Epoch: 39/50, step: 11/83, loss: 3.17410, accuracy: 0.63636\n","Epoch: 39/50, step: 12/83, loss: 3.32363, accuracy: 0.62500\n","Epoch: 39/50, step: 13/83, loss: 3.25781, accuracy: 0.63462\n","Epoch: 39/50, step: 14/83, loss: 3.13463, accuracy: 0.64286\n","Epoch: 39/50, step: 15/83, loss: 3.03451, accuracy: 0.65833\n","Epoch: 39/50, step: 16/83, loss: 2.88374, accuracy: 0.65625\n","Epoch: 39/50, step: 17/83, loss: 2.88590, accuracy: 0.66176\n","Epoch: 39/50, step: 18/83, loss: 2.75859, accuracy: 0.67361\n","Epoch: 39/50, step: 19/83, loss: 2.70223, accuracy: 0.67763\n","Epoch: 39/50, step: 20/83, loss: 2.59097, accuracy: 0.68750\n","Epoch: 39/50, step: 21/83, loss: 2.62451, accuracy: 0.68452\n","Epoch: 39/50, step: 22/83, loss: 2.63581, accuracy: 0.67614\n","Epoch: 39/50, step: 23/83, loss: 2.58959, accuracy: 0.67935\n","Epoch: 39/50, step: 24/83, loss: 2.68857, accuracy: 0.66146\n","Epoch: 39/50, step: 25/83, loss: 2.61143, accuracy: 0.66000\n","Epoch: 39/50, step: 26/83, loss: 2.56711, accuracy: 0.66827\n","Epoch: 39/50, step: 27/83, loss: 2.55985, accuracy: 0.67130\n","Epoch: 39/50, step: 28/83, loss: 2.49026, accuracy: 0.67411\n","Epoch: 39/50, step: 29/83, loss: 2.50120, accuracy: 0.67672\n","Epoch: 39/50, step: 30/83, loss: 2.54965, accuracy: 0.66667\n","Epoch: 39/50, step: 31/83, loss: 2.58018, accuracy: 0.66129\n","Epoch: 39/50, step: 32/83, loss: 2.64402, accuracy: 0.65234\n","Epoch: 39/50, step: 33/83, loss: 2.61139, accuracy: 0.65530\n","Epoch: 39/50, step: 34/83, loss: 2.57785, accuracy: 0.65809\n","Epoch: 39/50, step: 35/83, loss: 2.57126, accuracy: 0.66071\n","Epoch: 39/50, step: 36/83, loss: 2.57239, accuracy: 0.65625\n","Epoch: 39/50, step: 37/83, loss: 2.51796, accuracy: 0.66554\n","Epoch: 39/50, step: 38/83, loss: 2.48978, accuracy: 0.66447\n","Epoch: 39/50, step: 39/83, loss: 2.46522, accuracy: 0.66987\n","Epoch: 39/50, step: 40/83, loss: 2.48934, accuracy: 0.67188\n","Epoch: 39/50, step: 41/83, loss: 2.48876, accuracy: 0.67073\n","Epoch: 39/50, step: 42/83, loss: 2.44837, accuracy: 0.66964\n","Epoch: 39/50, step: 43/83, loss: 2.49102, accuracy: 0.66570\n","Epoch: 39/50, step: 44/83, loss: 2.46819, accuracy: 0.66761\n","Epoch: 39/50, step: 45/83, loss: 2.46759, accuracy: 0.66667\n","Epoch: 39/50, step: 46/83, loss: 2.50422, accuracy: 0.66576\n","Epoch: 39/50, step: 47/83, loss: 2.54488, accuracy: 0.65691\n","Epoch: 39/50, step: 48/83, loss: 2.54239, accuracy: 0.65885\n","Epoch: 39/50, step: 49/83, loss: 2.52578, accuracy: 0.65816\n","Epoch: 39/50, step: 50/83, loss: 2.52321, accuracy: 0.65750\n","Epoch: 39/50, step: 51/83, loss: 2.57075, accuracy: 0.65441\n","Epoch: 39/50, step: 52/83, loss: 2.60500, accuracy: 0.64904\n","Epoch: 39/50, step: 53/83, loss: 2.58325, accuracy: 0.65094\n","Epoch: 39/50, step: 54/83, loss: 2.66390, accuracy: 0.64583\n","Epoch: 39/50, step: 55/83, loss: 2.64551, accuracy: 0.64318\n","Epoch: 39/50, step: 56/83, loss: 2.62598, accuracy: 0.64732\n","Epoch: 39/50, step: 57/83, loss: 2.60607, accuracy: 0.65132\n","Epoch: 39/50, step: 58/83, loss: 2.64553, accuracy: 0.64871\n","Epoch: 39/50, step: 59/83, loss: 2.64624, accuracy: 0.64619\n","Epoch: 39/50, step: 60/83, loss: 2.64551, accuracy: 0.64792\n","Epoch: 39/50, step: 61/83, loss: 2.67881, accuracy: 0.64139\n","Epoch: 39/50, step: 62/83, loss: 2.67742, accuracy: 0.64315\n","Epoch: 39/50, step: 63/83, loss: 2.70183, accuracy: 0.64286\n","Epoch: 39/50, step: 64/83, loss: 2.69798, accuracy: 0.64258\n","Epoch: 39/50, step: 65/83, loss: 2.69168, accuracy: 0.64231\n","Epoch: 39/50, step: 66/83, loss: 2.73372, accuracy: 0.63636\n","Epoch: 39/50, step: 67/83, loss: 2.73062, accuracy: 0.63619\n","Epoch: 39/50, step: 68/83, loss: 2.71451, accuracy: 0.63603\n","Epoch: 39/50, step: 69/83, loss: 2.69946, accuracy: 0.63587\n","Epoch: 39/50, step: 70/83, loss: 2.68342, accuracy: 0.63750\n","Epoch: 39/50, step: 71/83, loss: 2.65732, accuracy: 0.63732\n","Epoch: 39/50, step: 72/83, loss: 2.65431, accuracy: 0.63889\n","Epoch: 39/50, step: 73/83, loss: 2.63757, accuracy: 0.64041\n","Epoch: 39/50, step: 74/83, loss: 2.64423, accuracy: 0.64020\n","Epoch: 39/50, step: 75/83, loss: 2.65629, accuracy: 0.63833\n","Epoch: 39/50, step: 76/83, loss: 2.64298, accuracy: 0.63816\n","Epoch: 39/50, step: 77/83, loss: 2.64100, accuracy: 0.63799\n","Epoch: 39/50, step: 78/83, loss: 2.64141, accuracy: 0.63782\n","Epoch: 39/50, step: 79/83, loss: 2.67595, accuracy: 0.63291\n","Epoch: 39/50, step: 80/83, loss: 2.68232, accuracy: 0.63125\n","Epoch: 39/50, step: 81/83, loss: 2.71487, accuracy: 0.62963\n","Epoch: 39/50, step: 82/83, loss: 2.70357, accuracy: 0.62957\n","Epoch: 39/50, step: 83/83, loss: 2.76155, accuracy: 0.63014\n","Epoch: 39/50, train loss: 2.76155, train accuracy: 0.63014, valid loss: 10.28208, valid accuracy: 0.14932\n","Epoch: 40/50, step: 1/83, loss: 3.23905, accuracy: 0.75000\n","Epoch: 40/50, step: 2/83, loss: 3.17526, accuracy: 0.81250\n","Epoch: 40/50, step: 3/83, loss: 2.94314, accuracy: 0.75000\n","Epoch: 40/50, step: 4/83, loss: 2.86025, accuracy: 0.71875\n","Epoch: 40/50, step: 5/83, loss: 2.66497, accuracy: 0.70000\n","Epoch: 40/50, step: 6/83, loss: 2.80675, accuracy: 0.66667\n","Epoch: 40/50, step: 7/83, loss: 2.76453, accuracy: 0.66071\n","Epoch: 40/50, step: 8/83, loss: 2.86110, accuracy: 0.62500\n","Epoch: 40/50, step: 9/83, loss: 2.70624, accuracy: 0.65278\n","Epoch: 40/50, step: 10/83, loss: 2.87732, accuracy: 0.63750\n","Epoch: 40/50, step: 11/83, loss: 2.76134, accuracy: 0.64773\n","Epoch: 40/50, step: 12/83, loss: 2.83128, accuracy: 0.62500\n","Epoch: 40/50, step: 13/83, loss: 2.66725, accuracy: 0.63462\n","Epoch: 40/50, step: 14/83, loss: 2.58075, accuracy: 0.65179\n","Epoch: 40/50, step: 15/83, loss: 2.56248, accuracy: 0.65000\n","Epoch: 40/50, step: 16/83, loss: 2.51742, accuracy: 0.64844\n","Epoch: 40/50, step: 17/83, loss: 2.56079, accuracy: 0.65441\n","Epoch: 40/50, step: 18/83, loss: 2.55431, accuracy: 0.66667\n","Epoch: 40/50, step: 19/83, loss: 2.59689, accuracy: 0.66447\n","Epoch: 40/50, step: 20/83, loss: 2.58859, accuracy: 0.66875\n","Epoch: 40/50, step: 21/83, loss: 2.72786, accuracy: 0.65476\n","Epoch: 40/50, step: 22/83, loss: 2.71419, accuracy: 0.65341\n","Epoch: 40/50, step: 23/83, loss: 2.85402, accuracy: 0.64130\n","Epoch: 40/50, step: 24/83, loss: 2.83858, accuracy: 0.63021\n","Epoch: 40/50, step: 25/83, loss: 2.85567, accuracy: 0.63500\n","Epoch: 40/50, step: 26/83, loss: 2.94380, accuracy: 0.62019\n","Epoch: 40/50, step: 27/83, loss: 2.96419, accuracy: 0.62037\n","Epoch: 40/50, step: 28/83, loss: 2.91491, accuracy: 0.62946\n","Epoch: 40/50, step: 29/83, loss: 2.84207, accuracy: 0.62931\n","Epoch: 40/50, step: 30/83, loss: 2.82731, accuracy: 0.62917\n","Epoch: 40/50, step: 31/83, loss: 2.81638, accuracy: 0.62903\n","Epoch: 40/50, step: 32/83, loss: 2.83565, accuracy: 0.62500\n","Epoch: 40/50, step: 33/83, loss: 2.79636, accuracy: 0.63258\n","Epoch: 40/50, step: 34/83, loss: 2.86538, accuracy: 0.62500\n","Epoch: 40/50, step: 35/83, loss: 2.88208, accuracy: 0.61786\n","Epoch: 40/50, step: 36/83, loss: 2.87501, accuracy: 0.61458\n","Epoch: 40/50, step: 37/83, loss: 2.93916, accuracy: 0.61149\n","Epoch: 40/50, step: 38/83, loss: 2.95303, accuracy: 0.61184\n","Epoch: 40/50, step: 39/83, loss: 2.89705, accuracy: 0.61859\n","Epoch: 40/50, step: 40/83, loss: 2.88643, accuracy: 0.62187\n","Epoch: 40/50, step: 41/83, loss: 2.85802, accuracy: 0.61585\n","Epoch: 40/50, step: 42/83, loss: 2.84800, accuracy: 0.61607\n","Epoch: 40/50, step: 43/83, loss: 2.83187, accuracy: 0.61919\n","Epoch: 40/50, step: 44/83, loss: 2.84422, accuracy: 0.61364\n","Epoch: 40/50, step: 45/83, loss: 2.87450, accuracy: 0.61389\n","Epoch: 40/50, step: 46/83, loss: 2.90441, accuracy: 0.61413\n","Epoch: 40/50, step: 47/83, loss: 2.89569, accuracy: 0.61702\n","Epoch: 40/50, step: 48/83, loss: 2.87473, accuracy: 0.62240\n","Epoch: 40/50, step: 49/83, loss: 2.90456, accuracy: 0.61990\n","Epoch: 40/50, step: 50/83, loss: 2.90586, accuracy: 0.61750\n","Epoch: 40/50, step: 51/83, loss: 2.91486, accuracy: 0.62010\n","Epoch: 40/50, step: 52/83, loss: 2.94282, accuracy: 0.61779\n","Epoch: 40/50, step: 53/83, loss: 2.92602, accuracy: 0.61792\n","Epoch: 40/50, step: 54/83, loss: 2.92218, accuracy: 0.61574\n","Epoch: 40/50, step: 55/83, loss: 2.91308, accuracy: 0.61591\n","Epoch: 40/50, step: 56/83, loss: 2.91871, accuracy: 0.61607\n","Epoch: 40/50, step: 57/83, loss: 2.87616, accuracy: 0.62281\n","Epoch: 40/50, step: 58/83, loss: 2.89628, accuracy: 0.62284\n","Epoch: 40/50, step: 59/83, loss: 2.87236, accuracy: 0.62500\n","Epoch: 40/50, step: 60/83, loss: 2.91708, accuracy: 0.62083\n","Epoch: 40/50, step: 61/83, loss: 2.90976, accuracy: 0.62090\n","Epoch: 40/50, step: 62/83, loss: 2.91658, accuracy: 0.62097\n","Epoch: 40/50, step: 63/83, loss: 2.90933, accuracy: 0.62103\n","Epoch: 40/50, step: 64/83, loss: 2.87164, accuracy: 0.62695\n","Epoch: 40/50, step: 65/83, loss: 2.89483, accuracy: 0.62308\n","Epoch: 40/50, step: 66/83, loss: 2.88962, accuracy: 0.62311\n","Epoch: 40/50, step: 67/83, loss: 2.89807, accuracy: 0.62313\n","Epoch: 40/50, step: 68/83, loss: 2.90486, accuracy: 0.62316\n","Epoch: 40/50, step: 69/83, loss: 2.88643, accuracy: 0.62319\n","Epoch: 40/50, step: 70/83, loss: 2.85593, accuracy: 0.62500\n","Epoch: 40/50, step: 71/83, loss: 2.83917, accuracy: 0.62500\n","Epoch: 40/50, step: 72/83, loss: 2.84782, accuracy: 0.62500\n","Epoch: 40/50, step: 73/83, loss: 2.82805, accuracy: 0.62671\n","Epoch: 40/50, step: 74/83, loss: 2.87460, accuracy: 0.62162\n","Epoch: 40/50, step: 75/83, loss: 2.86791, accuracy: 0.62500\n","Epoch: 40/50, step: 76/83, loss: 2.89555, accuracy: 0.62336\n","Epoch: 40/50, step: 77/83, loss: 2.88520, accuracy: 0.62500\n","Epoch: 40/50, step: 78/83, loss: 2.87870, accuracy: 0.62179\n","Epoch: 40/50, step: 79/83, loss: 2.88628, accuracy: 0.62184\n","Epoch: 40/50, step: 80/83, loss: 2.90087, accuracy: 0.62187\n","Epoch: 40/50, step: 81/83, loss: 2.89638, accuracy: 0.62346\n","Epoch: 40/50, step: 82/83, loss: 2.88008, accuracy: 0.62652\n","Epoch: 40/50, step: 83/83, loss: 2.85829, accuracy: 0.62557\n","Epoch: 40/50, train loss: 2.85829, train accuracy: 0.62557, valid loss: 10.08109, valid accuracy: 0.15837\n","Epoch: 41/50, step: 1/83, loss: 1.60467, accuracy: 0.75000\n","Epoch: 41/50, step: 2/83, loss: 3.03758, accuracy: 0.68750\n","Epoch: 41/50, step: 3/83, loss: 2.74152, accuracy: 0.66667\n","Epoch: 41/50, step: 4/83, loss: 3.58445, accuracy: 0.59375\n","Epoch: 41/50, step: 5/83, loss: 3.19001, accuracy: 0.65000\n","Epoch: 41/50, step: 6/83, loss: 3.49993, accuracy: 0.60417\n","Epoch: 41/50, step: 7/83, loss: 3.62425, accuracy: 0.58929\n","Epoch: 41/50, step: 8/83, loss: 3.48148, accuracy: 0.59375\n","Epoch: 41/50, step: 9/83, loss: 3.39955, accuracy: 0.59722\n","Epoch: 41/50, step: 10/83, loss: 3.11428, accuracy: 0.63750\n","Epoch: 41/50, step: 11/83, loss: 3.03369, accuracy: 0.63636\n","Epoch: 41/50, step: 12/83, loss: 3.15459, accuracy: 0.63542\n","Epoch: 41/50, step: 13/83, loss: 3.03542, accuracy: 0.63462\n","Epoch: 41/50, step: 14/83, loss: 2.92462, accuracy: 0.64286\n","Epoch: 41/50, step: 15/83, loss: 2.76620, accuracy: 0.66667\n","Epoch: 41/50, step: 16/83, loss: 2.69596, accuracy: 0.67188\n","Epoch: 41/50, step: 17/83, loss: 2.74121, accuracy: 0.66912\n","Epoch: 41/50, step: 18/83, loss: 2.76288, accuracy: 0.67361\n","Epoch: 41/50, step: 19/83, loss: 2.79704, accuracy: 0.67105\n","Epoch: 41/50, step: 20/83, loss: 2.78814, accuracy: 0.66875\n","Epoch: 41/50, step: 21/83, loss: 2.73099, accuracy: 0.66667\n","Epoch: 41/50, step: 22/83, loss: 2.76096, accuracy: 0.66477\n","Epoch: 41/50, step: 23/83, loss: 2.80044, accuracy: 0.65217\n","Epoch: 41/50, step: 24/83, loss: 2.72428, accuracy: 0.64583\n","Epoch: 41/50, step: 25/83, loss: 2.77961, accuracy: 0.64500\n","Epoch: 41/50, step: 26/83, loss: 2.76205, accuracy: 0.65385\n","Epoch: 41/50, step: 27/83, loss: 2.75042, accuracy: 0.65741\n","Epoch: 41/50, step: 28/83, loss: 2.80971, accuracy: 0.64286\n","Epoch: 41/50, step: 29/83, loss: 2.80420, accuracy: 0.64224\n","Epoch: 41/50, step: 30/83, loss: 2.76677, accuracy: 0.64167\n","Epoch: 41/50, step: 31/83, loss: 2.73664, accuracy: 0.63306\n","Epoch: 41/50, step: 32/83, loss: 2.79298, accuracy: 0.62891\n","Epoch: 41/50, step: 33/83, loss: 2.80782, accuracy: 0.62879\n","Epoch: 41/50, step: 34/83, loss: 2.78214, accuracy: 0.63235\n","Epoch: 41/50, step: 35/83, loss: 2.80209, accuracy: 0.62857\n","Epoch: 41/50, step: 36/83, loss: 2.78790, accuracy: 0.63542\n","Epoch: 41/50, step: 37/83, loss: 2.75594, accuracy: 0.63851\n","Epoch: 41/50, step: 38/83, loss: 2.74755, accuracy: 0.64145\n","Epoch: 41/50, step: 39/83, loss: 2.76908, accuracy: 0.63782\n","Epoch: 41/50, step: 40/83, loss: 2.73870, accuracy: 0.64062\n","Epoch: 41/50, step: 41/83, loss: 2.70833, accuracy: 0.64329\n","Epoch: 41/50, step: 42/83, loss: 2.68208, accuracy: 0.63988\n","Epoch: 41/50, step: 43/83, loss: 2.72329, accuracy: 0.63372\n","Epoch: 41/50, step: 44/83, loss: 2.69687, accuracy: 0.63636\n","Epoch: 41/50, step: 45/83, loss: 2.69454, accuracy: 0.63333\n","Epoch: 41/50, step: 46/83, loss: 2.69352, accuracy: 0.63043\n","Epoch: 41/50, step: 47/83, loss: 2.66982, accuracy: 0.63564\n","Epoch: 41/50, step: 48/83, loss: 2.64812, accuracy: 0.63802\n","Epoch: 41/50, step: 49/83, loss: 2.62222, accuracy: 0.64541\n","Epoch: 41/50, step: 50/83, loss: 2.67470, accuracy: 0.64000\n","Epoch: 41/50, step: 51/83, loss: 2.69160, accuracy: 0.63725\n","Epoch: 41/50, step: 52/83, loss: 2.68481, accuracy: 0.63942\n","Epoch: 41/50, step: 53/83, loss: 2.66389, accuracy: 0.64151\n","Epoch: 41/50, step: 54/83, loss: 2.64111, accuracy: 0.64583\n","Epoch: 41/50, step: 55/83, loss: 2.62099, accuracy: 0.64773\n","Epoch: 41/50, step: 56/83, loss: 2.62085, accuracy: 0.64732\n","Epoch: 41/50, step: 57/83, loss: 2.61905, accuracy: 0.64912\n","Epoch: 41/50, step: 58/83, loss: 2.61416, accuracy: 0.65086\n","Epoch: 41/50, step: 59/83, loss: 2.59581, accuracy: 0.65042\n","Epoch: 41/50, step: 60/83, loss: 2.58082, accuracy: 0.64792\n","Epoch: 41/50, step: 61/83, loss: 2.56638, accuracy: 0.65164\n","Epoch: 41/50, step: 62/83, loss: 2.53337, accuracy: 0.65524\n","Epoch: 41/50, step: 63/83, loss: 2.56151, accuracy: 0.65476\n","Epoch: 41/50, step: 64/83, loss: 2.54477, accuracy: 0.65820\n","Epoch: 41/50, step: 65/83, loss: 2.51538, accuracy: 0.66346\n","Epoch: 41/50, step: 66/83, loss: 2.53055, accuracy: 0.65909\n","Epoch: 41/50, step: 67/83, loss: 2.54085, accuracy: 0.66045\n","Epoch: 41/50, step: 68/83, loss: 2.56569, accuracy: 0.65993\n","Epoch: 41/50, step: 69/83, loss: 2.55025, accuracy: 0.66304\n","Epoch: 41/50, step: 70/83, loss: 2.54881, accuracy: 0.66429\n","Epoch: 41/50, step: 71/83, loss: 2.55936, accuracy: 0.66373\n","Epoch: 41/50, step: 72/83, loss: 2.54367, accuracy: 0.66493\n","Epoch: 41/50, step: 73/83, loss: 2.59320, accuracy: 0.66267\n","Epoch: 41/50, step: 74/83, loss: 2.58288, accuracy: 0.66047\n","Epoch: 41/50, step: 75/83, loss: 2.59457, accuracy: 0.65833\n","Epoch: 41/50, step: 76/83, loss: 2.58008, accuracy: 0.66118\n","Epoch: 41/50, step: 77/83, loss: 2.60546, accuracy: 0.65747\n","Epoch: 41/50, step: 78/83, loss: 2.60220, accuracy: 0.65705\n","Epoch: 41/50, step: 79/83, loss: 2.59977, accuracy: 0.65981\n","Epoch: 41/50, step: 80/83, loss: 2.62435, accuracy: 0.65938\n","Epoch: 41/50, step: 81/83, loss: 2.62241, accuracy: 0.65895\n","Epoch: 41/50, step: 82/83, loss: 2.61569, accuracy: 0.66006\n","Epoch: 41/50, step: 83/83, loss: 2.59845, accuracy: 0.65906\n","Epoch: 41/50, train loss: 2.59845, train accuracy: 0.65906, valid loss: 9.86203, valid accuracy: 0.15837\n","Epoch: 42/50, step: 1/83, loss: 0.58984, accuracy: 0.75000\n","Epoch: 42/50, step: 2/83, loss: 1.07868, accuracy: 0.75000\n","Epoch: 42/50, step: 3/83, loss: 1.88560, accuracy: 0.66667\n","Epoch: 42/50, step: 4/83, loss: 1.79681, accuracy: 0.65625\n","Epoch: 42/50, step: 5/83, loss: 1.74333, accuracy: 0.70000\n","Epoch: 42/50, step: 6/83, loss: 1.56424, accuracy: 0.70833\n","Epoch: 42/50, step: 7/83, loss: 1.93292, accuracy: 0.69643\n","Epoch: 42/50, step: 8/83, loss: 2.00526, accuracy: 0.67188\n","Epoch: 42/50, step: 9/83, loss: 2.07164, accuracy: 0.65278\n","Epoch: 42/50, step: 10/83, loss: 2.02828, accuracy: 0.67500\n","Epoch: 42/50, step: 11/83, loss: 2.07064, accuracy: 0.68182\n","Epoch: 42/50, step: 12/83, loss: 2.09366, accuracy: 0.68750\n","Epoch: 42/50, step: 13/83, loss: 2.05586, accuracy: 0.69231\n","Epoch: 42/50, step: 14/83, loss: 2.14577, accuracy: 0.67857\n","Epoch: 42/50, step: 15/83, loss: 2.34427, accuracy: 0.65000\n","Epoch: 42/50, step: 16/83, loss: 2.41794, accuracy: 0.64062\n","Epoch: 42/50, step: 17/83, loss: 2.58722, accuracy: 0.63235\n","Epoch: 42/50, step: 18/83, loss: 2.52911, accuracy: 0.63889\n","Epoch: 42/50, step: 19/83, loss: 2.50405, accuracy: 0.63158\n","Epoch: 42/50, step: 20/83, loss: 2.54757, accuracy: 0.63125\n","Epoch: 42/50, step: 21/83, loss: 2.60932, accuracy: 0.63690\n","Epoch: 42/50, step: 22/83, loss: 2.55871, accuracy: 0.64773\n","Epoch: 42/50, step: 23/83, loss: 2.60037, accuracy: 0.64130\n","Epoch: 42/50, step: 24/83, loss: 2.63557, accuracy: 0.64062\n","Epoch: 42/50, step: 25/83, loss: 2.59220, accuracy: 0.65000\n","Epoch: 42/50, step: 26/83, loss: 2.59098, accuracy: 0.64423\n","Epoch: 42/50, step: 27/83, loss: 2.55545, accuracy: 0.63889\n","Epoch: 42/50, step: 28/83, loss: 2.58529, accuracy: 0.63839\n","Epoch: 42/50, step: 29/83, loss: 2.61936, accuracy: 0.63793\n","Epoch: 42/50, step: 30/83, loss: 2.55887, accuracy: 0.64167\n","Epoch: 42/50, step: 31/83, loss: 2.53322, accuracy: 0.64919\n","Epoch: 42/50, step: 32/83, loss: 2.53377, accuracy: 0.64453\n","Epoch: 42/50, step: 33/83, loss: 2.47566, accuracy: 0.64773\n","Epoch: 42/50, step: 34/83, loss: 2.44574, accuracy: 0.65074\n","Epoch: 42/50, step: 35/83, loss: 2.48034, accuracy: 0.64643\n","Epoch: 42/50, step: 36/83, loss: 2.53130, accuracy: 0.64236\n","Epoch: 42/50, step: 37/83, loss: 2.57251, accuracy: 0.63851\n","Epoch: 42/50, step: 38/83, loss: 2.61376, accuracy: 0.63487\n","Epoch: 42/50, step: 39/83, loss: 2.58807, accuracy: 0.63462\n","Epoch: 42/50, step: 40/83, loss: 2.58348, accuracy: 0.63437\n","Epoch: 42/50, step: 41/83, loss: 2.58230, accuracy: 0.63110\n","Epoch: 42/50, step: 42/83, loss: 2.53599, accuracy: 0.63690\n","Epoch: 42/50, step: 43/83, loss: 2.48794, accuracy: 0.64535\n","Epoch: 42/50, step: 44/83, loss: 2.52915, accuracy: 0.64489\n","Epoch: 42/50, step: 45/83, loss: 2.51835, accuracy: 0.65000\n","Epoch: 42/50, step: 46/83, loss: 2.49612, accuracy: 0.65489\n","Epoch: 42/50, step: 47/83, loss: 2.51722, accuracy: 0.65160\n","Epoch: 42/50, step: 48/83, loss: 2.53307, accuracy: 0.65104\n","Epoch: 42/50, step: 49/83, loss: 2.60333, accuracy: 0.64796\n","Epoch: 42/50, step: 50/83, loss: 2.60280, accuracy: 0.65000\n","Epoch: 42/50, step: 51/83, loss: 2.56884, accuracy: 0.65441\n","Epoch: 42/50, step: 52/83, loss: 2.60308, accuracy: 0.64904\n","Epoch: 42/50, step: 53/83, loss: 2.58192, accuracy: 0.64858\n","Epoch: 42/50, step: 54/83, loss: 2.56312, accuracy: 0.65046\n","Epoch: 42/50, step: 55/83, loss: 2.56179, accuracy: 0.65227\n","Epoch: 42/50, step: 56/83, loss: 2.57540, accuracy: 0.65179\n","Epoch: 42/50, step: 57/83, loss: 2.57902, accuracy: 0.64912\n","Epoch: 42/50, step: 58/83, loss: 2.62225, accuracy: 0.64655\n","Epoch: 42/50, step: 59/83, loss: 2.66398, accuracy: 0.64407\n","Epoch: 42/50, step: 60/83, loss: 2.67414, accuracy: 0.64375\n","Epoch: 42/50, step: 61/83, loss: 2.64160, accuracy: 0.64549\n","Epoch: 42/50, step: 62/83, loss: 2.64025, accuracy: 0.64516\n","Epoch: 42/50, step: 63/83, loss: 2.65295, accuracy: 0.64484\n","Epoch: 42/50, step: 64/83, loss: 2.63544, accuracy: 0.64844\n","Epoch: 42/50, step: 65/83, loss: 2.63304, accuracy: 0.65000\n","Epoch: 42/50, step: 66/83, loss: 2.64453, accuracy: 0.64962\n","Epoch: 42/50, step: 67/83, loss: 2.67516, accuracy: 0.64366\n","Epoch: 42/50, step: 68/83, loss: 2.64547, accuracy: 0.64522\n","Epoch: 42/50, step: 69/83, loss: 2.65718, accuracy: 0.64312\n","Epoch: 42/50, step: 70/83, loss: 2.65364, accuracy: 0.64464\n","Epoch: 42/50, step: 71/83, loss: 2.63706, accuracy: 0.64437\n","Epoch: 42/50, step: 72/83, loss: 2.66337, accuracy: 0.64062\n","Epoch: 42/50, step: 73/83, loss: 2.65194, accuracy: 0.64041\n","Epoch: 42/50, step: 74/83, loss: 2.64881, accuracy: 0.63682\n","Epoch: 42/50, step: 75/83, loss: 2.63274, accuracy: 0.63667\n","Epoch: 42/50, step: 76/83, loss: 2.60595, accuracy: 0.63980\n","Epoch: 42/50, step: 77/83, loss: 2.62393, accuracy: 0.63961\n","Epoch: 42/50, step: 78/83, loss: 2.63175, accuracy: 0.64263\n","Epoch: 42/50, step: 79/83, loss: 2.64275, accuracy: 0.63924\n","Epoch: 42/50, step: 80/83, loss: 2.62915, accuracy: 0.64062\n","Epoch: 42/50, step: 81/83, loss: 2.61451, accuracy: 0.64352\n","Epoch: 42/50, step: 82/83, loss: 2.59991, accuracy: 0.64634\n","Epoch: 42/50, step: 83/83, loss: 2.66918, accuracy: 0.64536\n","Epoch: 42/50, train loss: 2.66918, train accuracy: 0.64536, valid loss: 9.78336, valid accuracy: 0.16742\n","Epoch: 43/50, step: 1/83, loss: 2.59930, accuracy: 0.50000\n","Epoch: 43/50, step: 2/83, loss: 2.63970, accuracy: 0.56250\n","Epoch: 43/50, step: 3/83, loss: 3.12978, accuracy: 0.54167\n","Epoch: 43/50, step: 4/83, loss: 2.94118, accuracy: 0.53125\n","Epoch: 43/50, step: 5/83, loss: 2.81231, accuracy: 0.55000\n","Epoch: 43/50, step: 6/83, loss: 3.03051, accuracy: 0.54167\n","Epoch: 43/50, step: 7/83, loss: 3.10959, accuracy: 0.53571\n","Epoch: 43/50, step: 8/83, loss: 3.65169, accuracy: 0.53125\n","Epoch: 43/50, step: 9/83, loss: 3.62488, accuracy: 0.54167\n","Epoch: 43/50, step: 10/83, loss: 3.51426, accuracy: 0.55000\n","Epoch: 43/50, step: 11/83, loss: 3.83952, accuracy: 0.53409\n","Epoch: 43/50, step: 12/83, loss: 3.72497, accuracy: 0.53125\n","Epoch: 43/50, step: 13/83, loss: 3.56406, accuracy: 0.54808\n","Epoch: 43/50, step: 14/83, loss: 3.42349, accuracy: 0.56250\n","Epoch: 43/50, step: 15/83, loss: 3.56419, accuracy: 0.56667\n","Epoch: 43/50, step: 16/83, loss: 3.43206, accuracy: 0.57812\n","Epoch: 43/50, step: 17/83, loss: 3.38418, accuracy: 0.56618\n","Epoch: 43/50, step: 18/83, loss: 3.33047, accuracy: 0.57639\n","Epoch: 43/50, step: 19/83, loss: 3.18225, accuracy: 0.59868\n","Epoch: 43/50, step: 20/83, loss: 3.19087, accuracy: 0.60000\n","Epoch: 43/50, step: 21/83, loss: 3.15916, accuracy: 0.60714\n","Epoch: 43/50, step: 22/83, loss: 3.30012, accuracy: 0.59091\n","Epoch: 43/50, step: 23/83, loss: 3.21819, accuracy: 0.59783\n","Epoch: 43/50, step: 24/83, loss: 3.26935, accuracy: 0.59375\n","Epoch: 43/50, step: 25/83, loss: 3.20926, accuracy: 0.59500\n","Epoch: 43/50, step: 26/83, loss: 3.24615, accuracy: 0.59135\n","Epoch: 43/50, step: 27/83, loss: 3.19787, accuracy: 0.59259\n","Epoch: 43/50, step: 28/83, loss: 3.20534, accuracy: 0.58929\n","Epoch: 43/50, step: 29/83, loss: 3.18215, accuracy: 0.59052\n","Epoch: 43/50, step: 30/83, loss: 3.09341, accuracy: 0.60417\n","Epoch: 43/50, step: 31/83, loss: 3.06804, accuracy: 0.60484\n","Epoch: 43/50, step: 32/83, loss: 3.07982, accuracy: 0.60156\n","Epoch: 43/50, step: 33/83, loss: 3.06173, accuracy: 0.60227\n","Epoch: 43/50, step: 34/83, loss: 3.04807, accuracy: 0.60294\n","Epoch: 43/50, step: 35/83, loss: 3.05733, accuracy: 0.59643\n","Epoch: 43/50, step: 36/83, loss: 3.08271, accuracy: 0.60069\n","Epoch: 43/50, step: 37/83, loss: 3.08958, accuracy: 0.60135\n","Epoch: 43/50, step: 38/83, loss: 3.05445, accuracy: 0.59868\n","Epoch: 43/50, step: 39/83, loss: 3.06363, accuracy: 0.59615\n","Epoch: 43/50, step: 40/83, loss: 3.02305, accuracy: 0.60312\n","Epoch: 43/50, step: 41/83, loss: 3.06757, accuracy: 0.60061\n","Epoch: 43/50, step: 42/83, loss: 3.09805, accuracy: 0.59524\n","Epoch: 43/50, step: 43/83, loss: 3.06427, accuracy: 0.59593\n","Epoch: 43/50, step: 44/83, loss: 3.05116, accuracy: 0.59659\n","Epoch: 43/50, step: 45/83, loss: 3.02354, accuracy: 0.59444\n","Epoch: 43/50, step: 46/83, loss: 3.02991, accuracy: 0.59511\n","Epoch: 43/50, step: 47/83, loss: 3.01692, accuracy: 0.59840\n","Epoch: 43/50, step: 48/83, loss: 2.97987, accuracy: 0.60417\n","Epoch: 43/50, step: 49/83, loss: 2.98808, accuracy: 0.60459\n","Epoch: 43/50, step: 50/83, loss: 2.96697, accuracy: 0.60000\n","Epoch: 43/50, step: 51/83, loss: 2.94059, accuracy: 0.60049\n","Epoch: 43/50, step: 52/83, loss: 2.94229, accuracy: 0.60337\n","Epoch: 43/50, step: 53/83, loss: 2.91748, accuracy: 0.60613\n","Epoch: 43/50, step: 54/83, loss: 2.89192, accuracy: 0.60648\n","Epoch: 43/50, step: 55/83, loss: 2.91278, accuracy: 0.60682\n","Epoch: 43/50, step: 56/83, loss: 2.90611, accuracy: 0.60714\n","Epoch: 43/50, step: 57/83, loss: 2.94392, accuracy: 0.60526\n","Epoch: 43/50, step: 58/83, loss: 2.93766, accuracy: 0.60560\n","Epoch: 43/50, step: 59/83, loss: 2.93224, accuracy: 0.60593\n","Epoch: 43/50, step: 60/83, loss: 2.94006, accuracy: 0.60625\n","Epoch: 43/50, step: 61/83, loss: 2.96321, accuracy: 0.60451\n","Epoch: 43/50, step: 62/83, loss: 2.94369, accuracy: 0.60282\n","Epoch: 43/50, step: 63/83, loss: 2.96521, accuracy: 0.60317\n","Epoch: 43/50, step: 64/83, loss: 2.95885, accuracy: 0.60547\n","Epoch: 43/50, step: 65/83, loss: 2.96473, accuracy: 0.60577\n","Epoch: 43/50, step: 66/83, loss: 2.95698, accuracy: 0.60606\n","Epoch: 43/50, step: 67/83, loss: 2.95024, accuracy: 0.60634\n","Epoch: 43/50, step: 68/83, loss: 2.98136, accuracy: 0.60294\n","Epoch: 43/50, step: 69/83, loss: 2.95941, accuracy: 0.60507\n","Epoch: 43/50, step: 70/83, loss: 2.92674, accuracy: 0.60893\n","Epoch: 43/50, step: 71/83, loss: 2.90828, accuracy: 0.61092\n","Epoch: 43/50, step: 72/83, loss: 2.93045, accuracy: 0.60938\n","Epoch: 43/50, step: 73/83, loss: 2.89868, accuracy: 0.61301\n","Epoch: 43/50, step: 74/83, loss: 2.91247, accuracy: 0.61486\n","Epoch: 43/50, step: 75/83, loss: 2.88059, accuracy: 0.62000\n","Epoch: 43/50, step: 76/83, loss: 2.86233, accuracy: 0.61842\n","Epoch: 43/50, step: 77/83, loss: 2.84721, accuracy: 0.61688\n","Epoch: 43/50, step: 78/83, loss: 2.87538, accuracy: 0.61378\n","Epoch: 43/50, step: 79/83, loss: 2.86865, accuracy: 0.61392\n","Epoch: 43/50, step: 80/83, loss: 2.85232, accuracy: 0.61563\n","Epoch: 43/50, step: 81/83, loss: 2.86007, accuracy: 0.61574\n","Epoch: 43/50, step: 82/83, loss: 2.84432, accuracy: 0.61890\n","Epoch: 43/50, step: 83/83, loss: 2.90066, accuracy: 0.61948\n","Epoch: 43/50, train loss: 2.90066, train accuracy: 0.61948, valid loss: 9.96718, valid accuracy: 0.16290\n","Epoch: 44/50, step: 1/83, loss: 2.50663, accuracy: 0.62500\n","Epoch: 44/50, step: 2/83, loss: 2.17745, accuracy: 0.62500\n","Epoch: 44/50, step: 3/83, loss: 3.25016, accuracy: 0.54167\n","Epoch: 44/50, step: 4/83, loss: 3.27387, accuracy: 0.59375\n","Epoch: 44/50, step: 5/83, loss: 2.94352, accuracy: 0.62500\n","Epoch: 44/50, step: 6/83, loss: 2.54186, accuracy: 0.66667\n","Epoch: 44/50, step: 7/83, loss: 2.65317, accuracy: 0.64286\n","Epoch: 44/50, step: 8/83, loss: 2.63145, accuracy: 0.65625\n","Epoch: 44/50, step: 9/83, loss: 2.71783, accuracy: 0.63889\n","Epoch: 44/50, step: 10/83, loss: 2.75898, accuracy: 0.61250\n","Epoch: 44/50, step: 11/83, loss: 2.72042, accuracy: 0.62500\n","Epoch: 44/50, step: 12/83, loss: 2.61545, accuracy: 0.63542\n","Epoch: 44/50, step: 13/83, loss: 2.54012, accuracy: 0.64423\n","Epoch: 44/50, step: 14/83, loss: 2.47265, accuracy: 0.66071\n","Epoch: 44/50, step: 15/83, loss: 2.40671, accuracy: 0.67500\n","Epoch: 44/50, step: 16/83, loss: 2.41484, accuracy: 0.66406\n","Epoch: 44/50, step: 17/83, loss: 2.41660, accuracy: 0.66912\n","Epoch: 44/50, step: 18/83, loss: 2.41476, accuracy: 0.67361\n","Epoch: 44/50, step: 19/83, loss: 2.51556, accuracy: 0.65789\n","Epoch: 44/50, step: 20/83, loss: 2.60888, accuracy: 0.65000\n","Epoch: 44/50, step: 21/83, loss: 2.55653, accuracy: 0.66071\n","Epoch: 44/50, step: 22/83, loss: 2.50161, accuracy: 0.65909\n","Epoch: 44/50, step: 23/83, loss: 2.59065, accuracy: 0.65217\n","Epoch: 44/50, step: 24/83, loss: 2.73969, accuracy: 0.64062\n","Epoch: 44/50, step: 25/83, loss: 2.72506, accuracy: 0.64500\n","Epoch: 44/50, step: 26/83, loss: 2.75963, accuracy: 0.62981\n","Epoch: 44/50, step: 27/83, loss: 2.73932, accuracy: 0.63426\n","Epoch: 44/50, step: 28/83, loss: 2.75869, accuracy: 0.63393\n","Epoch: 44/50, step: 29/83, loss: 2.71646, accuracy: 0.63362\n","Epoch: 44/50, step: 30/83, loss: 2.73900, accuracy: 0.63333\n","Epoch: 44/50, step: 31/83, loss: 2.78930, accuracy: 0.63306\n","Epoch: 44/50, step: 32/83, loss: 2.84468, accuracy: 0.62500\n","Epoch: 44/50, step: 33/83, loss: 2.86312, accuracy: 0.62121\n","Epoch: 44/50, step: 34/83, loss: 2.87678, accuracy: 0.62132\n","Epoch: 44/50, step: 35/83, loss: 2.88927, accuracy: 0.61786\n","Epoch: 44/50, step: 36/83, loss: 2.85354, accuracy: 0.61806\n","Epoch: 44/50, step: 37/83, loss: 2.86861, accuracy: 0.61486\n","Epoch: 44/50, step: 38/83, loss: 2.82666, accuracy: 0.62171\n","Epoch: 44/50, step: 39/83, loss: 2.77068, accuracy: 0.62179\n","Epoch: 44/50, step: 40/83, loss: 2.85137, accuracy: 0.61563\n","Epoch: 44/50, step: 41/83, loss: 2.82390, accuracy: 0.61585\n","Epoch: 44/50, step: 42/83, loss: 2.80702, accuracy: 0.62202\n","Epoch: 44/50, step: 43/83, loss: 2.77790, accuracy: 0.62791\n","Epoch: 44/50, step: 44/83, loss: 2.78820, accuracy: 0.62784\n","Epoch: 44/50, step: 45/83, loss: 2.84399, accuracy: 0.62222\n","Epoch: 44/50, step: 46/83, loss: 2.83952, accuracy: 0.62228\n","Epoch: 44/50, step: 47/83, loss: 2.83300, accuracy: 0.62234\n","Epoch: 44/50, step: 48/83, loss: 2.82598, accuracy: 0.61979\n","Epoch: 44/50, step: 49/83, loss: 2.77864, accuracy: 0.62755\n","Epoch: 44/50, step: 50/83, loss: 2.79389, accuracy: 0.62750\n","Epoch: 44/50, step: 51/83, loss: 2.77005, accuracy: 0.62990\n","Epoch: 44/50, step: 52/83, loss: 2.79852, accuracy: 0.62500\n","Epoch: 44/50, step: 53/83, loss: 2.82670, accuracy: 0.62264\n","Epoch: 44/50, step: 54/83, loss: 2.80284, accuracy: 0.62269\n","Epoch: 44/50, step: 55/83, loss: 2.78055, accuracy: 0.62727\n","Epoch: 44/50, step: 56/83, loss: 2.82687, accuracy: 0.62277\n","Epoch: 44/50, step: 57/83, loss: 2.80325, accuracy: 0.62281\n","Epoch: 44/50, step: 58/83, loss: 2.81058, accuracy: 0.62500\n","Epoch: 44/50, step: 59/83, loss: 2.80222, accuracy: 0.62712\n","Epoch: 44/50, step: 60/83, loss: 2.79682, accuracy: 0.62917\n","Epoch: 44/50, step: 61/83, loss: 2.82117, accuracy: 0.62705\n","Epoch: 44/50, step: 62/83, loss: 2.82297, accuracy: 0.62702\n","Epoch: 44/50, step: 63/83, loss: 2.84365, accuracy: 0.62500\n","Epoch: 44/50, step: 64/83, loss: 2.85485, accuracy: 0.62109\n","Epoch: 44/50, step: 65/83, loss: 2.85907, accuracy: 0.61923\n","Epoch: 44/50, step: 66/83, loss: 2.82449, accuracy: 0.62121\n","Epoch: 44/50, step: 67/83, loss: 2.81732, accuracy: 0.62127\n","Epoch: 44/50, step: 68/83, loss: 2.80007, accuracy: 0.62316\n","Epoch: 44/50, step: 69/83, loss: 2.78065, accuracy: 0.62500\n","Epoch: 44/50, step: 70/83, loss: 2.78994, accuracy: 0.62321\n","Epoch: 44/50, step: 71/83, loss: 2.78773, accuracy: 0.62148\n","Epoch: 44/50, step: 72/83, loss: 2.76877, accuracy: 0.62326\n","Epoch: 44/50, step: 73/83, loss: 2.75448, accuracy: 0.62329\n","Epoch: 44/50, step: 74/83, loss: 2.73788, accuracy: 0.62669\n","Epoch: 44/50, step: 75/83, loss: 2.73174, accuracy: 0.62667\n","Epoch: 44/50, step: 76/83, loss: 2.71792, accuracy: 0.62664\n","Epoch: 44/50, step: 77/83, loss: 2.71518, accuracy: 0.62662\n","Epoch: 44/50, step: 78/83, loss: 2.75612, accuracy: 0.62500\n","Epoch: 44/50, step: 79/83, loss: 2.75471, accuracy: 0.62500\n","Epoch: 44/50, step: 80/83, loss: 2.73848, accuracy: 0.62656\n","Epoch: 44/50, step: 81/83, loss: 2.72588, accuracy: 0.62654\n","Epoch: 44/50, step: 82/83, loss: 2.74368, accuracy: 0.62652\n","Epoch: 44/50, step: 83/83, loss: 2.81121, accuracy: 0.62557\n","Epoch: 44/50, train loss: 2.81121, train accuracy: 0.62557, valid loss: 10.16269, valid accuracy: 0.15837\n","Epoch: 45/50, step: 1/83, loss: 5.13984, accuracy: 0.25000\n","Epoch: 45/50, step: 2/83, loss: 3.29491, accuracy: 0.50000\n","Epoch: 45/50, step: 3/83, loss: 3.57067, accuracy: 0.50000\n","Epoch: 45/50, step: 4/83, loss: 2.98161, accuracy: 0.53125\n","Epoch: 45/50, step: 5/83, loss: 2.50222, accuracy: 0.55000\n","Epoch: 45/50, step: 6/83, loss: 2.49715, accuracy: 0.56250\n","Epoch: 45/50, step: 7/83, loss: 2.86426, accuracy: 0.55357\n","Epoch: 45/50, step: 8/83, loss: 2.69109, accuracy: 0.59375\n","Epoch: 45/50, step: 9/83, loss: 2.55963, accuracy: 0.61111\n","Epoch: 45/50, step: 10/83, loss: 2.66182, accuracy: 0.60000\n","Epoch: 45/50, step: 11/83, loss: 2.46800, accuracy: 0.63636\n","Epoch: 45/50, step: 12/83, loss: 2.60530, accuracy: 0.62500\n","Epoch: 45/50, step: 13/83, loss: 2.58299, accuracy: 0.63462\n","Epoch: 45/50, step: 14/83, loss: 2.50676, accuracy: 0.65179\n","Epoch: 45/50, step: 15/83, loss: 2.67522, accuracy: 0.64167\n","Epoch: 45/50, step: 16/83, loss: 2.72486, accuracy: 0.63281\n","Epoch: 45/50, step: 17/83, loss: 2.70753, accuracy: 0.63235\n","Epoch: 45/50, step: 18/83, loss: 2.68706, accuracy: 0.63889\n","Epoch: 45/50, step: 19/83, loss: 2.67964, accuracy: 0.63158\n","Epoch: 45/50, step: 20/83, loss: 2.73359, accuracy: 0.62500\n","Epoch: 45/50, step: 21/83, loss: 2.67775, accuracy: 0.63690\n","Epoch: 45/50, step: 22/83, loss: 2.67008, accuracy: 0.63636\n","Epoch: 45/50, step: 23/83, loss: 2.62866, accuracy: 0.63587\n","Epoch: 45/50, step: 24/83, loss: 2.54940, accuracy: 0.64062\n","Epoch: 45/50, step: 25/83, loss: 2.51055, accuracy: 0.64500\n","Epoch: 45/50, step: 26/83, loss: 2.55254, accuracy: 0.63942\n","Epoch: 45/50, step: 27/83, loss: 2.53564, accuracy: 0.63426\n","Epoch: 45/50, step: 28/83, loss: 2.53024, accuracy: 0.63839\n","Epoch: 45/50, step: 29/83, loss: 2.52667, accuracy: 0.63793\n","Epoch: 45/50, step: 30/83, loss: 2.56122, accuracy: 0.63333\n","Epoch: 45/50, step: 31/83, loss: 2.52509, accuracy: 0.63710\n","Epoch: 45/50, step: 32/83, loss: 2.52529, accuracy: 0.63672\n","Epoch: 45/50, step: 33/83, loss: 2.49072, accuracy: 0.64015\n","Epoch: 45/50, step: 34/83, loss: 2.48515, accuracy: 0.64338\n","Epoch: 45/50, step: 35/83, loss: 2.48649, accuracy: 0.64286\n","Epoch: 45/50, step: 36/83, loss: 2.43335, accuracy: 0.64931\n","Epoch: 45/50, step: 37/83, loss: 2.43354, accuracy: 0.64865\n","Epoch: 45/50, step: 38/83, loss: 2.40636, accuracy: 0.65789\n","Epoch: 45/50, step: 39/83, loss: 2.47663, accuracy: 0.65064\n","Epoch: 45/50, step: 40/83, loss: 2.51579, accuracy: 0.65000\n","Epoch: 45/50, step: 41/83, loss: 2.55880, accuracy: 0.64939\n","Epoch: 45/50, step: 42/83, loss: 2.52897, accuracy: 0.65476\n","Epoch: 45/50, step: 43/83, loss: 2.53013, accuracy: 0.65116\n","Epoch: 45/50, step: 44/83, loss: 2.56723, accuracy: 0.64773\n","Epoch: 45/50, step: 45/83, loss: 2.54412, accuracy: 0.65000\n","Epoch: 45/50, step: 46/83, loss: 2.55762, accuracy: 0.64946\n","Epoch: 45/50, step: 47/83, loss: 2.55176, accuracy: 0.65160\n","Epoch: 45/50, step: 48/83, loss: 2.53371, accuracy: 0.65104\n","Epoch: 45/50, step: 49/83, loss: 2.56066, accuracy: 0.65051\n","Epoch: 45/50, step: 50/83, loss: 2.56427, accuracy: 0.64750\n","Epoch: 45/50, step: 51/83, loss: 2.54768, accuracy: 0.64951\n","Epoch: 45/50, step: 52/83, loss: 2.51092, accuracy: 0.65144\n","Epoch: 45/50, step: 53/83, loss: 2.47536, accuracy: 0.65566\n","Epoch: 45/50, step: 54/83, loss: 2.49168, accuracy: 0.65509\n","Epoch: 45/50, step: 55/83, loss: 2.48788, accuracy: 0.65682\n","Epoch: 45/50, step: 56/83, loss: 2.47100, accuracy: 0.66071\n","Epoch: 45/50, step: 57/83, loss: 2.43802, accuracy: 0.66667\n","Epoch: 45/50, step: 58/83, loss: 2.42391, accuracy: 0.67026\n","Epoch: 45/50, step: 59/83, loss: 2.47371, accuracy: 0.66525\n","Epoch: 45/50, step: 60/83, loss: 2.51653, accuracy: 0.66250\n","Epoch: 45/50, step: 61/83, loss: 2.53188, accuracy: 0.65779\n","Epoch: 45/50, step: 62/83, loss: 2.49925, accuracy: 0.66331\n","Epoch: 45/50, step: 63/83, loss: 2.56159, accuracy: 0.66270\n","Epoch: 45/50, step: 64/83, loss: 2.57137, accuracy: 0.66211\n","Epoch: 45/50, step: 65/83, loss: 2.55556, accuracy: 0.66538\n","Epoch: 45/50, step: 66/83, loss: 2.58432, accuracy: 0.66288\n","Epoch: 45/50, step: 67/83, loss: 2.60504, accuracy: 0.65858\n","Epoch: 45/50, step: 68/83, loss: 2.62775, accuracy: 0.65625\n","Epoch: 45/50, step: 69/83, loss: 2.63721, accuracy: 0.65580\n","Epoch: 45/50, step: 70/83, loss: 2.62994, accuracy: 0.65714\n","Epoch: 45/50, step: 71/83, loss: 2.61300, accuracy: 0.66021\n","Epoch: 45/50, step: 72/83, loss: 2.59735, accuracy: 0.66319\n","Epoch: 45/50, step: 73/83, loss: 2.59572, accuracy: 0.66438\n","Epoch: 45/50, step: 74/83, loss: 2.63276, accuracy: 0.66047\n","Epoch: 45/50, step: 75/83, loss: 2.67599, accuracy: 0.65667\n","Epoch: 45/50, step: 76/83, loss: 2.67331, accuracy: 0.65461\n","Epoch: 45/50, step: 77/83, loss: 2.67061, accuracy: 0.65584\n","Epoch: 45/50, step: 78/83, loss: 2.70407, accuracy: 0.65385\n","Epoch: 45/50, step: 79/83, loss: 2.73683, accuracy: 0.65032\n","Epoch: 45/50, step: 80/83, loss: 2.76601, accuracy: 0.64688\n","Epoch: 45/50, step: 81/83, loss: 2.75087, accuracy: 0.64969\n","Epoch: 45/50, step: 82/83, loss: 2.75814, accuracy: 0.64787\n","Epoch: 45/50, step: 83/83, loss: 2.81520, accuracy: 0.64840\n","Epoch: 45/50, train loss: 2.81520, train accuracy: 0.64840, valid loss: 10.07791, valid accuracy: 0.16290\n","Epoch: 46/50, step: 1/83, loss: 3.19568, accuracy: 0.62500\n","Epoch: 46/50, step: 2/83, loss: 2.79810, accuracy: 0.68750\n","Epoch: 46/50, step: 3/83, loss: 2.32015, accuracy: 0.75000\n","Epoch: 46/50, step: 4/83, loss: 2.56326, accuracy: 0.68750\n","Epoch: 46/50, step: 5/83, loss: 3.24591, accuracy: 0.62500\n","Epoch: 46/50, step: 6/83, loss: 3.55270, accuracy: 0.60417\n","Epoch: 46/50, step: 7/83, loss: 3.39812, accuracy: 0.60714\n","Epoch: 46/50, step: 8/83, loss: 3.15001, accuracy: 0.60938\n","Epoch: 46/50, step: 9/83, loss: 2.96934, accuracy: 0.63889\n","Epoch: 46/50, step: 10/83, loss: 2.81752, accuracy: 0.66250\n","Epoch: 46/50, step: 11/83, loss: 2.94983, accuracy: 0.64773\n","Epoch: 46/50, step: 12/83, loss: 2.92132, accuracy: 0.64583\n","Epoch: 46/50, step: 13/83, loss: 2.81980, accuracy: 0.65385\n","Epoch: 46/50, step: 14/83, loss: 2.89962, accuracy: 0.64286\n","Epoch: 46/50, step: 15/83, loss: 2.75536, accuracy: 0.64167\n","Epoch: 46/50, step: 16/83, loss: 2.80619, accuracy: 0.63281\n","Epoch: 46/50, step: 17/83, loss: 3.00577, accuracy: 0.63235\n","Epoch: 46/50, step: 18/83, loss: 3.12191, accuracy: 0.63194\n","Epoch: 46/50, step: 19/83, loss: 3.04586, accuracy: 0.63158\n","Epoch: 46/50, step: 20/83, loss: 3.07026, accuracy: 0.63125\n","Epoch: 46/50, step: 21/83, loss: 2.99115, accuracy: 0.64881\n","Epoch: 46/50, step: 22/83, loss: 2.95635, accuracy: 0.65341\n","Epoch: 46/50, step: 23/83, loss: 2.89223, accuracy: 0.66304\n","Epoch: 46/50, step: 24/83, loss: 2.87381, accuracy: 0.66667\n","Epoch: 46/50, step: 25/83, loss: 2.80984, accuracy: 0.67500\n","Epoch: 46/50, step: 26/83, loss: 2.77016, accuracy: 0.67308\n","Epoch: 46/50, step: 27/83, loss: 2.72022, accuracy: 0.67593\n","Epoch: 46/50, step: 28/83, loss: 2.71685, accuracy: 0.67411\n","Epoch: 46/50, step: 29/83, loss: 2.71052, accuracy: 0.67241\n","Epoch: 46/50, step: 30/83, loss: 2.66527, accuracy: 0.67917\n","Epoch: 46/50, step: 31/83, loss: 2.66054, accuracy: 0.67742\n","Epoch: 46/50, step: 32/83, loss: 2.65320, accuracy: 0.67578\n","Epoch: 46/50, step: 33/83, loss: 2.69623, accuracy: 0.67803\n","Epoch: 46/50, step: 34/83, loss: 2.71901, accuracy: 0.67279\n","Epoch: 46/50, step: 35/83, loss: 2.74397, accuracy: 0.66429\n","Epoch: 46/50, step: 36/83, loss: 2.71773, accuracy: 0.65625\n","Epoch: 46/50, step: 37/83, loss: 2.68579, accuracy: 0.65878\n","Epoch: 46/50, step: 38/83, loss: 2.74868, accuracy: 0.64803\n","Epoch: 46/50, step: 39/83, loss: 2.79446, accuracy: 0.64423\n","Epoch: 46/50, step: 40/83, loss: 2.83341, accuracy: 0.64375\n","Epoch: 46/50, step: 41/83, loss: 2.80370, accuracy: 0.64939\n","Epoch: 46/50, step: 42/83, loss: 2.83755, accuracy: 0.64286\n","Epoch: 46/50, step: 43/83, loss: 2.85173, accuracy: 0.64535\n","Epoch: 46/50, step: 44/83, loss: 2.88096, accuracy: 0.64205\n","Epoch: 46/50, step: 45/83, loss: 2.85008, accuracy: 0.65000\n","Epoch: 46/50, step: 46/83, loss: 2.81688, accuracy: 0.65489\n","Epoch: 46/50, step: 47/83, loss: 2.79228, accuracy: 0.65691\n","Epoch: 46/50, step: 48/83, loss: 2.81652, accuracy: 0.65625\n","Epoch: 46/50, step: 49/83, loss: 2.79070, accuracy: 0.66071\n","Epoch: 46/50, step: 50/83, loss: 2.80680, accuracy: 0.65500\n","Epoch: 46/50, step: 51/83, loss: 2.76973, accuracy: 0.65686\n","Epoch: 46/50, step: 52/83, loss: 2.76595, accuracy: 0.65625\n","Epoch: 46/50, step: 53/83, loss: 2.78394, accuracy: 0.64858\n","Epoch: 46/50, step: 54/83, loss: 2.78429, accuracy: 0.64583\n","Epoch: 46/50, step: 55/83, loss: 2.77616, accuracy: 0.64773\n","Epoch: 46/50, step: 56/83, loss: 2.76813, accuracy: 0.64955\n","Epoch: 46/50, step: 57/83, loss: 2.76051, accuracy: 0.64912\n","Epoch: 46/50, step: 58/83, loss: 2.74112, accuracy: 0.65302\n","Epoch: 46/50, step: 59/83, loss: 2.77043, accuracy: 0.65042\n","Epoch: 46/50, step: 60/83, loss: 2.74903, accuracy: 0.65208\n","Epoch: 46/50, step: 61/83, loss: 2.74380, accuracy: 0.65164\n","Epoch: 46/50, step: 62/83, loss: 2.76859, accuracy: 0.64718\n","Epoch: 46/50, step: 63/83, loss: 2.78430, accuracy: 0.64683\n","Epoch: 46/50, step: 64/83, loss: 2.74685, accuracy: 0.65039\n","Epoch: 46/50, step: 65/83, loss: 2.76040, accuracy: 0.65000\n","Epoch: 46/50, step: 66/83, loss: 2.75649, accuracy: 0.64962\n","Epoch: 46/50, step: 67/83, loss: 2.76501, accuracy: 0.64925\n","Epoch: 46/50, step: 68/83, loss: 2.74770, accuracy: 0.64890\n","Epoch: 46/50, step: 69/83, loss: 2.75770, accuracy: 0.64674\n","Epoch: 46/50, step: 70/83, loss: 2.75301, accuracy: 0.64821\n","Epoch: 46/50, step: 71/83, loss: 2.77241, accuracy: 0.64613\n","Epoch: 46/50, step: 72/83, loss: 2.78184, accuracy: 0.64236\n","Epoch: 46/50, step: 73/83, loss: 2.80261, accuracy: 0.64212\n","Epoch: 46/50, step: 74/83, loss: 2.79452, accuracy: 0.64189\n","Epoch: 46/50, step: 75/83, loss: 2.81506, accuracy: 0.64000\n","Epoch: 46/50, step: 76/83, loss: 2.82032, accuracy: 0.63980\n","Epoch: 46/50, step: 77/83, loss: 2.82049, accuracy: 0.63799\n","Epoch: 46/50, step: 78/83, loss: 2.82245, accuracy: 0.63942\n","Epoch: 46/50, step: 79/83, loss: 2.81627, accuracy: 0.64082\n","Epoch: 46/50, step: 80/83, loss: 2.83558, accuracy: 0.63906\n","Epoch: 46/50, step: 81/83, loss: 2.83051, accuracy: 0.63735\n","Epoch: 46/50, step: 82/83, loss: 2.81777, accuracy: 0.63567\n","Epoch: 46/50, step: 83/83, loss: 2.88883, accuracy: 0.63470\n","Epoch: 46/50, train loss: 2.88883, train accuracy: 0.63470, valid loss: 10.28001, valid accuracy: 0.15837\n","Epoch: 47/50, step: 1/83, loss: 2.73424, accuracy: 0.62500\n","Epoch: 47/50, step: 2/83, loss: 3.38672, accuracy: 0.56250\n","Epoch: 47/50, step: 3/83, loss: 2.90236, accuracy: 0.54167\n","Epoch: 47/50, step: 4/83, loss: 2.82360, accuracy: 0.59375\n","Epoch: 47/50, step: 5/83, loss: 2.56965, accuracy: 0.57500\n","Epoch: 47/50, step: 6/83, loss: 2.38081, accuracy: 0.62500\n","Epoch: 47/50, step: 7/83, loss: 2.26504, accuracy: 0.64286\n","Epoch: 47/50, step: 8/83, loss: 2.85914, accuracy: 0.60938\n","Epoch: 47/50, step: 9/83, loss: 2.98512, accuracy: 0.62500\n","Epoch: 47/50, step: 10/83, loss: 2.76205, accuracy: 0.63750\n","Epoch: 47/50, step: 11/83, loss: 2.99810, accuracy: 0.61364\n","Epoch: 47/50, step: 12/83, loss: 2.87538, accuracy: 0.62500\n","Epoch: 47/50, step: 13/83, loss: 2.84937, accuracy: 0.63462\n","Epoch: 47/50, step: 14/83, loss: 2.69414, accuracy: 0.64286\n","Epoch: 47/50, step: 15/83, loss: 2.67988, accuracy: 0.64167\n","Epoch: 47/50, step: 16/83, loss: 2.88035, accuracy: 0.62500\n","Epoch: 47/50, step: 17/83, loss: 2.75895, accuracy: 0.63971\n","Epoch: 47/50, step: 18/83, loss: 2.72066, accuracy: 0.65278\n","Epoch: 47/50, step: 19/83, loss: 2.66054, accuracy: 0.65789\n","Epoch: 47/50, step: 20/83, loss: 2.62342, accuracy: 0.65625\n","Epoch: 47/50, step: 21/83, loss: 2.56966, accuracy: 0.66071\n","Epoch: 47/50, step: 22/83, loss: 2.59880, accuracy: 0.65909\n","Epoch: 47/50, step: 23/83, loss: 2.62372, accuracy: 0.66304\n","Epoch: 47/50, step: 24/83, loss: 2.65298, accuracy: 0.65104\n","Epoch: 47/50, step: 25/83, loss: 2.60628, accuracy: 0.66000\n","Epoch: 47/50, step: 26/83, loss: 2.60748, accuracy: 0.65865\n","Epoch: 47/50, step: 27/83, loss: 2.66911, accuracy: 0.65278\n","Epoch: 47/50, step: 28/83, loss: 2.66658, accuracy: 0.65179\n","Epoch: 47/50, step: 29/83, loss: 2.66162, accuracy: 0.65086\n","Epoch: 47/50, step: 30/83, loss: 2.62230, accuracy: 0.65000\n","Epoch: 47/50, step: 31/83, loss: 2.65442, accuracy: 0.64113\n","Epoch: 47/50, step: 32/83, loss: 2.61724, accuracy: 0.65234\n","Epoch: 47/50, step: 33/83, loss: 2.60873, accuracy: 0.65909\n","Epoch: 47/50, step: 34/83, loss: 2.60070, accuracy: 0.66544\n","Epoch: 47/50, step: 35/83, loss: 2.60004, accuracy: 0.66786\n","Epoch: 47/50, step: 36/83, loss: 2.61829, accuracy: 0.67014\n","Epoch: 47/50, step: 37/83, loss: 2.64929, accuracy: 0.66216\n","Epoch: 47/50, step: 38/83, loss: 2.62122, accuracy: 0.66447\n","Epoch: 47/50, step: 39/83, loss: 2.64407, accuracy: 0.66346\n","Epoch: 47/50, step: 40/83, loss: 2.68582, accuracy: 0.65938\n","Epoch: 47/50, step: 41/83, loss: 2.65785, accuracy: 0.66463\n","Epoch: 47/50, step: 42/83, loss: 2.67360, accuracy: 0.66369\n","Epoch: 47/50, step: 43/83, loss: 2.68854, accuracy: 0.66279\n","Epoch: 47/50, step: 44/83, loss: 2.67979, accuracy: 0.66193\n","Epoch: 47/50, step: 45/83, loss: 2.65302, accuracy: 0.66389\n","Epoch: 47/50, step: 46/83, loss: 2.66860, accuracy: 0.66033\n","Epoch: 47/50, step: 47/83, loss: 2.68462, accuracy: 0.65957\n","Epoch: 47/50, step: 48/83, loss: 2.72112, accuracy: 0.65365\n","Epoch: 47/50, step: 49/83, loss: 2.79075, accuracy: 0.65051\n","Epoch: 47/50, step: 50/83, loss: 2.76542, accuracy: 0.65250\n","Epoch: 47/50, step: 51/83, loss: 2.81145, accuracy: 0.64951\n","Epoch: 47/50, step: 52/83, loss: 2.81621, accuracy: 0.64904\n","Epoch: 47/50, step: 53/83, loss: 2.79535, accuracy: 0.64858\n","Epoch: 47/50, step: 54/83, loss: 2.80237, accuracy: 0.64815\n","Epoch: 47/50, step: 55/83, loss: 2.77716, accuracy: 0.65227\n","Epoch: 47/50, step: 56/83, loss: 2.82293, accuracy: 0.64732\n","Epoch: 47/50, step: 57/83, loss: 2.79878, accuracy: 0.65132\n","Epoch: 47/50, step: 58/83, loss: 2.80809, accuracy: 0.65086\n","Epoch: 47/50, step: 59/83, loss: 2.82118, accuracy: 0.65042\n","Epoch: 47/50, step: 60/83, loss: 2.85352, accuracy: 0.64792\n","Epoch: 47/50, step: 61/83, loss: 2.83250, accuracy: 0.64959\n","Epoch: 47/50, step: 62/83, loss: 2.84326, accuracy: 0.64718\n","Epoch: 47/50, step: 63/83, loss: 2.83773, accuracy: 0.64683\n","Epoch: 47/50, step: 64/83, loss: 2.81534, accuracy: 0.64844\n","Epoch: 47/50, step: 65/83, loss: 2.79671, accuracy: 0.65000\n","Epoch: 47/50, step: 66/83, loss: 2.79014, accuracy: 0.64962\n","Epoch: 47/50, step: 67/83, loss: 2.76075, accuracy: 0.64925\n","Epoch: 47/50, step: 68/83, loss: 2.75442, accuracy: 0.64890\n","Epoch: 47/50, step: 69/83, loss: 2.77587, accuracy: 0.64674\n","Epoch: 47/50, step: 70/83, loss: 2.77395, accuracy: 0.64643\n","Epoch: 47/50, step: 71/83, loss: 2.77012, accuracy: 0.64789\n","Epoch: 47/50, step: 72/83, loss: 2.76786, accuracy: 0.64410\n","Epoch: 47/50, step: 73/83, loss: 2.76278, accuracy: 0.64555\n","Epoch: 47/50, step: 74/83, loss: 2.78318, accuracy: 0.64358\n","Epoch: 47/50, step: 75/83, loss: 2.76900, accuracy: 0.64333\n","Epoch: 47/50, step: 76/83, loss: 2.75645, accuracy: 0.64309\n","Epoch: 47/50, step: 77/83, loss: 2.74085, accuracy: 0.64286\n","Epoch: 47/50, step: 78/83, loss: 2.72724, accuracy: 0.64263\n","Epoch: 47/50, step: 79/83, loss: 2.71438, accuracy: 0.64082\n","Epoch: 47/50, step: 80/83, loss: 2.72348, accuracy: 0.63906\n","Epoch: 47/50, step: 81/83, loss: 2.76198, accuracy: 0.63426\n","Epoch: 47/50, step: 82/83, loss: 2.75901, accuracy: 0.63415\n","Epoch: 47/50, step: 83/83, loss: 2.80763, accuracy: 0.63318\n","Epoch: 47/50, train loss: 2.80763, train accuracy: 0.63318, valid loss: 9.97632, valid accuracy: 0.16742\n","Epoch: 48/50, step: 1/83, loss: 2.44088, accuracy: 0.62500\n","Epoch: 48/50, step: 2/83, loss: 3.86212, accuracy: 0.62500\n","Epoch: 48/50, step: 3/83, loss: 4.54577, accuracy: 0.50000\n","Epoch: 48/50, step: 4/83, loss: 4.00829, accuracy: 0.53125\n","Epoch: 48/50, step: 5/83, loss: 4.09836, accuracy: 0.50000\n","Epoch: 48/50, step: 6/83, loss: 3.85032, accuracy: 0.52083\n","Epoch: 48/50, step: 7/83, loss: 3.76540, accuracy: 0.51786\n","Epoch: 48/50, step: 8/83, loss: 3.37266, accuracy: 0.56250\n","Epoch: 48/50, step: 9/83, loss: 3.34266, accuracy: 0.58333\n","Epoch: 48/50, step: 10/83, loss: 3.48266, accuracy: 0.58750\n","Epoch: 48/50, step: 11/83, loss: 3.46527, accuracy: 0.59091\n","Epoch: 48/50, step: 12/83, loss: 3.31480, accuracy: 0.60417\n","Epoch: 48/50, step: 13/83, loss: 3.25207, accuracy: 0.60577\n","Epoch: 48/50, step: 14/83, loss: 3.19967, accuracy: 0.61607\n","Epoch: 48/50, step: 15/83, loss: 3.25869, accuracy: 0.61667\n","Epoch: 48/50, step: 16/83, loss: 3.14850, accuracy: 0.63281\n","Epoch: 48/50, step: 17/83, loss: 3.04799, accuracy: 0.64706\n","Epoch: 48/50, step: 18/83, loss: 3.07301, accuracy: 0.63889\n","Epoch: 48/50, step: 19/83, loss: 2.94556, accuracy: 0.65132\n","Epoch: 48/50, step: 20/83, loss: 2.86567, accuracy: 0.66875\n","Epoch: 48/50, step: 21/83, loss: 2.79872, accuracy: 0.67262\n","Epoch: 48/50, step: 22/83, loss: 2.82111, accuracy: 0.67614\n","Epoch: 48/50, step: 23/83, loss: 2.80614, accuracy: 0.67391\n","Epoch: 48/50, step: 24/83, loss: 2.86804, accuracy: 0.66667\n","Epoch: 48/50, step: 25/83, loss: 2.81683, accuracy: 0.66000\n","Epoch: 48/50, step: 26/83, loss: 2.79259, accuracy: 0.66346\n","Epoch: 48/50, step: 27/83, loss: 2.81471, accuracy: 0.66204\n","Epoch: 48/50, step: 28/83, loss: 2.84404, accuracy: 0.65179\n","Epoch: 48/50, step: 29/83, loss: 2.76737, accuracy: 0.66379\n","Epoch: 48/50, step: 30/83, loss: 2.78359, accuracy: 0.65833\n","Epoch: 48/50, step: 31/83, loss: 2.74310, accuracy: 0.66532\n","Epoch: 48/50, step: 32/83, loss: 2.68086, accuracy: 0.66797\n","Epoch: 48/50, step: 33/83, loss: 2.67293, accuracy: 0.66667\n","Epoch: 48/50, step: 34/83, loss: 2.69263, accuracy: 0.66544\n","Epoch: 48/50, step: 35/83, loss: 2.76063, accuracy: 0.65714\n","Epoch: 48/50, step: 36/83, loss: 2.75672, accuracy: 0.65625\n","Epoch: 48/50, step: 37/83, loss: 2.80043, accuracy: 0.65203\n","Epoch: 48/50, step: 38/83, loss: 2.78663, accuracy: 0.64803\n","Epoch: 48/50, step: 39/83, loss: 2.74911, accuracy: 0.65385\n","Epoch: 48/50, step: 40/83, loss: 2.74991, accuracy: 0.65312\n","Epoch: 48/50, step: 41/83, loss: 2.74347, accuracy: 0.65549\n","Epoch: 48/50, step: 42/83, loss: 2.71457, accuracy: 0.65774\n","Epoch: 48/50, step: 43/83, loss: 2.72962, accuracy: 0.65698\n","Epoch: 48/50, step: 44/83, loss: 2.70379, accuracy: 0.65909\n","Epoch: 48/50, step: 45/83, loss: 2.69713, accuracy: 0.66111\n","Epoch: 48/50, step: 46/83, loss: 2.71276, accuracy: 0.65489\n","Epoch: 48/50, step: 47/83, loss: 2.74547, accuracy: 0.65160\n","Epoch: 48/50, step: 48/83, loss: 2.75429, accuracy: 0.65365\n","Epoch: 48/50, step: 49/83, loss: 2.80441, accuracy: 0.64541\n","Epoch: 48/50, step: 50/83, loss: 2.77746, accuracy: 0.65000\n","Epoch: 48/50, step: 51/83, loss: 2.80628, accuracy: 0.64951\n","Epoch: 48/50, step: 52/83, loss: 2.79964, accuracy: 0.64904\n","Epoch: 48/50, step: 53/83, loss: 2.84581, accuracy: 0.64387\n","Epoch: 48/50, step: 54/83, loss: 2.83876, accuracy: 0.64352\n","Epoch: 48/50, step: 55/83, loss: 2.88120, accuracy: 0.63864\n","Epoch: 48/50, step: 56/83, loss: 2.87362, accuracy: 0.63616\n","Epoch: 48/50, step: 57/83, loss: 2.84806, accuracy: 0.63816\n","Epoch: 48/50, step: 58/83, loss: 2.82449, accuracy: 0.64224\n","Epoch: 48/50, step: 59/83, loss: 2.80670, accuracy: 0.63983\n","Epoch: 48/50, step: 60/83, loss: 2.83300, accuracy: 0.63750\n","Epoch: 48/50, step: 61/83, loss: 2.84413, accuracy: 0.63525\n","Epoch: 48/50, step: 62/83, loss: 2.87416, accuracy: 0.63306\n","Epoch: 48/50, step: 63/83, loss: 2.88366, accuracy: 0.63095\n","Epoch: 48/50, step: 64/83, loss: 2.90561, accuracy: 0.62891\n","Epoch: 48/50, step: 65/83, loss: 2.90113, accuracy: 0.62692\n","Epoch: 48/50, step: 66/83, loss: 2.89520, accuracy: 0.62879\n","Epoch: 48/50, step: 67/83, loss: 2.89119, accuracy: 0.62687\n","Epoch: 48/50, step: 68/83, loss: 2.86924, accuracy: 0.62868\n","Epoch: 48/50, step: 69/83, loss: 2.85252, accuracy: 0.63043\n","Epoch: 48/50, step: 70/83, loss: 2.84512, accuracy: 0.63214\n","Epoch: 48/50, step: 71/83, loss: 2.87176, accuracy: 0.63028\n","Epoch: 48/50, step: 72/83, loss: 2.84281, accuracy: 0.63194\n","Epoch: 48/50, step: 73/83, loss: 2.83903, accuracy: 0.63356\n","Epoch: 48/50, step: 74/83, loss: 2.83156, accuracy: 0.63176\n","Epoch: 48/50, step: 75/83, loss: 2.81350, accuracy: 0.63333\n","Epoch: 48/50, step: 76/83, loss: 2.78320, accuracy: 0.63651\n","Epoch: 48/50, step: 77/83, loss: 2.79363, accuracy: 0.63474\n","Epoch: 48/50, step: 78/83, loss: 2.78864, accuracy: 0.63462\n","Epoch: 48/50, step: 79/83, loss: 2.78547, accuracy: 0.63291\n","Epoch: 48/50, step: 80/83, loss: 2.79349, accuracy: 0.63281\n","Epoch: 48/50, step: 81/83, loss: 2.80159, accuracy: 0.63272\n","Epoch: 48/50, step: 82/83, loss: 2.79721, accuracy: 0.63262\n","Epoch: 48/50, step: 83/83, loss: 2.85999, accuracy: 0.63166\n","Epoch: 48/50, train loss: 2.85999, train accuracy: 0.63166, valid loss: 10.04971, valid accuracy: 0.17195\n","Epoch: 49/50, step: 1/83, loss: 4.09404, accuracy: 0.50000\n","Epoch: 49/50, step: 2/83, loss: 2.39337, accuracy: 0.62500\n","Epoch: 49/50, step: 3/83, loss: 3.62279, accuracy: 0.54167\n","Epoch: 49/50, step: 4/83, loss: 3.59365, accuracy: 0.53125\n","Epoch: 49/50, step: 5/83, loss: 3.13588, accuracy: 0.57500\n","Epoch: 49/50, step: 6/83, loss: 3.46192, accuracy: 0.54167\n","Epoch: 49/50, step: 7/83, loss: 3.45620, accuracy: 0.55357\n","Epoch: 49/50, step: 8/83, loss: 3.38009, accuracy: 0.54688\n","Epoch: 49/50, step: 9/83, loss: 3.40438, accuracy: 0.54167\n","Epoch: 49/50, step: 10/83, loss: 3.22484, accuracy: 0.56250\n","Epoch: 49/50, step: 11/83, loss: 3.24496, accuracy: 0.56818\n","Epoch: 49/50, step: 12/83, loss: 3.10002, accuracy: 0.58333\n","Epoch: 49/50, step: 13/83, loss: 3.13214, accuracy: 0.55769\n","Epoch: 49/50, step: 14/83, loss: 3.16097, accuracy: 0.55357\n","Epoch: 49/50, step: 15/83, loss: 3.24928, accuracy: 0.53333\n","Epoch: 49/50, step: 16/83, loss: 3.19850, accuracy: 0.54688\n","Epoch: 49/50, step: 17/83, loss: 3.14898, accuracy: 0.55882\n","Epoch: 49/50, step: 18/83, loss: 3.16080, accuracy: 0.56250\n","Epoch: 49/50, step: 19/83, loss: 3.11794, accuracy: 0.57237\n","Epoch: 49/50, step: 20/83, loss: 3.04464, accuracy: 0.59375\n","Epoch: 49/50, step: 21/83, loss: 2.97828, accuracy: 0.60119\n","Epoch: 49/50, step: 22/83, loss: 2.95339, accuracy: 0.60795\n","Epoch: 49/50, step: 23/83, loss: 2.84997, accuracy: 0.61957\n","Epoch: 49/50, step: 24/83, loss: 2.91794, accuracy: 0.61458\n","Epoch: 49/50, step: 25/83, loss: 2.83992, accuracy: 0.61500\n","Epoch: 49/50, step: 26/83, loss: 2.89859, accuracy: 0.61058\n","Epoch: 49/50, step: 27/83, loss: 2.87529, accuracy: 0.61574\n","Epoch: 49/50, step: 28/83, loss: 2.93679, accuracy: 0.60714\n","Epoch: 49/50, step: 29/83, loss: 2.88525, accuracy: 0.61638\n","Epoch: 49/50, step: 30/83, loss: 2.87075, accuracy: 0.61667\n","Epoch: 49/50, step: 31/83, loss: 2.83008, accuracy: 0.60887\n","Epoch: 49/50, step: 32/83, loss: 2.89557, accuracy: 0.60547\n","Epoch: 49/50, step: 33/83, loss: 2.82548, accuracy: 0.60985\n","Epoch: 49/50, step: 34/83, loss: 2.78896, accuracy: 0.61029\n","Epoch: 49/50, step: 35/83, loss: 2.77737, accuracy: 0.61429\n","Epoch: 49/50, step: 36/83, loss: 2.79568, accuracy: 0.60764\n","Epoch: 49/50, step: 37/83, loss: 2.78711, accuracy: 0.61149\n","Epoch: 49/50, step: 38/83, loss: 2.75576, accuracy: 0.61513\n","Epoch: 49/50, step: 39/83, loss: 2.74831, accuracy: 0.61859\n","Epoch: 49/50, step: 40/83, loss: 2.76136, accuracy: 0.61875\n","Epoch: 49/50, step: 41/83, loss: 2.74843, accuracy: 0.62195\n","Epoch: 49/50, step: 42/83, loss: 2.76028, accuracy: 0.62500\n","Epoch: 49/50, step: 43/83, loss: 2.73259, accuracy: 0.62209\n","Epoch: 49/50, step: 44/83, loss: 2.72413, accuracy: 0.62216\n","Epoch: 49/50, step: 45/83, loss: 2.70510, accuracy: 0.61944\n","Epoch: 49/50, step: 46/83, loss: 2.70255, accuracy: 0.61957\n","Epoch: 49/50, step: 47/83, loss: 2.72242, accuracy: 0.61436\n","Epoch: 49/50, step: 48/83, loss: 2.69725, accuracy: 0.61719\n","Epoch: 49/50, step: 49/83, loss: 2.70801, accuracy: 0.61480\n","Epoch: 49/50, step: 50/83, loss: 2.72540, accuracy: 0.61250\n","Epoch: 49/50, step: 51/83, loss: 2.71371, accuracy: 0.61275\n","Epoch: 49/50, step: 52/83, loss: 2.73189, accuracy: 0.60817\n","Epoch: 49/50, step: 53/83, loss: 2.72827, accuracy: 0.61085\n","Epoch: 49/50, step: 54/83, loss: 2.74307, accuracy: 0.60648\n","Epoch: 49/50, step: 55/83, loss: 2.78604, accuracy: 0.60455\n","Epoch: 49/50, step: 56/83, loss: 2.76329, accuracy: 0.60714\n","Epoch: 49/50, step: 57/83, loss: 2.77532, accuracy: 0.60526\n","Epoch: 49/50, step: 58/83, loss: 2.77125, accuracy: 0.60560\n","Epoch: 49/50, step: 59/83, loss: 2.76817, accuracy: 0.60593\n","Epoch: 49/50, step: 60/83, loss: 2.74898, accuracy: 0.61042\n","Epoch: 49/50, step: 61/83, loss: 2.71277, accuracy: 0.60861\n","Epoch: 49/50, step: 62/83, loss: 2.71026, accuracy: 0.61089\n","Epoch: 49/50, step: 63/83, loss: 2.73778, accuracy: 0.60714\n","Epoch: 49/50, step: 64/83, loss: 2.75274, accuracy: 0.59961\n","Epoch: 49/50, step: 65/83, loss: 2.77403, accuracy: 0.60000\n","Epoch: 49/50, step: 66/83, loss: 2.76864, accuracy: 0.60227\n","Epoch: 49/50, step: 67/83, loss: 2.77701, accuracy: 0.60075\n","Epoch: 49/50, step: 68/83, loss: 2.78004, accuracy: 0.60294\n","Epoch: 49/50, step: 69/83, loss: 2.78583, accuracy: 0.60326\n","Epoch: 49/50, step: 70/83, loss: 2.78820, accuracy: 0.60357\n","Epoch: 49/50, step: 71/83, loss: 2.77154, accuracy: 0.60387\n","Epoch: 49/50, step: 72/83, loss: 2.78964, accuracy: 0.60417\n","Epoch: 49/50, step: 73/83, loss: 2.75857, accuracy: 0.60959\n","Epoch: 49/50, step: 74/83, loss: 2.74315, accuracy: 0.61318\n","Epoch: 49/50, step: 75/83, loss: 2.74209, accuracy: 0.61333\n","Epoch: 49/50, step: 76/83, loss: 2.75309, accuracy: 0.61184\n","Epoch: 49/50, step: 77/83, loss: 2.76352, accuracy: 0.61039\n","Epoch: 49/50, step: 78/83, loss: 2.74612, accuracy: 0.61218\n","Epoch: 49/50, step: 79/83, loss: 2.72742, accuracy: 0.61551\n","Epoch: 49/50, step: 80/83, loss: 2.72311, accuracy: 0.61563\n","Epoch: 49/50, step: 81/83, loss: 2.73061, accuracy: 0.61420\n","Epoch: 49/50, step: 82/83, loss: 2.72621, accuracy: 0.61585\n","Epoch: 49/50, step: 83/83, loss: 2.78369, accuracy: 0.61644\n","Epoch: 49/50, train loss: 2.78369, train accuracy: 0.61644, valid loss: 9.83753, valid accuracy: 0.17195\n","Epoch: 50/50, step: 1/83, loss: 4.43542, accuracy: 0.37500\n","Epoch: 50/50, step: 2/83, loss: 4.61991, accuracy: 0.43750\n","Epoch: 50/50, step: 3/83, loss: 3.91788, accuracy: 0.50000\n","Epoch: 50/50, step: 4/83, loss: 3.13386, accuracy: 0.56250\n","Epoch: 50/50, step: 5/83, loss: 3.02447, accuracy: 0.57500\n","Epoch: 50/50, step: 6/83, loss: 2.90393, accuracy: 0.60417\n","Epoch: 50/50, step: 7/83, loss: 2.83522, accuracy: 0.64286\n","Epoch: 50/50, step: 8/83, loss: 2.85230, accuracy: 0.65625\n","Epoch: 50/50, step: 9/83, loss: 2.70469, accuracy: 0.68056\n","Epoch: 50/50, step: 10/83, loss: 2.75874, accuracy: 0.67500\n","Epoch: 50/50, step: 11/83, loss: 2.73787, accuracy: 0.68182\n","Epoch: 50/50, step: 12/83, loss: 2.69793, accuracy: 0.68750\n","Epoch: 50/50, step: 13/83, loss: 2.74990, accuracy: 0.68269\n","Epoch: 50/50, step: 14/83, loss: 2.59311, accuracy: 0.69643\n","Epoch: 50/50, step: 15/83, loss: 2.64000, accuracy: 0.68333\n","Epoch: 50/50, step: 16/83, loss: 2.51072, accuracy: 0.69531\n","Epoch: 50/50, step: 17/83, loss: 2.49830, accuracy: 0.69853\n","Epoch: 50/50, step: 18/83, loss: 2.50562, accuracy: 0.68056\n","Epoch: 50/50, step: 19/83, loss: 2.59899, accuracy: 0.67105\n","Epoch: 50/50, step: 20/83, loss: 2.54424, accuracy: 0.67500\n","Epoch: 50/50, step: 21/83, loss: 2.64285, accuracy: 0.66667\n","Epoch: 50/50, step: 22/83, loss: 2.71224, accuracy: 0.65909\n","Epoch: 50/50, step: 23/83, loss: 2.63555, accuracy: 0.65217\n","Epoch: 50/50, step: 24/83, loss: 2.62248, accuracy: 0.65104\n","Epoch: 50/50, step: 25/83, loss: 2.61584, accuracy: 0.65500\n","Epoch: 50/50, step: 26/83, loss: 2.64876, accuracy: 0.65385\n","Epoch: 50/50, step: 27/83, loss: 2.69281, accuracy: 0.63889\n","Epoch: 50/50, step: 28/83, loss: 2.74291, accuracy: 0.63839\n","Epoch: 50/50, step: 29/83, loss: 2.74924, accuracy: 0.63362\n","Epoch: 50/50, step: 30/83, loss: 2.78588, accuracy: 0.62500\n","Epoch: 50/50, step: 31/83, loss: 2.78196, accuracy: 0.62097\n","Epoch: 50/50, step: 32/83, loss: 2.80966, accuracy: 0.60938\n","Epoch: 50/50, step: 33/83, loss: 2.77096, accuracy: 0.61364\n","Epoch: 50/50, step: 34/83, loss: 2.76690, accuracy: 0.61397\n","Epoch: 50/50, step: 35/83, loss: 2.73240, accuracy: 0.61786\n","Epoch: 50/50, step: 36/83, loss: 2.76949, accuracy: 0.61111\n","Epoch: 50/50, step: 37/83, loss: 2.76668, accuracy: 0.60811\n","Epoch: 50/50, step: 38/83, loss: 2.83190, accuracy: 0.60855\n","Epoch: 50/50, step: 39/83, loss: 2.77736, accuracy: 0.61218\n","Epoch: 50/50, step: 40/83, loss: 2.78303, accuracy: 0.61563\n","Epoch: 50/50, step: 41/83, loss: 2.79337, accuracy: 0.60976\n","Epoch: 50/50, step: 42/83, loss: 2.77563, accuracy: 0.61310\n","Epoch: 50/50, step: 43/83, loss: 2.77241, accuracy: 0.61047\n","Epoch: 50/50, step: 44/83, loss: 2.76140, accuracy: 0.60795\n","Epoch: 50/50, step: 45/83, loss: 2.79723, accuracy: 0.60556\n","Epoch: 50/50, step: 46/83, loss: 2.74964, accuracy: 0.61141\n","Epoch: 50/50, step: 47/83, loss: 2.74874, accuracy: 0.61170\n","Epoch: 50/50, step: 48/83, loss: 2.74084, accuracy: 0.61198\n","Epoch: 50/50, step: 49/83, loss: 2.75469, accuracy: 0.60969\n","Epoch: 50/50, step: 50/83, loss: 2.76413, accuracy: 0.61250\n","Epoch: 50/50, step: 51/83, loss: 2.75838, accuracy: 0.61029\n","Epoch: 50/50, step: 52/83, loss: 2.73515, accuracy: 0.61058\n","Epoch: 50/50, step: 53/83, loss: 2.74455, accuracy: 0.61321\n","Epoch: 50/50, step: 54/83, loss: 2.72479, accuracy: 0.61111\n","Epoch: 50/50, step: 55/83, loss: 2.73984, accuracy: 0.61136\n","Epoch: 50/50, step: 56/83, loss: 2.70167, accuracy: 0.61384\n","Epoch: 50/50, step: 57/83, loss: 2.72351, accuracy: 0.61404\n","Epoch: 50/50, step: 58/83, loss: 2.73258, accuracy: 0.61638\n","Epoch: 50/50, step: 59/83, loss: 2.74499, accuracy: 0.61441\n","Epoch: 50/50, step: 60/83, loss: 2.73872, accuracy: 0.61458\n","Epoch: 50/50, step: 61/83, loss: 2.72163, accuracy: 0.61680\n","Epoch: 50/50, step: 62/83, loss: 2.73232, accuracy: 0.61290\n","Epoch: 50/50, step: 63/83, loss: 2.72805, accuracy: 0.61508\n","Epoch: 50/50, step: 64/83, loss: 2.69403, accuracy: 0.62109\n","Epoch: 50/50, step: 65/83, loss: 2.69237, accuracy: 0.62115\n","Epoch: 50/50, step: 66/83, loss: 2.70694, accuracy: 0.61932\n","Epoch: 50/50, step: 67/83, loss: 2.67578, accuracy: 0.62313\n","Epoch: 50/50, step: 68/83, loss: 2.66199, accuracy: 0.62684\n","Epoch: 50/50, step: 69/83, loss: 2.66028, accuracy: 0.62862\n","Epoch: 50/50, step: 70/83, loss: 2.67016, accuracy: 0.62857\n","Epoch: 50/50, step: 71/83, loss: 2.68632, accuracy: 0.62852\n","Epoch: 50/50, step: 72/83, loss: 2.69305, accuracy: 0.62847\n","Epoch: 50/50, step: 73/83, loss: 2.67609, accuracy: 0.63014\n","Epoch: 50/50, step: 74/83, loss: 2.68622, accuracy: 0.63007\n","Epoch: 50/50, step: 75/83, loss: 2.69373, accuracy: 0.63167\n","Epoch: 50/50, step: 76/83, loss: 2.70323, accuracy: 0.63158\n","Epoch: 50/50, step: 77/83, loss: 2.68889, accuracy: 0.63312\n","Epoch: 50/50, step: 78/83, loss: 2.67449, accuracy: 0.63622\n","Epoch: 50/50, step: 79/83, loss: 2.67148, accuracy: 0.63608\n","Epoch: 50/50, step: 80/83, loss: 2.71746, accuracy: 0.63281\n","Epoch: 50/50, step: 81/83, loss: 2.73917, accuracy: 0.63272\n","Epoch: 50/50, step: 82/83, loss: 2.74772, accuracy: 0.63262\n","Epoch: 50/50, step: 83/83, loss: 2.79943, accuracy: 0.63166\n","Epoch: 50/50, train loss: 2.79943, train accuracy: 0.63166, valid loss: 9.90553, valid accuracy: 0.16742\n"]}]},{"cell_type":"code","source":["loss_object = tf.keras.metrics.SparseCategoricalCrossentropy()\n","test_loss = tf.keras.metrics.Mean()\n","test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n","\n","@tf.function\n","def test_step(images, labels):\n","  predictions = model(images, include_aux_logits=False, training=False)\n","  t_loss = loss_object(labels, predictions)\n","\n","  test_loss(t_loss)\n","  test_accuracy(labels, predictions)\n","\n","for test_images, test_labels in test_dataset:\n","  test_step(test_images, test_labels)\n","  print(\"loss: {:.5f}, test accuracy: {:.5f}\".format(test_loss.result(),\n","                                                           test_accuracy.result()))\n","\n","print(\"The accuracy on test set is: {:.3f}%\".format(test_accuracy.result()*100))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Gsio9uODcrbG","executionInfo":{"status":"ok","timestamp":1668031913721,"user_tz":300,"elapsed":9242,"user":{"displayName":"Zhou Zeqi","userId":"01018575624929983032"}},"outputId":"aac62ad4-9d22-4673-e336-ae880d11a7f1"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["loss: 11.96437, test accuracy: 0.00000\n","loss: 11.84746, test accuracy: 0.00000\n","loss: 11.60177, test accuracy: 0.00000\n","loss: 10.92788, test accuracy: 0.18750\n","loss: 10.17590, test accuracy: 0.35000\n","loss: 9.48048, test accuracy: 0.45833\n","loss: 8.86546, test accuracy: 0.53571\n","loss: 8.46888, test accuracy: 0.51562\n","loss: 8.09548, test accuracy: 0.55556\n","loss: 7.78769, test accuracy: 0.55000\n","loss: 7.52819, test accuracy: 0.56818\n","loss: 7.30400, test accuracy: 0.57292\n","loss: 7.10695, test accuracy: 0.58654\n","loss: 6.93403, test accuracy: 0.58929\n","loss: 6.80468, test accuracy: 0.57500\n","loss: 6.68670, test accuracy: 0.57812\n","loss: 6.58557, test accuracy: 0.58088\n","loss: 6.49201, test accuracy: 0.59028\n","loss: 6.40486, test accuracy: 0.59868\n","loss: 6.32359, test accuracy: 0.60625\n","loss: 6.23943, test accuracy: 0.61905\n","loss: 6.15734, test accuracy: 0.63068\n","loss: 6.07773, test accuracy: 0.64130\n","loss: 6.00399, test accuracy: 0.64583\n","loss: 5.93234, test accuracy: 0.65000\n","loss: 5.86296, test accuracy: 0.65385\n","loss: 5.79577, test accuracy: 0.65741\n","loss: 5.73148, test accuracy: 0.66210\n","The accuracy on test set is: 66.210%\n"]}]}]}